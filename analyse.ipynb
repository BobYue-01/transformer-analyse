{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "config = GPT2Config()\n",
    "original_model = GPT2Model(config).cuda()\n",
    "folded_model = GPT2Model(config).cuda()\n",
    "\n",
    "folded_model.load_state_dict(original_model.state_dict())\n",
    "original_model.eval()\n",
    "folded_model.eval()\n",
    "\n",
    "import utils\n",
    "counter = utils.Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <  GPT2Model >\n",
      "   wte : Embedding\n",
      "   wpe : Embedding\n",
      "   drop : Dropout\n",
      "   h : ModuleList\n",
      "   ln_f : LayerNorm\n",
      "   <- MetadataTensor False (1, 128) 0 set()\n",
      "   <  Embedding >\n",
      "     <- MetadataTensor False (1, 128) 0 set()\n",
      "     -> MetadataTensor True (1, 128, 768) 1 {Embedding(50257, 768)}\n",
      "   </ Embedding >\n",
      "   <  Embedding >\n",
      "     <- Tensor None (1, 128) 0 set()\n",
      "     -> MetadataTensor True (1, 128, 768) 1 {Embedding(1024, 768)}\n",
      "   </ Embedding >\n",
      "   <  Dropout >\n",
      "     <- MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "     -> MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "   </ Dropout >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 3 {Embedding(50257, 768), Conv1D(nf=768, nx=768), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 4 {Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 4 {Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 4 {Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 5 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 6 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Embedding(1024, 768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 6 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Embedding(1024, 768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 6 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 7 {Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 8 {Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 8 {Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 8 {Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 9 {Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 10 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 10 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 10 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 11 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 12 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 12 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 12 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 13 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 14 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 14 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 14 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 15 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 16 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 16 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 16 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 17 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 18 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 18 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 18 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 19 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 20 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 20 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 20 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 21 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 22 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 22 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 22 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 23 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 24 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 24 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 24 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 25 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 26 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  LayerNorm >\n",
      "     <- MetadataTensor True (1, 128, 768) 26 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "   </ LayerNorm >\n",
      "LayerNorm: 25\n",
      "Foldable: 25\n",
      "Center modules: {Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BobYu\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\_tensor.py:1437: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "hook_pre_fn, hook_fn = utils.create_analyse_hook_fns(counter)\n",
    "\n",
    "input_ids = torch.randint(0, 1000, (1, 128)).cuda()\n",
    "my_input_ids = utils.MetadataTensor(input_ids, centered=False).cuda()\n",
    "\n",
    "with utils.HookManager(folded_model, hook_fn, hook_pre_fn):\n",
    "    folded_model(my_input_ids)\n",
    "\n",
    "print('LayerNorm:', counter.ln_cnt)\n",
    "print('Foldable:', counter.foldable_cnt)\n",
    "print('Center modules:', counter.center_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules\n",
    "\n",
    "for layer in counter.layernorms:\n",
    "    modules.replace_layer_norm_forward(layer)\n",
    "\n",
    "for layer in counter.center_modules:\n",
    "    modules.center_modules(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_queue = []\n",
    "check = utils.Check()\n",
    "replace = False\n",
    "\n",
    "def hook_original(module, input, output):\n",
    "    name = module.__class__.__name__\n",
    "    output_queue.append((output, name))\n",
    "\n",
    "    # if isinstance(output, tuple):\n",
    "    #     output = output[0]\n",
    "\n",
    "    # with torch._tensor_str.printoptions(precision=10, sci_mode=True):\n",
    "    #     len_shape = len(output.shape)\n",
    "    #     index = tuple([0] * (len_shape - 2) + [slice(None, 4), slice(None, 4)])\n",
    "    #     print(module.__class__.__name__, output[index])\n",
    "\n",
    "def check_close_and_replace(tensor_a, tensor_b, check: utils.Check, tensor_a_str, tensor_b_str):\n",
    "    check.hide_val()\n",
    "    locals()[tensor_a_str] = tensor_a\n",
    "    locals()[tensor_b_str] = tensor_b\n",
    "    if check.check_eq(tensor_a_str, tensor_b_str, abs_tol=1e-5, local_vars=locals()):\n",
    "        if replace and isinstance(tensor_a, torch.Tensor) and isinstance(tensor_b, torch.Tensor):\n",
    "            tensor_b.data = tensor_a.data\n",
    "    check.show_val()\n",
    "\n",
    "def apply_func_to_nested_tuple_pair(t1, t2, func, *args, **kwargs):\n",
    "    if isinstance(t1, tuple) and isinstance(t2, tuple):\n",
    "        return tuple(apply_func_to_nested_tuple_pair(x1, x2, func, *args, **kwargs) for x1, x2 in zip(t1, t2))\n",
    "    else:\n",
    "        return func(t1, t2, *args, **kwargs)\n",
    "\n",
    "def hook_folded(module, input, output):\n",
    "    folded_name = module.__class__.__name__ + '_folded'\n",
    "    original_output, original_name = output_queue.pop(0)\n",
    "    original_name += '_original'\n",
    "    apply_func_to_nested_tuple_pair(original_output, output, check_close_and_replace, check, original_name, folded_name)\n",
    "\n",
    "    # if isinstance(output, tuple):\n",
    "    #     output0 = output[0]\n",
    "\n",
    "    # with torch._tensor_str.printoptions(precision=10, sci_mode=True):\n",
    "    #     len_shape = len(output0.shape)\n",
    "    #     index = tuple([0] * (len_shape - 2) + [slice(None, 4), slice(None, 4)])\n",
    "    #     print(module.__class__.__name__, output0[index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34m# 0 [ Test ] Embedding_original ?= Embedding_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0005862714606337249\u001b[0m\n",
      "\u001b[1;31m# 0 [ Fail ] Embedding_original != Embedding_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 1 [ Test ] Embedding_original ?= Embedding_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0006422363221645355\u001b[0m\n",
      "\u001b[1;31m# 1 [ Fail ] Embedding_original != Embedding_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 2 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0008070007897913456\u001b[0m\n",
      "\u001b[1;31m# 2 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 3 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.024968624114990234\n",
      "Location: [0, 88, 120]\n",
      "LayerNorm_original: 4.170929908752441\n",
      "SOLayerNorm_folded: 4.195898532867432\u001b[0m\n",
      "\u001b[1;31m# 3 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 4 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.014473915100097656\n",
      "Location: [0, 108, 553]\n",
      "Conv1D_original: 2.291804313659668\n",
      "Conv1D_folded: 2.3062782287597656\u001b[0m\n",
      "\u001b[1;31m# 4 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 5 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.004483520984649658\n",
      "Location: [0, 0, 682]\n",
      "Conv1D_original: -0.1537477970123291\n",
      "Conv1D_folded: -0.15823131799697876\u001b[0m\n",
      "\u001b[1;31m# 5 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 6 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.004483520984649658\n",
      "Location: [0, 0, 682]\n",
      "Dropout_original: -0.1537477970123291\n",
      "Dropout_folded: -0.15823131799697876\u001b[0m\n",
      "\u001b[1;31m# 6 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 7 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.004483520984649658\n",
      "Location: [0, 0, 682]\n",
      "GPT2SdpaAttention_original: -0.1537477970123291\n",
      "GPT2SdpaAttention_folded: -0.15823131799697876\u001b[0m\n",
      "\u001b[1;31m# 7 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 8 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.014325618743896484\n",
      "Location: [0, 0, 93, 13]\n",
      "GPT2SdpaAttention_original: -2.2161917686462402\n",
      "GPT2SdpaAttention_folded: -2.2305173873901367\u001b[0m\n",
      "\u001b[1;31m# 8 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 9 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.013627767562866211\n",
      "Location: [0, 10, 107, 38]\n",
      "GPT2SdpaAttention_original: -2.328652858734131\n",
      "GPT2SdpaAttention_folded: -2.342280626296997\u001b[0m\n",
      "\u001b[1;31m# 9 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 10 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 10 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 11 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.024352550506591797\n",
      "Location: [0, 80, 171]\n",
      "LayerNorm_original: 4.0517988204956055\n",
      "SOLayerNorm_folded: 4.076151371002197\u001b[0m\n",
      "\u001b[1;31m# 11 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 12 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.014632463455200195\n",
      "Location: [0, 53, 1955]\n",
      "Conv1D_original: 2.7105870246887207\n",
      "Conv1D_folded: 2.725219488143921\u001b[0m\n",
      "\u001b[1;31m# 12 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 13 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.015511155128479004\n",
      "Location: [0, 93, 35]\n",
      "NewGELUActivation_original: 1.9024022817611694\n",
      "NewGELUActivation_folded: 1.9179134368896484\u001b[0m\n",
      "\u001b[1;31m# 13 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 14 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.008244022727012634\n",
      "Location: [0, 81, 344]\n",
      "Conv1D_original: 0.21273955702781677\n",
      "Conv1D_folded: 0.2209835797548294\u001b[0m\n",
      "\u001b[1;31m# 14 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 15 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.008244022727012634\n",
      "Location: [0, 81, 344]\n",
      "Dropout_original: 0.21273955702781677\n",
      "Dropout_folded: 0.2209835797548294\u001b[0m\n",
      "\u001b[1;31m# 15 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 16 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.008244022727012634\n",
      "Location: [0, 81, 344]\n",
      "GPT2MLP_original: 0.21273955702781677\n",
      "GPT2MLP_folded: 0.2209835797548294\u001b[0m\n",
      "\u001b[1;31m# 16 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 17 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010029181838035583\n",
      "Location: [0, 81, 544]\n",
      "GPT2Block_original: 0.23104456067085266\n",
      "GPT2Block_folded: 0.24107374250888824\u001b[0m\n",
      "\u001b[1;31m# 17 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 18 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.014325618743896484\n",
      "Location: [0, 0, 93, 13]\n",
      "GPT2Block_original: -2.2161917686462402\n",
      "GPT2Block_folded: -2.2305173873901367\u001b[0m\n",
      "\u001b[1;31m# 18 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 19 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.013627767562866211\n",
      "Location: [0, 10, 107, 38]\n",
      "GPT2Block_original: -2.328652858734131\n",
      "GPT2Block_folded: -2.342280626296997\u001b[0m\n",
      "\u001b[1;31m# 19 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 20 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 20 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 21 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01695460081100464\n",
      "Location: [0, 25, 680]\n",
      "LayerNorm_original: -0.6648486852645874\n",
      "SOLayerNorm_folded: -0.681803286075592\u001b[0m\n",
      "\u001b[1;31m# 21 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 22 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.009375721216201782\n",
      "Location: [0, 18, 1005]\n",
      "Conv1D_original: 0.18902811408042908\n",
      "Conv1D_folded: 0.1796523928642273\u001b[0m\n",
      "\u001b[1;31m# 22 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 23 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0026455558836460114\n",
      "Location: [0, 0, 499]\n",
      "Conv1D_original: 0.041134849190711975\n",
      "Conv1D_folded: 0.043780405074357986\u001b[0m\n",
      "\u001b[1;31m# 23 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 24 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0026455558836460114\n",
      "Location: [0, 0, 499]\n",
      "Dropout_original: 0.041134849190711975\n",
      "Dropout_folded: 0.043780405074357986\u001b[0m\n",
      "\u001b[1;31m# 24 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 25 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0026455558836460114\n",
      "Location: [0, 0, 499]\n",
      "GPT2SdpaAttention_original: 0.041134849190711975\n",
      "GPT2SdpaAttention_folded: 0.043780405074357986\u001b[0m\n",
      "\u001b[1;31m# 25 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 26 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.009375721216201782\n",
      "Location: [0, 3, 18, 45]\n",
      "GPT2SdpaAttention_original: 0.18902811408042908\n",
      "GPT2SdpaAttention_folded: 0.1796523928642273\u001b[0m\n",
      "\u001b[1;31m# 26 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 27 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.008806407451629639\n",
      "Location: [0, 7, 17, 17]\n",
      "GPT2SdpaAttention_original: 0.7052639722824097\n",
      "GPT2SdpaAttention_folded: 0.7140703797340393\u001b[0m\n",
      "\u001b[1;31m# 27 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 28 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 28 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 29 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.016617774963378906\n",
      "Location: [0, 3, 751]\n",
      "LayerNorm_original: 1.1968921422958374\n",
      "SOLayerNorm_folded: 1.2135099172592163\u001b[0m\n",
      "\u001b[1;31m# 29 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 30 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.012927666306495667\n",
      "Location: [0, 1, 1617]\n",
      "Conv1D_original: 0.10270251333713531\n",
      "Conv1D_folded: 0.08977484703063965\u001b[0m\n",
      "\u001b[1;31m# 30 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 31 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.009754776954650879\n",
      "Location: [0, 21, 1655]\n",
      "NewGELUActivation_original: 0.8887275457382202\n",
      "NewGELUActivation_folded: 0.8984823226928711\u001b[0m\n",
      "\u001b[1;31m# 31 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 32 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011033445596694946\n",
      "Location: [0, 56, 161]\n",
      "Conv1D_original: 0.02945467084646225\n",
      "Conv1D_folded: 0.040488116443157196\u001b[0m\n",
      "\u001b[1;31m# 32 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 33 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011033445596694946\n",
      "Location: [0, 56, 161]\n",
      "Dropout_original: 0.02945467084646225\n",
      "Dropout_folded: 0.040488116443157196\u001b[0m\n",
      "\u001b[1;31m# 33 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 34 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011033445596694946\n",
      "Location: [0, 56, 161]\n",
      "GPT2MLP_original: 0.02945467084646225\n",
      "GPT2MLP_folded: 0.040488116443157196\u001b[0m\n",
      "\u001b[1;31m# 34 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 35 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01670794188976288\n",
      "Location: [0, 56, 545]\n",
      "GPT2Block_original: 0.15251177549362183\n",
      "GPT2Block_folded: 0.1692197173833847\u001b[0m\n",
      "\u001b[1;31m# 35 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 36 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.009375721216201782\n",
      "Location: [0, 3, 18, 45]\n",
      "GPT2Block_original: 0.18902811408042908\n",
      "GPT2Block_folded: 0.1796523928642273\u001b[0m\n",
      "\u001b[1;31m# 36 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 37 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.008806407451629639\n",
      "Location: [0, 7, 17, 17]\n",
      "GPT2Block_original: 0.7052639722824097\n",
      "GPT2Block_folded: 0.7140703797340393\u001b[0m\n",
      "\u001b[1;31m# 37 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 38 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 38 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 39 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.019272923469543457\n",
      "Location: [0, 2, 339]\n",
      "LayerNorm_original: 1.2872278690338135\n",
      "SOLayerNorm_folded: 1.26795494556427\u001b[0m\n",
      "\u001b[1;31m# 39 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 40 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01137399673461914\n",
      "Location: [0, 103, 1005]\n",
      "Conv1D_original: 0.3172738552093506\n",
      "Conv1D_folded: 0.30589985847473145\u001b[0m\n",
      "\u001b[1;31m# 40 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 41 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0037462450563907623\n",
      "Location: [0, 0, 639]\n",
      "Conv1D_original: -0.017950747162103653\n",
      "Conv1D_folded: -0.021696992218494415\u001b[0m\n",
      "\u001b[1;31m# 41 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 42 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0037462450563907623\n",
      "Location: [0, 0, 639]\n",
      "Dropout_original: -0.017950747162103653\n",
      "Dropout_folded: -0.021696992218494415\u001b[0m\n",
      "\u001b[1;31m# 42 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 43 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0037462450563907623\n",
      "Location: [0, 0, 639]\n",
      "GPT2SdpaAttention_original: -0.017950747162103653\n",
      "GPT2SdpaAttention_folded: -0.021696992218494415\u001b[0m\n",
      "\u001b[1;31m# 43 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 44 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01137399673461914\n",
      "Location: [0, 3, 103, 45]\n",
      "GPT2SdpaAttention_original: 0.3172738552093506\n",
      "GPT2SdpaAttention_folded: 0.30589985847473145\u001b[0m\n",
      "\u001b[1;31m# 44 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 45 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010216593742370605\n",
      "Location: [0, 4, 75, 15]\n",
      "GPT2SdpaAttention_original: 0.8135813474655151\n",
      "GPT2SdpaAttention_folded: 0.8237979412078857\u001b[0m\n",
      "\u001b[1;31m# 45 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 46 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 46 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 47 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.017747066915035248\n",
      "Location: [0, 11, 744]\n",
      "LayerNorm_original: 0.06702687591314316\n",
      "SOLayerNorm_folded: 0.04927980899810791\u001b[0m\n",
      "\u001b[1;31m# 47 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 48 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0126693956553936\n",
      "Location: [0, 10, 2082]\n",
      "Conv1D_original: 0.07243213802576065\n",
      "Conv1D_folded: 0.05976274237036705\u001b[0m\n",
      "\u001b[1;31m# 48 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 49 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010422110557556152\n",
      "Location: [0, 38, 923]\n",
      "NewGELUActivation_original: 1.0340142250061035\n",
      "NewGELUActivation_folded: 1.0235921144485474\u001b[0m\n",
      "\u001b[1;31m# 49 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 50 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005781389772891998\n",
      "Location: [0, 37, 455]\n",
      "Conv1D_original: 0.10896207392215729\n",
      "Conv1D_folded: 0.10318068414926529\u001b[0m\n",
      "\u001b[1;31m# 50 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 51 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005781389772891998\n",
      "Location: [0, 37, 455]\n",
      "Dropout_original: 0.10896207392215729\n",
      "Dropout_folded: 0.10318068414926529\u001b[0m\n",
      "\u001b[1;31m# 51 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 52 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005781389772891998\n",
      "Location: [0, 37, 455]\n",
      "GPT2MLP_original: 0.10896207392215729\n",
      "GPT2MLP_folded: 0.10318068414926529\u001b[0m\n",
      "\u001b[1;31m# 52 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 53 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.020275399088859558\n",
      "Location: [0, 56, 161]\n",
      "GPT2Block_original: 0.1565457582473755\n",
      "GPT2Block_folded: 0.17682115733623505\u001b[0m\n",
      "\u001b[1;31m# 53 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 54 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01137399673461914\n",
      "Location: [0, 3, 103, 45]\n",
      "GPT2Block_original: 0.3172738552093506\n",
      "GPT2Block_folded: 0.30589985847473145\u001b[0m\n",
      "\u001b[1;31m# 54 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 55 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010216593742370605\n",
      "Location: [0, 4, 75, 15]\n",
      "GPT2Block_original: 0.8135813474655151\n",
      "GPT2Block_folded: 0.8237979412078857\u001b[0m\n",
      "\u001b[1;31m# 55 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 56 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 56 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 57 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01905760169029236\n",
      "Location: [0, 92, 269]\n",
      "LayerNorm_original: -0.34400251507759094\n",
      "SOLayerNorm_folded: -0.3249449133872986\u001b[0m\n",
      "\u001b[1;31m# 57 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 58 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011582881212234497\n",
      "Location: [0, 28, 505]\n",
      "Conv1D_original: -0.4137447476387024\n",
      "Conv1D_folded: -0.4021618664264679\u001b[0m\n",
      "\u001b[1;31m# 58 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 59 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.003918759524822235\n",
      "Location: [0, 0, 566]\n",
      "Conv1D_original: -0.08833425492048264\n",
      "Conv1D_folded: -0.09225301444530487\u001b[0m\n",
      "\u001b[1;31m# 59 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 60 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.003918759524822235\n",
      "Location: [0, 0, 566]\n",
      "Dropout_original: -0.08833425492048264\n",
      "Dropout_folded: -0.09225301444530487\u001b[0m\n",
      "\u001b[1;31m# 60 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 61 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.003918759524822235\n",
      "Location: [0, 0, 566]\n",
      "GPT2SdpaAttention_original: -0.08833425492048264\n",
      "GPT2SdpaAttention_folded: -0.09225301444530487\u001b[0m\n",
      "\u001b[1;31m# 61 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 62 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01100054383277893\n",
      "Location: [0, 7, 41, 21]\n",
      "GPT2SdpaAttention_original: -0.3457162380218506\n",
      "GPT2SdpaAttention_folded: -0.3567167818546295\u001b[0m\n",
      "\u001b[1;31m# 62 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 63 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01109546422958374\n",
      "Location: [0, 3, 75, 48]\n",
      "GPT2SdpaAttention_original: 0.5344060659408569\n",
      "GPT2SdpaAttention_folded: 0.5233106017112732\u001b[0m\n",
      "\u001b[1;31m# 63 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 64 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 64 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 65 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.018473148345947266\n",
      "Location: [0, 45, 258]\n",
      "LayerNorm_original: -1.6005680561065674\n",
      "SOLayerNorm_folded: -1.5820949077606201\u001b[0m\n",
      "\u001b[1;31m# 65 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 66 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011124223470687866\n",
      "Location: [0, 80, 2666]\n",
      "Conv1D_original: -0.020859748125076294\n",
      "Conv1D_folded: -0.03198397159576416\u001b[0m\n",
      "\u001b[1;31m# 66 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 67 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011983036994934082\n",
      "Location: [0, 19, 671]\n",
      "NewGELUActivation_original: 1.0775835514068604\n",
      "NewGELUActivation_folded: 1.0895665884017944\u001b[0m\n",
      "\u001b[1;31m# 67 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 68 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0055893585085868835\n",
      "Location: [0, 16, 486]\n",
      "Conv1D_original: 0.04837939888238907\n",
      "Conv1D_folded: 0.05396875739097595\u001b[0m\n",
      "\u001b[1;31m# 68 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 69 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0055893585085868835\n",
      "Location: [0, 16, 486]\n",
      "Dropout_original: 0.04837939888238907\n",
      "Dropout_folded: 0.05396875739097595\u001b[0m\n",
      "\u001b[1;31m# 69 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 70 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0055893585085868835\n",
      "Location: [0, 16, 486]\n",
      "GPT2MLP_original: 0.04837939888238907\n",
      "GPT2MLP_folded: 0.05396875739097595\u001b[0m\n",
      "\u001b[1;31m# 70 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 71 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.020469650626182556\n",
      "Location: [0, 56, 161]\n",
      "GPT2Block_original: 0.161776602268219\n",
      "GPT2Block_folded: 0.18224625289440155\u001b[0m\n",
      "\u001b[1;31m# 71 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 72 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01100054383277893\n",
      "Location: [0, 7, 41, 21]\n",
      "GPT2Block_original: -0.3457162380218506\n",
      "GPT2Block_folded: -0.3567167818546295\u001b[0m\n",
      "\u001b[1;31m# 72 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 73 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01109546422958374\n",
      "Location: [0, 3, 75, 48]\n",
      "GPT2Block_original: 0.5344060659408569\n",
      "GPT2Block_folded: 0.5233106017112732\u001b[0m\n",
      "\u001b[1;31m# 73 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 74 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 74 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 75 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.019119173288345337\n",
      "Location: [0, 45, 445]\n",
      "LayerNorm_original: 0.4020572900772095\n",
      "SOLayerNorm_folded: 0.4211764633655548\u001b[0m\n",
      "\u001b[1;31m# 75 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 76 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011778831481933594\n",
      "Location: [0, 83, 2058]\n",
      "Conv1D_original: 0.09774797409772873\n",
      "Conv1D_folded: 0.08596914261579514\u001b[0m\n",
      "\u001b[1;31m# 76 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 77 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.001265939325094223\n",
      "Location: [0, 0, 103]\n",
      "Conv1D_original: 3.855675458908081e-07\n",
      "Conv1D_folded: -0.0012655537575483322\u001b[0m\n",
      "\u001b[1;31m# 77 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 78 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.001265939325094223\n",
      "Location: [0, 0, 103]\n",
      "Dropout_original: 3.855675458908081e-07\n",
      "Dropout_folded: -0.0012655537575483322\u001b[0m\n",
      "\u001b[1;31m# 78 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 79 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.001265939325094223\n",
      "Location: [0, 0, 103]\n",
      "GPT2SdpaAttention_original: 3.855675458908081e-07\n",
      "GPT2SdpaAttention_folded: -0.0012655537575483322\u001b[0m\n",
      "\u001b[1;31m# 79 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 80 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010970056056976318\n",
      "Location: [0, 0, 40, 13]\n",
      "GPT2SdpaAttention_original: -0.6157967448234558\n",
      "GPT2SdpaAttention_folded: -0.6267668008804321\u001b[0m\n",
      "\u001b[1;31m# 80 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 81 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011778831481933594\n",
      "Location: [0, 8, 83, 10]\n",
      "GPT2SdpaAttention_original: 0.09774797409772873\n",
      "GPT2SdpaAttention_folded: 0.08596914261579514\u001b[0m\n",
      "\u001b[1;31m# 81 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 82 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 82 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 83 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.019959062337875366\n",
      "Location: [0, 45, 445]\n",
      "LayerNorm_original: 0.4275323748588562\n",
      "SOLayerNorm_folded: 0.44749143719673157\u001b[0m\n",
      "\u001b[1;31m# 83 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 84 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01224982738494873\n",
      "Location: [0, 91, 2901]\n",
      "Conv1D_original: 0.2768430709838867\n",
      "Conv1D_folded: 0.264593243598938\u001b[0m\n",
      "\u001b[1;31m# 84 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 85 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011804759502410889\n",
      "Location: [0, 3, 2274]\n",
      "NewGELUActivation_original: 0.9147935509681702\n",
      "NewGELUActivation_folded: 0.926598310470581\u001b[0m\n",
      "\u001b[1;31m# 85 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 86 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.008847031742334366\n",
      "Location: [0, 108, 408]\n",
      "Conv1D_original: 0.0597367100417614\n",
      "Conv1D_folded: 0.06858374178409576\u001b[0m\n",
      "\u001b[1;31m# 86 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 87 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.008847031742334366\n",
      "Location: [0, 108, 408]\n",
      "Dropout_original: 0.0597367100417614\n",
      "Dropout_folded: 0.06858374178409576\u001b[0m\n",
      "\u001b[1;31m# 87 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 88 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.008847031742334366\n",
      "Location: [0, 108, 408]\n",
      "GPT2MLP_original: 0.0597367100417614\n",
      "GPT2MLP_folded: 0.06858374178409576\u001b[0m\n",
      "\u001b[1;31m# 88 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 89 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.019831430166959763\n",
      "Location: [0, 56, 219]\n",
      "GPT2Block_original: 0.0368267223238945\n",
      "GPT2Block_folded: 0.05665815249085426\u001b[0m\n",
      "\u001b[1;31m# 89 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 90 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010970056056976318\n",
      "Location: [0, 0, 40, 13]\n",
      "GPT2Block_original: -0.6157967448234558\n",
      "GPT2Block_folded: -0.6267668008804321\u001b[0m\n",
      "\u001b[1;31m# 90 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 91 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011778831481933594\n",
      "Location: [0, 8, 83, 10]\n",
      "GPT2Block_original: 0.09774797409772873\n",
      "GPT2Block_folded: 0.08596914261579514\u001b[0m\n",
      "\u001b[1;31m# 91 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 92 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 92 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 93 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0186384916305542\n",
      "Location: [0, 83, 50]\n",
      "LayerNorm_original: 1.909059762954712\n",
      "SOLayerNorm_folded: 1.8904212713241577\u001b[0m\n",
      "\u001b[1;31m# 93 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 94 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01238250732421875\n",
      "Location: [0, 83, 1389]\n",
      "Conv1D_original: -0.7469867467880249\n",
      "Conv1D_folded: -0.7346042394638062\u001b[0m\n",
      "\u001b[1;31m# 94 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 95 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005831016227602959\n",
      "Location: [0, 0, 152]\n",
      "Conv1D_original: 0.01423658151179552\n",
      "Conv1D_folded: 0.008405565284192562\u001b[0m\n",
      "\u001b[1;31m# 95 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 96 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005831016227602959\n",
      "Location: [0, 0, 152]\n",
      "Dropout_original: 0.01423658151179552\n",
      "Dropout_folded: 0.008405565284192562\u001b[0m\n",
      "\u001b[1;31m# 96 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 97 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005831016227602959\n",
      "Location: [0, 0, 152]\n",
      "GPT2SdpaAttention_original: 0.01423658151179552\n",
      "GPT2SdpaAttention_folded: 0.008405565284192562\u001b[0m\n",
      "\u001b[1;31m# 97 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 98 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01238250732421875\n",
      "Location: [0, 9, 83, 45]\n",
      "GPT2SdpaAttention_original: -0.7469867467880249\n",
      "GPT2SdpaAttention_folded: -0.7346042394638062\u001b[0m\n",
      "\u001b[1;31m# 98 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 99 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010766774415969849\n",
      "Location: [0, 8, 83, 44]\n",
      "GPT2SdpaAttention_original: 0.12956896424293518\n",
      "GPT2SdpaAttention_folded: 0.11880218982696533\u001b[0m\n",
      "\u001b[1;31m# 99 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 100 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 100 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 101 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01805567741394043\n",
      "Location: [0, 83, 50]\n",
      "LayerNorm_original: 2.11942982673645\n",
      "SOLayerNorm_folded: 2.1013741493225098\u001b[0m\n",
      "\u001b[1;31m# 101 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 102 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011197388172149658\n",
      "Location: [0, 76, 2522]\n",
      "Conv1D_original: 0.3808187246322632\n",
      "Conv1D_folded: 0.39201611280441284\u001b[0m\n",
      "\u001b[1;31m# 102 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 103 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010344743728637695\n",
      "Location: [0, 69, 1540]\n",
      "NewGELUActivation_original: 1.155124306678772\n",
      "NewGELUActivation_folded: 1.1447795629501343\u001b[0m\n",
      "\u001b[1;31m# 103 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 104 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.006207529455423355\n",
      "Location: [0, 85, 673]\n",
      "Conv1D_original: 0.02764463797211647\n",
      "Conv1D_folded: 0.033852167427539825\u001b[0m\n",
      "\u001b[1;31m# 104 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 105 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.006207529455423355\n",
      "Location: [0, 85, 673]\n",
      "Dropout_original: 0.02764463797211647\n",
      "Dropout_folded: 0.033852167427539825\u001b[0m\n",
      "\u001b[1;31m# 105 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 106 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.006207529455423355\n",
      "Location: [0, 85, 673]\n",
      "GPT2MLP_original: 0.02764463797211647\n",
      "GPT2MLP_folded: 0.033852167427539825\u001b[0m\n",
      "\u001b[1;31m# 106 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 107 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.017522843554615974\n",
      "Location: [0, 56, 219]\n",
      "GPT2Block_original: -0.00016738474369049072\n",
      "GPT2Block_folded: 0.017355458810925484\u001b[0m\n",
      "\u001b[1;31m# 107 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 108 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01238250732421875\n",
      "Location: [0, 9, 83, 45]\n",
      "GPT2Block_original: -0.7469867467880249\n",
      "GPT2Block_folded: -0.7346042394638062\u001b[0m\n",
      "\u001b[1;31m# 108 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 109 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010766774415969849\n",
      "Location: [0, 8, 83, 44]\n",
      "GPT2Block_original: 0.12956896424293518\n",
      "GPT2Block_folded: 0.11880218982696533\u001b[0m\n",
      "\u001b[1;31m# 109 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 110 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 110 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 111 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01909482479095459\n",
      "Location: [0, 17, 602]\n",
      "LayerNorm_original: -0.36182960867881775\n",
      "SOLayerNorm_folded: -0.34273478388786316\u001b[0m\n",
      "\u001b[1;31m# 111 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 112 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011177778244018555\n",
      "Location: [0, 45, 22]\n",
      "Conv1D_original: -0.7524056434631348\n",
      "Conv1D_folded: -0.7412278652191162\u001b[0m\n",
      "\u001b[1;31m# 112 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 113 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0033466648310422897\n",
      "Location: [0, 0, 228]\n",
      "Conv1D_original: -0.014837881550192833\n",
      "Conv1D_folded: -0.018184546381235123\u001b[0m\n",
      "\u001b[1;31m# 113 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 114 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0033466648310422897\n",
      "Location: [0, 0, 228]\n",
      "Dropout_original: -0.014837881550192833\n",
      "Dropout_folded: -0.018184546381235123\u001b[0m\n",
      "\u001b[1;31m# 114 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 115 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0033466648310422897\n",
      "Location: [0, 0, 228]\n",
      "GPT2SdpaAttention_original: -0.014837881550192833\n",
      "GPT2SdpaAttention_folded: -0.018184546381235123\u001b[0m\n",
      "\u001b[1;31m# 115 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 116 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0109957754611969\n",
      "Location: [0, 0, 92, 19]\n",
      "GPT2SdpaAttention_original: -0.011545151472091675\n",
      "GPT2SdpaAttention_folded: -0.0005493760108947754\u001b[0m\n",
      "\u001b[1;31m# 116 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 117 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010460138320922852\n",
      "Location: [0, 10, 63, 20]\n",
      "GPT2SdpaAttention_original: -1.1593849658966064\n",
      "GPT2SdpaAttention_folded: -1.1698451042175293\u001b[0m\n",
      "\u001b[1;31m# 117 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 118 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 118 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 119 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.018535137176513672\n",
      "Location: [0, 17, 602]\n",
      "LayerNorm_original: -0.6885823607444763\n",
      "SOLayerNorm_folded: -0.6700472235679626\u001b[0m\n",
      "\u001b[1;31m# 119 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 120 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01248788833618164\n",
      "Location: [0, 126, 761]\n",
      "Conv1D_original: 0.5002275705337524\n",
      "Conv1D_folded: 0.4877396821975708\u001b[0m\n",
      "\u001b[1;31m# 120 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 121 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010856032371520996\n",
      "Location: [0, 90, 1419]\n",
      "NewGELUActivation_original: 1.0857970714569092\n",
      "NewGELUActivation_folded: 1.0749410390853882\u001b[0m\n",
      "\u001b[1;31m# 121 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 122 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.008718714118003845\n",
      "Location: [0, 104, 621]\n",
      "Conv1D_original: 0.038579829037189484\n",
      "Conv1D_folded: 0.04729854315519333\u001b[0m\n",
      "\u001b[1;31m# 122 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 123 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.008718714118003845\n",
      "Location: [0, 104, 621]\n",
      "Dropout_original: 0.038579829037189484\n",
      "Dropout_folded: 0.04729854315519333\u001b[0m\n",
      "\u001b[1;31m# 123 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 124 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.008718714118003845\n",
      "Location: [0, 104, 621]\n",
      "GPT2MLP_original: 0.038579829037189484\n",
      "GPT2MLP_folded: 0.04729854315519333\u001b[0m\n",
      "\u001b[1;31m# 124 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 125 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01746039092540741\n",
      "Location: [0, 56, 219]\n",
      "GPT2Block_original: 0.011356703005731106\n",
      "GPT2Block_folded: 0.02881709486246109\u001b[0m\n",
      "\u001b[1;31m# 125 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 126 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0109957754611969\n",
      "Location: [0, 0, 92, 19]\n",
      "GPT2Block_original: -0.011545151472091675\n",
      "GPT2Block_folded: -0.0005493760108947754\u001b[0m\n",
      "\u001b[1;31m# 126 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 127 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010460138320922852\n",
      "Location: [0, 10, 63, 20]\n",
      "GPT2Block_original: -1.1593849658966064\n",
      "GPT2Block_folded: -1.1698451042175293\u001b[0m\n",
      "\u001b[1;31m# 127 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 128 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 128 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 129 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.019013643264770508\n",
      "Location: [0, 17, 602]\n",
      "LayerNorm_original: -0.9045088291168213\n",
      "SOLayerNorm_folded: -0.8854951858520508\u001b[0m\n",
      "\u001b[1;31m# 129 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 130 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011920943856239319\n",
      "Location: [0, 54, 1841]\n",
      "Conv1D_original: -0.11497445404529572\n",
      "Conv1D_folded: -0.12689539790153503\u001b[0m\n",
      "\u001b[1;31m# 130 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 131 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0018640011548995972\n",
      "Location: [0, 1, 155]\n",
      "Conv1D_original: 0.1387168914079666\n",
      "Conv1D_folded: 0.1405808925628662\u001b[0m\n",
      "\u001b[1;31m# 131 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 132 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0018640011548995972\n",
      "Location: [0, 1, 155]\n",
      "Dropout_original: 0.1387168914079666\n",
      "Dropout_folded: 0.1405808925628662\u001b[0m\n",
      "\u001b[1;31m# 132 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 133 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0018640011548995972\n",
      "Location: [0, 1, 155]\n",
      "GPT2SdpaAttention_original: 0.1387168914079666\n",
      "GPT2SdpaAttention_folded: 0.1405808925628662\u001b[0m\n",
      "\u001b[1;31m# 133 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 134 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010721743106842041\n",
      "Location: [0, 3, 108, 36]\n",
      "GPT2SdpaAttention_original: -0.572698175907135\n",
      "GPT2SdpaAttention_folded: -0.583419919013977\u001b[0m\n",
      "\u001b[1;31m# 134 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 135 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011920943856239319\n",
      "Location: [0, 4, 54, 49]\n",
      "GPT2SdpaAttention_original: -0.11497445404529572\n",
      "GPT2SdpaAttention_folded: -0.12689539790153503\u001b[0m\n",
      "\u001b[1;31m# 135 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 136 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 136 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 137 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.019019126892089844\n",
      "Location: [0, 123, 137]\n",
      "LayerNorm_original: 0.8038737773895264\n",
      "SOLayerNorm_folded: 0.7848546504974365\u001b[0m\n",
      "\u001b[1;31m# 137 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 138 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011505842208862305\n",
      "Location: [0, 90, 2980]\n",
      "Conv1D_original: 0.8077024221420288\n",
      "Conv1D_folded: 0.7961965799331665\u001b[0m\n",
      "\u001b[1;31m# 138 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 139 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.012652337551116943\n",
      "Location: [0, 73, 2676]\n",
      "NewGELUActivation_original: 0.9767898321151733\n",
      "NewGELUActivation_folded: 0.9894421696662903\u001b[0m\n",
      "\u001b[1;31m# 139 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 140 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.00699867308139801\n",
      "Location: [0, 29, 117]\n",
      "Conv1D_original: 0.0848725289106369\n",
      "Conv1D_folded: 0.07787385582923889\u001b[0m\n",
      "\u001b[1;31m# 140 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 141 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.00699867308139801\n",
      "Location: [0, 29, 117]\n",
      "Dropout_original: 0.0848725289106369\n",
      "Dropout_folded: 0.07787385582923889\u001b[0m\n",
      "\u001b[1;31m# 141 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 142 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.00699867308139801\n",
      "Location: [0, 29, 117]\n",
      "GPT2MLP_original: 0.0848725289106369\n",
      "GPT2MLP_folded: 0.07787385582923889\u001b[0m\n",
      "\u001b[1;31m# 142 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 143 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01697281002998352\n",
      "Location: [0, 20, 392]\n",
      "GPT2Block_original: -0.04058798402547836\n",
      "GPT2Block_folded: -0.057560794055461884\u001b[0m\n",
      "\u001b[1;31m# 143 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 144 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010721743106842041\n",
      "Location: [0, 3, 108, 36]\n",
      "GPT2Block_original: -0.572698175907135\n",
      "GPT2Block_folded: -0.583419919013977\u001b[0m\n",
      "\u001b[1;31m# 144 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 145 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011920943856239319\n",
      "Location: [0, 4, 54, 49]\n",
      "GPT2Block_original: -0.11497445404529572\n",
      "GPT2Block_folded: -0.12689539790153503\u001b[0m\n",
      "\u001b[1;31m# 145 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 146 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 146 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 147 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01999080181121826\n",
      "Location: [0, 123, 137]\n",
      "LayerNorm_original: 1.129399299621582\n",
      "SOLayerNorm_folded: 1.1094084978103638\u001b[0m\n",
      "\u001b[1;31m# 147 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 148 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011506348848342896\n",
      "Location: [0, 87, 1225]\n",
      "Conv1D_original: 0.35655584931373596\n",
      "Conv1D_folded: 0.34504950046539307\u001b[0m\n",
      "\u001b[1;31m# 148 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 149 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0024890899658203125\n",
      "Location: [0, 36, 372]\n",
      "Conv1D_original: -0.043433427810668945\n",
      "Conv1D_folded: -0.04094433784484863\u001b[0m\n",
      "\u001b[1;31m# 149 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 150 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0024890899658203125\n",
      "Location: [0, 36, 372]\n",
      "Dropout_original: -0.043433427810668945\n",
      "Dropout_folded: -0.04094433784484863\u001b[0m\n",
      "\u001b[1;31m# 150 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 151 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0024890899658203125\n",
      "Location: [0, 36, 372]\n",
      "GPT2SdpaAttention_original: -0.043433427810668945\n",
      "GPT2SdpaAttention_folded: -0.04094433784484863\u001b[0m\n",
      "\u001b[1;31m# 151 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 152 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011506348848342896\n",
      "Location: [0, 7, 87, 9]\n",
      "GPT2SdpaAttention_original: 0.35655584931373596\n",
      "GPT2SdpaAttention_folded: 0.34504950046539307\u001b[0m\n",
      "\u001b[1;31m# 152 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 153 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011076271533966064\n",
      "Location: [0, 6, 103, 8]\n",
      "GPT2SdpaAttention_original: 0.5530322790145874\n",
      "GPT2SdpaAttention_folded: 0.5419560074806213\u001b[0m\n",
      "\u001b[1;31m# 153 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 154 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 154 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 155 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02023935317993164\n",
      "Location: [0, 123, 137]\n",
      "LayerNorm_original: 1.0988065004348755\n",
      "SOLayerNorm_folded: 1.0785671472549438\u001b[0m\n",
      "\u001b[1;31m# 155 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 156 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.012589573860168457\n",
      "Location: [0, 108, 1840]\n",
      "Conv1D_original: 1.00754714012146\n",
      "Conv1D_folded: 1.0201367139816284\u001b[0m\n",
      "\u001b[1;31m# 156 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 157 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.013675332069396973\n",
      "Location: [0, 108, 1840]\n",
      "NewGELUActivation_original: 0.849372148513794\n",
      "NewGELUActivation_folded: 0.8630474805831909\u001b[0m\n",
      "\u001b[1;31m# 157 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 158 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005689635872840881\n",
      "Location: [0, 89, 449]\n",
      "Conv1D_original: -0.13654237985610962\n",
      "Conv1D_folded: -0.13085274398326874\u001b[0m\n",
      "\u001b[1;31m# 158 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 159 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005689635872840881\n",
      "Location: [0, 89, 449]\n",
      "Dropout_original: -0.13654237985610962\n",
      "Dropout_folded: -0.13085274398326874\u001b[0m\n",
      "\u001b[1;31m# 159 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 160 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005689635872840881\n",
      "Location: [0, 89, 449]\n",
      "GPT2MLP_original: -0.13654237985610962\n",
      "GPT2MLP_folded: -0.13085274398326874\u001b[0m\n",
      "\u001b[1;31m# 160 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 161 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.019379958510398865\n",
      "Location: [0, 56, 219]\n",
      "GPT2Block_original: -0.20614230632781982\n",
      "GPT2Block_folded: -0.18676234781742096\u001b[0m\n",
      "\u001b[1;31m# 161 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 162 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011506348848342896\n",
      "Location: [0, 7, 87, 9]\n",
      "GPT2Block_original: 0.35655584931373596\n",
      "GPT2Block_folded: 0.34504950046539307\u001b[0m\n",
      "\u001b[1;31m# 162 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 163 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011076271533966064\n",
      "Location: [0, 6, 103, 8]\n",
      "GPT2Block_original: 0.5530322790145874\n",
      "GPT2Block_folded: 0.5419560074806213\u001b[0m\n",
      "\u001b[1;31m# 163 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 164 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 164 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 165 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01924002170562744\n",
      "Location: [0, 123, 137]\n",
      "LayerNorm_original: 1.1168510913848877\n",
      "SOLayerNorm_folded: 1.0976110696792603\u001b[0m\n",
      "\u001b[1;31m# 165 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 166 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011641621589660645\n",
      "Location: [0, 87, 95]\n",
      "Conv1D_original: -0.19555014371871948\n",
      "Conv1D_folded: -0.18390852212905884\u001b[0m\n",
      "\u001b[1;31m# 166 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 167 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.002219492569565773\n",
      "Location: [0, 76, 460]\n",
      "Conv1D_original: -0.00748868752270937\n",
      "Conv1D_folded: -0.009708180092275143\u001b[0m\n",
      "\u001b[1;31m# 167 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 168 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.002219492569565773\n",
      "Location: [0, 76, 460]\n",
      "Dropout_original: -0.00748868752270937\n",
      "Dropout_folded: -0.009708180092275143\u001b[0m\n",
      "\u001b[1;31m# 168 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 169 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.002219492569565773\n",
      "Location: [0, 76, 460]\n",
      "GPT2SdpaAttention_original: -0.00748868752270937\n",
      "GPT2SdpaAttention_folded: -0.009708180092275143\u001b[0m\n",
      "\u001b[1;31m# 169 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 170 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010803520679473877\n",
      "Location: [0, 4, 87, 0]\n",
      "GPT2SdpaAttention_original: 0.32257696986198425\n",
      "GPT2SdpaAttention_folded: 0.33338049054145813\u001b[0m\n",
      "\u001b[1;31m# 170 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 171 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010842204093933105\n",
      "Location: [0, 1, 30, 33]\n",
      "GPT2SdpaAttention_original: -1.1130505800247192\n",
      "GPT2SdpaAttention_folded: -1.1238927841186523\u001b[0m\n",
      "\u001b[1;31m# 171 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 172 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 172 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 173 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.019471943378448486\n",
      "Location: [0, 123, 137]\n",
      "LayerNorm_original: 1.0087316036224365\n",
      "SOLayerNorm_folded: 0.989259660243988\u001b[0m\n",
      "\u001b[1;31m# 173 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 174 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.012723147869110107\n",
      "Location: [0, 119, 912]\n",
      "Conv1D_original: 0.48633503913879395\n",
      "Conv1D_folded: 0.49905818700790405\u001b[0m\n",
      "\u001b[1;31m# 174 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 175 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011184275150299072\n",
      "Location: [0, 103, 381]\n",
      "NewGELUActivation_original: 0.4923948645591736\n",
      "NewGELUActivation_folded: 0.4812105894088745\u001b[0m\n",
      "\u001b[1;31m# 175 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 176 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.009915776550769806\n",
      "Location: [0, 66, 545]\n",
      "Conv1D_original: 0.03767213970422745\n",
      "Conv1D_folded: 0.04758791625499725\u001b[0m\n",
      "\u001b[1;31m# 176 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 177 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.009915776550769806\n",
      "Location: [0, 66, 545]\n",
      "Dropout_original: 0.03767213970422745\n",
      "Dropout_folded: 0.04758791625499725\u001b[0m\n",
      "\u001b[1;31m# 177 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 178 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.009915776550769806\n",
      "Location: [0, 66, 545]\n",
      "GPT2MLP_original: 0.03767213970422745\n",
      "GPT2MLP_folded: 0.04758791625499725\u001b[0m\n",
      "\u001b[1;31m# 178 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 179 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.021925315260887146\n",
      "Location: [0, 56, 219]\n",
      "GPT2Block_original: 0.036715686321258545\n",
      "GPT2Block_folded: 0.05864100158214569\u001b[0m\n",
      "\u001b[1;31m# 179 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 180 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010803520679473877\n",
      "Location: [0, 4, 87, 0]\n",
      "GPT2Block_original: 0.32257696986198425\n",
      "GPT2Block_folded: 0.33338049054145813\u001b[0m\n",
      "\u001b[1;31m# 180 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 181 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010842204093933105\n",
      "Location: [0, 1, 30, 33]\n",
      "GPT2Block_original: -1.1130505800247192\n",
      "GPT2Block_folded: -1.1238927841186523\u001b[0m\n",
      "\u001b[1;31m# 181 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 182 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 182 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 183 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.018776774406433105\n",
      "Location: [0, 103, 329]\n",
      "LayerNorm_original: -1.5978553295135498\n",
      "SOLayerNorm_folded: -1.5790785551071167\u001b[0m\n",
      "\u001b[1;31m# 183 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 184 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01303526759147644\n",
      "Location: [0, 30, 1209]\n",
      "Conv1D_original: 0.13344816863536835\n",
      "Conv1D_folded: 0.1204129010438919\u001b[0m\n",
      "\u001b[1;31m# 184 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 185 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005651962012052536\n",
      "Location: [0, 0, 378]\n",
      "Conv1D_original: -0.03199206665158272\n",
      "Conv1D_folded: -0.037644028663635254\u001b[0m\n",
      "\u001b[1;31m# 185 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 186 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005651962012052536\n",
      "Location: [0, 0, 378]\n",
      "Dropout_original: -0.03199206665158272\n",
      "Dropout_folded: -0.037644028663635254\u001b[0m\n",
      "\u001b[1;31m# 186 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 187 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.005651962012052536\n",
      "Location: [0, 0, 378]\n",
      "GPT2SdpaAttention_original: -0.03199206665158272\n",
      "GPT2SdpaAttention_folded: -0.037644028663635254\u001b[0m\n",
      "\u001b[1;31m# 187 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 188 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01303526759147644\n",
      "Location: [0, 6, 30, 57]\n",
      "GPT2SdpaAttention_original: 0.13344816863536835\n",
      "GPT2SdpaAttention_folded: 0.1204129010438919\u001b[0m\n",
      "\u001b[1;31m# 188 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 189 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010093092918395996\n",
      "Location: [0, 3, 108, 34]\n",
      "GPT2SdpaAttention_original: -0.40557074546813965\n",
      "GPT2SdpaAttention_folded: -0.41566383838653564\u001b[0m\n",
      "\u001b[1;31m# 189 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 190 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 190 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 191 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.018861770629882812\n",
      "Location: [0, 123, 137]\n",
      "LayerNorm_original: 1.1120703220367432\n",
      "SOLayerNorm_folded: 1.0932085514068604\u001b[0m\n",
      "\u001b[1;31m# 191 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 192 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.012706965208053589\n",
      "Location: [0, 30, 2525]\n",
      "Conv1D_original: -0.46695688366889954\n",
      "Conv1D_folded: -0.4796638488769531\u001b[0m\n",
      "\u001b[1;31m# 192 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 193 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.013043999671936035\n",
      "Location: [0, 103, 1304]\n",
      "NewGELUActivation_original: 1.1491825580596924\n",
      "NewGELUActivation_folded: 1.1622265577316284\u001b[0m\n",
      "\u001b[1;31m# 193 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 194 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.007866770029067993\n",
      "Location: [0, 73, 627]\n",
      "Conv1D_original: 0.12921413779258728\n",
      "Conv1D_folded: 0.13708090782165527\u001b[0m\n",
      "\u001b[1;31m# 194 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 195 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.007866770029067993\n",
      "Location: [0, 73, 627]\n",
      "Dropout_original: 0.12921413779258728\n",
      "Dropout_folded: 0.13708090782165527\u001b[0m\n",
      "\u001b[1;31m# 195 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 196 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.007866770029067993\n",
      "Location: [0, 73, 627]\n",
      "GPT2MLP_original: 0.12921413779258728\n",
      "GPT2MLP_folded: 0.13708090782165527\u001b[0m\n",
      "\u001b[1;31m# 196 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 197 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.022832810878753662\n",
      "Location: [0, 124, 690]\n",
      "GPT2Block_original: -0.38560914993286133\n",
      "GPT2Block_folded: -0.36277633905410767\u001b[0m\n",
      "\u001b[1;31m# 197 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 198 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01303526759147644\n",
      "Location: [0, 6, 30, 57]\n",
      "GPT2Block_original: 0.13344816863536835\n",
      "GPT2Block_folded: 0.1204129010438919\u001b[0m\n",
      "\u001b[1;31m# 198 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 199 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.010093092918395996\n",
      "Location: [0, 3, 108, 34]\n",
      "GPT2Block_original: -0.40557074546813965\n",
      "GPT2Block_folded: -0.41566383838653564\u001b[0m\n",
      "\u001b[1;31m# 199 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 200 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 200 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 201 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.019698500633239746\n",
      "Location: [0, 103, 329]\n",
      "LayerNorm_original: -1.8806754350662231\n",
      "SOLayerNorm_folded: -1.8609769344329834\u001b[0m\n",
      "\u001b[1;31m# 201 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 202 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011475160717964172\n",
      "Location: [0, 45, 844]\n",
      "Conv1D_original: -0.17895498871803284\n",
      "Conv1D_folded: -0.190430149435997\u001b[0m\n",
      "\u001b[1;31m# 202 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 203 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0021997615694999695\n",
      "Location: [0, 0, 397]\n",
      "Conv1D_original: 0.09817295521497726\n",
      "Conv1D_folded: 0.0959731936454773\u001b[0m\n",
      "\u001b[1;31m# 203 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 204 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0021997615694999695\n",
      "Location: [0, 0, 397]\n",
      "Dropout_original: 0.09817295521497726\n",
      "Dropout_folded: 0.0959731936454773\u001b[0m\n",
      "\u001b[1;31m# 204 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 205 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0021997615694999695\n",
      "Location: [0, 0, 397]\n",
      "GPT2SdpaAttention_original: 0.09817295521497726\n",
      "GPT2SdpaAttention_folded: 0.0959731936454773\u001b[0m\n",
      "\u001b[1;31m# 205 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 206 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011475160717964172\n",
      "Location: [0, 1, 45, 12]\n",
      "GPT2SdpaAttention_original: -0.17895498871803284\n",
      "GPT2SdpaAttention_folded: -0.190430149435997\u001b[0m\n",
      "\u001b[1;31m# 206 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 207 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01056671142578125\n",
      "Location: [0, 0, 37, 18]\n",
      "GPT2SdpaAttention_original: -0.3604477345943451\n",
      "GPT2SdpaAttention_folded: -0.34988102316856384\u001b[0m\n",
      "\u001b[1;31m# 207 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 208 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 208 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 209 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.019136905670166016\n",
      "Location: [0, 103, 329]\n",
      "LayerNorm_original: -1.713705062866211\n",
      "SOLayerNorm_folded: -1.694568157196045\u001b[0m\n",
      "\u001b[1;31m# 209 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 210 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01066640019416809\n",
      "Location: [0, 87, 1090]\n",
      "Conv1D_original: 0.26260125637054443\n",
      "Conv1D_folded: 0.25193485617637634\u001b[0m\n",
      "\u001b[1;31m# 210 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 211 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011759281158447266\n",
      "Location: [0, 80, 365]\n",
      "NewGELUActivation_original: 1.6605255603790283\n",
      "NewGELUActivation_folded: 1.648766279220581\u001b[0m\n",
      "\u001b[1;31m# 211 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 212 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.006757311522960663\n",
      "Location: [0, 37, 670]\n",
      "Conv1D_original: -0.08054829388856888\n",
      "Conv1D_folded: -0.08730560541152954\u001b[0m\n",
      "\u001b[1;31m# 212 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 213 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.006757311522960663\n",
      "Location: [0, 37, 670]\n",
      "Dropout_original: -0.08054829388856888\n",
      "Dropout_folded: -0.08730560541152954\u001b[0m\n",
      "\u001b[1;31m# 213 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 214 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.006757311522960663\n",
      "Location: [0, 37, 670]\n",
      "GPT2MLP_original: -0.08054829388856888\n",
      "GPT2MLP_folded: -0.08730560541152954\u001b[0m\n",
      "\u001b[1;31m# 214 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 215 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.023247256875038147\n",
      "Location: [0, 85, 57]\n",
      "GPT2Block_original: 0.24839721620082855\n",
      "GPT2Block_folded: 0.2716444730758667\u001b[0m\n",
      "\u001b[1;31m# 215 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 216 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.011475160717964172\n",
      "Location: [0, 1, 45, 12]\n",
      "GPT2Block_original: -0.17895498871803284\n",
      "GPT2Block_folded: -0.190430149435997\u001b[0m\n",
      "\u001b[1;31m# 216 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 217 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01056671142578125\n",
      "Location: [0, 0, 37, 18]\n",
      "GPT2Block_original: -0.3604477345943451\n",
      "GPT2Block_folded: -0.34988102316856384\u001b[0m\n",
      "\u001b[1;31m# 217 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 218 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 218 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 219 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.018994450569152832\n",
      "Location: [0, 108, 28]\n",
      "LayerNorm_original: -1.6408802270889282\n",
      "SOLayerNorm_folded: -1.659874677658081\u001b[0m\n",
      "\u001b[1;31m# 219 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with utils.HookManager(original_model, hook_original, None, list(original_model.modules())[1:]):\n",
    "    original_out = original_model(input_ids)\n",
    "\n",
    "with utils.HookManager(folded_model, hook_folded, None, list(folded_model.modules())[1:]):\n",
    "    folded_out = folded_model(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34m# 220 [ Test ] folded_out[0] ?= original_out[0]\u001b[0m\n",
      "\u001b[2m=== folded_out[0] ===\u001b[0m\n",
      "tensor([[[2.4871e-02, -1.3620e-01, -3.4582e-01,  ..., 5.6413e-03, 6.5393e-01, -4.3367e-01],\n",
      "         [5.1748e-01, 4.3117e-01, 7.8653e-01,  ..., -9.6986e-01, 8.7109e-01, -1.2818e+00],\n",
      "         [7.8329e-01, 7.4768e-01, -2.5939e-01,  ..., 8.3396e-03, 1.0855e+00, -5.6037e-01],\n",
      "         ...,\n",
      "         [1.9191e-01, 7.3121e-01, -6.8771e-01,  ..., -2.5439e-01, -3.4247e-01, -2.6406e-01],\n",
      "         [3.8035e-02, 6.7756e-01, -1.0164e+00,  ..., -1.9307e-01, 1.0875e+00, -6.1144e-01],\n",
      "         [4.3601e-02, 2.6367e+00, -8.0250e-01,  ..., -1.9235e-01, 1.4889e+00, 7.4541e-01]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "\u001b[2m=== original_out[0] ===\u001b[0m\n",
      "tensor([[[2.1712e-02, -1.4150e-01, -3.4320e-01,  ..., 3.9669e-03, 6.5941e-01, -4.2921e-01],\n",
      "         [5.1380e-01, 4.2810e-01, 7.8693e-01,  ..., -9.6909e-01, 8.7120e-01, -1.2768e+00],\n",
      "         [7.8011e-01, 7.4773e-01, -2.6297e-01,  ..., 6.7078e-03, 1.0873e+00, -5.5771e-01],\n",
      "         ...,\n",
      "         [1.8941e-01, 7.3671e-01, -6.8908e-01,  ..., -2.5222e-01, -3.3449e-01, -2.5983e-01],\n",
      "         [3.0730e-02, 6.8014e-01, -1.0200e+00,  ..., -1.9081e-01, 1.1007e+00, -6.1009e-01],\n",
      "         [3.8301e-02, 2.6348e+00, -8.0384e-01,  ..., -1.8258e-01, 1.4888e+00, 7.4053e-01]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "\u001b[1;33mMax diff: 0.018994450569152832\n",
      "Location: [0, 108, 28]\n",
      "folded_out[0]: -1.659874677658081\n",
      "original_out[0]: -1.6408802270889282\u001b[0m\n",
      "\u001b[1;31m# 220 [ Fail ] folded_out[0] != original_out[0]\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check.check_eq('folded_out[0]', 'original_out[0]', local_vars=locals(), abs_tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34m==== < Summary > ====\u001b[0m\n",
      "\u001b[1;31m# 0 [ Fail ]\u001b[0m Embedding_original != Embedding_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 1 [ Fail ]\u001b[0m Embedding_original != Embedding_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 2 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 3 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 4 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 5 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 6 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 7 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 8 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 9 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 10 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 11 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 12 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 13 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 14 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 15 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 16 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 17 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 18 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 19 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 20 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 21 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 22 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 23 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 24 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 25 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 26 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 27 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 28 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 29 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 30 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 31 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 32 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 33 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 34 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 35 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 36 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 37 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 38 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 39 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 40 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 41 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 42 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 43 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 44 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 45 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 46 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 47 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 48 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 49 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 50 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 51 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 52 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 53 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 54 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 55 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 56 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 57 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 58 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 59 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 60 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 61 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 62 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 63 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 64 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 65 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 66 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 67 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 68 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 69 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 70 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 71 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 72 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 73 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 74 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 75 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 76 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 77 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 78 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 79 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 80 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 81 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 82 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 83 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 84 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 85 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 86 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 87 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 88 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 89 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 90 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 91 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 92 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 93 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 94 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 95 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 96 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 97 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 98 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 99 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 100 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 101 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 102 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 103 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 104 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 105 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 106 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 107 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 108 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 109 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 110 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 111 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 112 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 113 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 114 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 115 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 116 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 117 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 118 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 119 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 120 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 121 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 122 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 123 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 124 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 125 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 126 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 127 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 128 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 129 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 130 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 131 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 132 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 133 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 134 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 135 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 136 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 137 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 138 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 139 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 140 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 141 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 142 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 143 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 144 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 145 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 146 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 147 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 148 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 149 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 150 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 151 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 152 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 153 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 154 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 155 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 156 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 157 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 158 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 159 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 160 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 161 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 162 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 163 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 164 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 165 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 166 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 167 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 168 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 169 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 170 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 171 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 172 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 173 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 174 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 175 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 176 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 177 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 178 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 179 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 180 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 181 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 182 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 183 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 184 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 185 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 186 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 187 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 188 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 189 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 190 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 191 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 192 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 193 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 194 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 195 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 196 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 197 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 198 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 199 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 200 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 201 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 202 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 203 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 204 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 205 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 206 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 207 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 208 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 209 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 210 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 211 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 212 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 213 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 214 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 215 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 216 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 217 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 218 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 219 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 220 [ Fail ]\u001b[0m folded_out[0] != original_out[0] \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "-------------------\n",
      "\u001b[1;34m(24/221) [\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;34m]\u001b[0m\n",
      "\u001b[1;34m==== </Summary > ====\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folded_model = GPT2Model(config).cuda()\n",
    "original_model = GPT2Model(config).cuda()\n",
    "\n",
    "folded_model.load_state_dict(original_model.state_dict())\n",
    "folded_model.eval()\n",
    "original_model.eval()\n",
    "\n",
    "folded_counter = utils.Counter()\n",
    "original_counter = utils.Counter()\n",
    "\n",
    "folded_hook_pre_fn, folded_hook_fn = utils.create_analyse_hook_fns(folded_counter, _print=False)\n",
    "original_hook_pre_fn, original_hook_fn = utils.create_analyse_hook_fns(original_counter, _print=False)\n",
    "\n",
    "with utils.HookManager(folded_model, folded_hook_fn, folded_hook_pre_fn):\n",
    "    folded_model(my_input_ids)\n",
    "\n",
    "with utils.HookManager(original_model, original_hook_fn, original_hook_pre_fn):\n",
    "    original_model(my_input_ids)\n",
    "\n",
    "for layer in folded_counter.center_modules:\n",
    "    modules.center_modules(layer)\n",
    "\n",
    "for layer in folded_counter.layernorms:\n",
    "    modules.replace_layer_norm_forward(layer, forward_fn=modules.soln_forward)\n",
    "\n",
    "for layer in original_counter.layernorms:\n",
    "    modules.replace_layer_norm_forward(layer, forward_fn=modules.myln_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    ProfilerStep*        49.87%       15.427s       100.00%       30.932s      41.243ms        7.369s        23.86%       30.883s      41.177ms           750  \n",
      "                                       aten::view         1.38%     426.227ms         1.38%     426.227ms       3.789us        1.031s         3.34%        1.031s       9.162us        112500  \n",
      "                                     aten::arange         0.21%      65.322ms         0.36%     110.509ms      73.673us      58.173ms         0.19%     119.659ms      79.773us          1500  \n",
      "                                      aten::empty         0.44%     136.180ms         0.44%     136.180ms       3.560us     501.080ms         1.62%     501.080ms      13.100us         38250  \n",
      "                                    aten::resize_         0.03%      10.678ms         0.03%      10.678ms       4.746us      29.750ms         0.10%      29.750ms      13.222us          2250  \n",
      "                                  aten::unsqueeze         0.07%      20.305ms         0.07%      21.947ms      29.263us      25.855ms         0.08%      33.653ms      44.871us           750  \n",
      "                                 aten::as_strided         0.46%     141.426ms         0.46%     141.426ms       1.418us     872.839ms         2.83%     872.839ms       8.750us         99750  \n",
      "                                  aten::embedding         0.32%      99.233ms         0.77%     239.345ms     159.563us      71.341ms         0.23%     259.348ms     172.899us          1500  \n",
      "                                    aten::reshape         0.10%      31.219ms         0.11%      34.266ms      22.844us      27.589ms         0.09%      45.022ms      30.015us          1500  \n",
      "                               aten::index_select         0.30%      92.913ms         0.33%     103.472ms      68.981us      87.289ms         0.28%     127.285ms      84.857us          1500  \n",
      "                                        aten::add         5.73%        1.772s         5.73%        1.772s      23.872us        2.436s         7.89%        2.436s      32.803us         74250  \n",
      "                                    aten::dropout         0.06%      18.893ms         0.06%      18.893ms       1.008us     105.604ms         0.34%     105.604ms       5.632us         18750  \n",
      "                         aten::linalg_vector_norm         1.87%     578.744ms         1.87%     578.744ms      30.866us     569.476ms         1.84%     569.476ms      30.372us         18750  \n",
      "                                        aten::mul         5.44%        1.683s         5.44%        1.683s      22.893us        2.643s         8.56%        2.643s      35.957us         73500  \n",
      "                                        aten::div         1.37%     424.577ms         1.37%     424.577ms      22.644us     578.315ms         1.87%     578.315ms      30.843us         18750  \n",
      "                                      aten::addmm         7.99%        2.471s         7.99%        2.471s      68.637us        8.269s        26.78%        8.269s     229.698us         36000  \n",
      "                                      aten::split         2.15%     665.020ms         7.08%        2.190s     243.367us     279.102ms         0.90%        1.600s     177.749us          9000  \n",
      "                                     aten::narrow         2.71%     838.764ms         4.93%        1.525s      56.492us     796.704ms         2.58%        1.321s      48.913us         27000  \n",
      "                                      aten::slice         2.08%     643.748ms         2.22%     686.516ms      25.427us     356.382ms         1.15%     523.939ms      19.405us         27000  \n",
      "                                    aten::permute         2.74%     846.982ms         2.86%     885.976ms      32.814us     683.933ms         2.21%     955.410ms      35.386us         27000  \n",
      "               aten::scaled_dot_product_attention         0.97%     300.268ms        12.10%        3.743s     415.833us     244.236ms         0.79%        3.484s     387.143us          9000  \n",
      "    aten::_scaled_dot_product_efficient_attention         4.11%        1.271s        11.13%        3.442s     382.470us     593.159ms         1.92%        3.240s     360.006us          9000  \n",
      "                                  aten::transpose         3.62%        1.119s         3.81%        1.177s      26.157us     896.161ms         2.90%        1.322s      29.382us         45000  \n",
      "               aten::_efficient_attention_forward         3.57%        1.105s         3.98%        1.232s     136.841us        1.129s         3.65%        1.599s     177.685us          9000  \n",
      "                                        aten::pow         1.76%     544.245ms         1.80%     557.259ms      61.918us     735.282ms         2.38%     804.549ms      89.394us          9000  \n",
      "                                aten::result_type         0.02%       5.769ms         0.02%       5.769ms       0.641us      32.509ms         0.11%      32.509ms       3.612us          9000  \n",
      "                                         aten::to         0.02%       7.246ms         0.02%       7.246ms       0.805us      36.758ms         0.12%      36.758ms       4.084us          9000  \n",
      "                                       aten::tanh         0.60%     186.071ms         0.60%     186.071ms      20.675us     424.528ms         1.37%     424.528ms      47.170us          9000  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 30.932s\n",
      "Self CUDA time total: 30.883s\n",
      "\n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    ProfilerStep*        50.07%       18.868s       100.00%       37.680s      50.240ms        9.555s        25.33%       37.727s      50.303ms           750  \n",
      "                                       aten::view         1.25%     472.296ms         1.25%     472.296ms       4.198us        1.203s         3.19%        1.203s      10.696us        112500  \n",
      "                                     aten::arange         0.30%     112.952ms         0.49%     184.915ms     123.277us      79.498ms         0.21%     146.163ms      97.442us          1500  \n",
      "                                      aten::empty         0.58%     217.245ms         0.58%     217.245ms       3.811us     773.313ms         2.05%     773.313ms      13.567us         57000  \n",
      "                                    aten::resize_         0.24%      89.097ms         0.24%      89.097ms       4.243us     261.686ms         0.69%     261.686ms      12.461us         21000  \n",
      "                                  aten::unsqueeze         0.06%      21.471ms         0.06%      23.549ms      31.398us      26.737ms         0.07%      35.893ms      47.857us           750  \n",
      "                                 aten::as_strided         0.41%     154.022ms         0.41%     154.022ms       1.544us     988.658ms         2.62%     988.658ms       9.911us         99750  \n",
      "                                  aten::embedding         0.31%     115.927ms         0.73%     276.948ms     184.632us      84.131ms         0.22%     300.830ms     200.553us          1500  \n",
      "                                    aten::reshape         0.10%      36.823ms         0.11%      39.873ms      26.582us      33.052ms         0.09%      52.344ms      34.896us          1500  \n",
      "                               aten::index_select         0.28%     106.689ms         0.31%     118.233ms      78.822us      90.969ms         0.24%     142.593ms      95.062us          1500  \n",
      "                                        aten::add         5.03%        1.895s         5.03%        1.895s      25.528us        2.775s         7.36%        2.775s      37.379us         74250  \n",
      "                                    aten::dropout         0.06%      20.859ms         0.06%      20.859ms       1.112us     127.022ms         0.34%     127.022ms       6.775us         18750  \n",
      "                                       aten::mean         1.51%     568.880ms         1.51%     568.880ms      30.340us     565.771ms         1.50%     565.771ms      30.174us         18750  \n",
      "                                        aten::var         4.91%        1.852s         5.31%        2.000s     106.674us        1.077s         2.86%        1.521s      81.146us         18750  \n",
      "                                        aten::sub         1.43%     537.950ms         1.43%     537.950ms      28.691us     624.007ms         1.65%     624.007ms      33.280us         18750  \n",
      "                                       aten::sqrt         1.16%     438.207ms         1.16%     438.207ms      23.371us     553.893ms         1.47%     553.893ms      29.541us         18750  \n",
      "                                        aten::div         1.33%     502.506ms         1.33%     502.506ms      26.800us     628.070ms         1.66%     628.070ms      33.497us         18750  \n",
      "                                        aten::mul         3.42%        1.288s         3.42%        1.288s      23.529us        2.528s         6.70%        2.528s      46.173us         54750  \n",
      "                                      aten::addmm         7.14%        2.690s         7.14%        2.690s      74.711us        8.936s        23.69%        8.936s     248.228us         36000  \n",
      "                                      aten::split         1.71%     645.335ms         5.06%        1.906s     211.743us     323.368ms         0.86%        1.782s     197.998us          9000  \n",
      "                                     aten::narrow         1.64%     618.647ms         3.34%        1.260s      46.680us     861.434ms         2.28%        1.459s      54.023us         27000  \n",
      "                                      aten::slice         1.58%     594.049ms         1.70%     641.706ms      23.767us     403.677ms         1.07%     597.180ms      22.118us         27000  \n",
      "                                    aten::permute         2.24%     845.046ms         2.35%     886.320ms      32.827us     721.119ms         1.91%        1.048s      38.814us         27000  \n",
      "               aten::scaled_dot_product_attention         1.24%     468.410ms        10.90%        4.108s     456.499us     242.453ms         0.64%        3.786s     420.647us          9000  \n",
      "    aten::_scaled_dot_product_efficient_attention         3.34%        1.260s         9.66%        3.640s     404.453us     676.687ms         1.79%        3.543s     393.708us          9000  \n",
      "                                  aten::transpose         2.93%        1.105s         3.10%        1.168s      25.965us        1.034s         2.74%        1.493s      33.183us         45000  \n",
      "               aten::_efficient_attention_forward         3.54%        1.334s         3.90%        1.471s     163.457us        1.184s         3.14%        1.701s     189.051us          9000  \n",
      "                                        aten::pow         1.60%     601.303ms         1.64%     616.670ms      68.519us     782.286ms         2.07%     881.323ms      97.925us          9000  \n",
      "                                aten::result_type         0.02%       6.713ms         0.02%       6.713ms       0.746us      40.302ms         0.11%      40.302ms       4.478us          9000  \n",
      "                                         aten::to         0.02%       8.655ms         0.02%       8.655ms       0.962us      58.735ms         0.16%      58.735ms       6.526us          9000  \n",
      "                                       aten::tanh         0.54%     205.244ms         0.54%     205.244ms      22.805us     487.243ms         1.29%     487.243ms      54.138us          9000  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 37.680s\n",
      "Self CUDA time total: 37.727s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity, schedule\n",
    "\n",
    "my_schedule = schedule(\n",
    "    wait=100,\n",
    "    warmup=50,\n",
    "    active=250,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    with profile(\n",
    "        activities=[\n",
    "            ProfilerActivity.CPU, ProfilerActivity.CUDA\n",
    "        ],\n",
    "        schedule=my_schedule\n",
    "    ) as prof:\n",
    "        with record_function(\"folded_model_inference\"):\n",
    "            for _ in range(1200):\n",
    "                folded_model(input_ids)\n",
    "                prof.step()\n",
    "    print(prof.key_averages().table())\n",
    "    prof.export_chrome_trace(\"tmp/folded_trace.json\")\n",
    "\n",
    "    with profile(\n",
    "        activities=[\n",
    "            ProfilerActivity.CPU, ProfilerActivity.CUDA\n",
    "        ],\n",
    "        schedule=my_schedule\n",
    "    ) as prof:\n",
    "        with record_function(\"original_model_inference\"):\n",
    "            for _ in range(1200):\n",
    "                original_model(input_ids)\n",
    "                prof.step()\n",
    "    print(prof.key_averages().table())\n",
    "    prof.export_chrome_trace(\"tmp/original_trace.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
