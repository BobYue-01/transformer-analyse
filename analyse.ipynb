{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2Model, GPT2Tokenizer\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "config = GPT2Config()\n",
    "my_ln_model = GPT2Model(config).cuda()\n",
    "my_so_ln_model = GPT2Model(config).cuda()\n",
    "\n",
    "my_so_ln_model.load_state_dict(my_ln_model.state_dict())\n",
    "my_ln_model.eval()\n",
    "my_so_ln_model.eval()\n",
    "\n",
    "import utils\n",
    "counter = utils.Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <  GPT2Model >\n",
      "   wte : Embedding\n",
      "   wpe : Embedding\n",
      "   drop : Dropout\n",
      "   h : ModuleList\n",
      "   ln_f : LayerNorm\n",
      "   <- MetadataTensor False (1, 128) 0 set()\n",
      "   <  Embedding >\n",
      "     <- MetadataTensor False (1, 128) 0 set()\n",
      "     -> MetadataTensor True (1, 128, 768) 1 {Embedding(50257, 768)}\n",
      "   </ Embedding >\n",
      "   <  Embedding >\n",
      "     <- Tensor None (1, 128) 0 set()\n",
      "     -> MetadataTensor True (1, 128, 768) 1 {Embedding(1024, 768)}\n",
      "   </ Embedding >\n",
      "   <  Dropout >\n",
      "     <- MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "     -> MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "   </ Dropout >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 3 {Embedding(50257, 768), Embedding(1024, 768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 4 {Conv1D(nf=768, nx=3072), Embedding(50257, 768), Embedding(1024, 768), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 4 {Conv1D(nf=768, nx=3072), Embedding(50257, 768), Embedding(1024, 768), Conv1D(nf=768, nx=768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 4 {Conv1D(nf=768, nx=3072), Embedding(50257, 768), Embedding(1024, 768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 5 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Embedding(50257, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 6 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Embedding(50257, 768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 6 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Embedding(50257, 768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 6 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Embedding(50257, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 7 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 8 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 8 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 8 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 9 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 10 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 10 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 10 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 11 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 12 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 12 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 12 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 13 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 14 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 14 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 14 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 15 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 16 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 16 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 16 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 17 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 18 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 18 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 18 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 19 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 20 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 20 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 20 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 21 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 22 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 22 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 22 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 23 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 24 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 24 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 24 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 25 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 26 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  LayerNorm >\n",
      "     <- MetadataTensor True (1, 128, 768) 26 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "   </ LayerNorm >\n",
      "LayerNorm: 25\n",
      "Foldable: 25\n",
      "Center modules: {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BobYu\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\_tensor.py:1437: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "hook_pre_fn, hook_fn = utils.create_analyse_hook_fns(counter)\n",
    "\n",
    "input_ids = torch.randint(0, 1000, (1, 128)).cuda()\n",
    "my_input_ids = utils.MetadataTensor(input_ids, centered=False).cuda()\n",
    "\n",
    "with utils.HookManager(my_so_ln_model, hook_fn, hook_pre_fn):\n",
    "    my_so_ln_model(my_input_ids)\n",
    "\n",
    "print('LayerNorm:', counter.ln_cnt)\n",
    "print('Foldable:', counter.foldable_cnt)\n",
    "print('Center modules:', counter.center_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules\n",
    "\n",
    "for layer in counter.layernorms:\n",
    "    modules.replace_layer_norm_forward(layer)\n",
    "\n",
    "for layer in counter.center_modules:\n",
    "    modules.center_modules(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_queue = []\n",
    "check = utils.Check()\n",
    "replace = True\n",
    "\n",
    "def hook_original(module, input, output):\n",
    "    name = module.__class__.__name__\n",
    "    output_queue.append((output, name))\n",
    "\n",
    "    # if isinstance(output, tuple):\n",
    "    #     output = output[0]\n",
    "\n",
    "    # with torch._tensor_str.printoptions(precision=10, sci_mode=True):\n",
    "    #     len_shape = len(output.shape)\n",
    "    #     index = tuple([0] * (len_shape - 2) + [slice(None, 4), slice(None, 4)])\n",
    "    #     print(module.__class__.__name__, output[index])\n",
    "\n",
    "def check_close_and_replace(tensor_a, tensor_b, check: utils.Check, tensor_a_str, tensor_b_str):\n",
    "    check.hide_val()\n",
    "    locals()[tensor_a_str] = tensor_a\n",
    "    locals()[tensor_b_str] = tensor_b\n",
    "    if check.check_eq(tensor_a_str, tensor_b_str, abs_tol=1e-2, local_vars=locals()):\n",
    "        if replace and isinstance(tensor_a, torch.Tensor) and isinstance(tensor_b, torch.Tensor):\n",
    "            tensor_b.data = tensor_a.data\n",
    "    check.show_val()\n",
    "\n",
    "def apply_func_to_nested_tuple_pair(t1, t2, func, *args, **kwargs):\n",
    "    if isinstance(t1, tuple) and isinstance(t2, tuple):\n",
    "        return tuple(apply_func_to_nested_tuple_pair(x1, x2, func, *args, **kwargs) for x1, x2 in zip(t1, t2))\n",
    "    else:\n",
    "        return func(t1, t2, *args, **kwargs)\n",
    "\n",
    "def hook_folded(module, input, output):\n",
    "    folded_name = module.__class__.__name__ + '_folded'\n",
    "    original_output, original_name = output_queue.pop(0)\n",
    "    original_name += '_original'\n",
    "    apply_func_to_nested_tuple_pair(original_output, output, check_close_and_replace, check, original_name, folded_name)\n",
    "\n",
    "    # if isinstance(output, tuple):\n",
    "    #     output0 = output[0]\n",
    "\n",
    "    # with torch._tensor_str.printoptions(precision=10, sci_mode=True):\n",
    "    #     len_shape = len(output0.shape)\n",
    "    #     index = tuple([0] * (len_shape - 2) + [slice(None, 4), slice(None, 4)])\n",
    "    #     print(module.__class__.__name__, output0[index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34m# 0 [ Test ] Embedding_original ?= Embedding_folded\u001b[0m\n",
      "\u001b[1;32m# 0 [ Pass ] Embedding_original == Embedding_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 1 [ Test ] Embedding_original ?= Embedding_folded\u001b[0m\n",
      "\u001b[1;32m# 1 [ Pass ] Embedding_original == Embedding_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 2 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;32m# 2 [ Pass ] Dropout_original == Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 3 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.13479137420654297\n",
      "Location: [0, 37, 709]\n",
      "LayerNorm_original: 2.488018274307251\n",
      "SOLayerNorm_folded: 2.353226900100708\u001b[0m\n",
      "\u001b[1;31m# 3 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 4 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2537236213684082\n",
      "Location: [0, 37, 88]\n",
      "Conv1D_original: -0.9486808776855469\n",
      "Conv1D_folded: -1.202404499053955\u001b[0m\n",
      "\u001b[1;31m# 4 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 5 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 5 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 6 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;32m# 6 [ Pass ] Dropout_original == Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 7 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 7 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 8 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20648691058158875\n",
      "Location: [0, 1, 37, 60]\n",
      "GPT2SdpaAttention_original: 0.6662099957466125\n",
      "GPT2SdpaAttention_folded: 0.4597230851650238\u001b[0m\n",
      "\u001b[1;31m# 8 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 9 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2226211428642273\n",
      "Location: [0, 7, 37, 0]\n",
      "GPT2SdpaAttention_original: -0.3416423201560974\n",
      "GPT2SdpaAttention_folded: -0.5642634630203247\u001b[0m\n",
      "\u001b[1;31m# 9 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 10 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 10 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 11 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.14541029930114746\n",
      "Location: [0, 37, 248]\n",
      "LayerNorm_original: 2.880509853363037\n",
      "SOLayerNorm_folded: 2.7350995540618896\u001b[0m\n",
      "\u001b[1;31m# 11 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 12 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.24477648735046387\n",
      "Location: [0, 37, 2774]\n",
      "Conv1D_original: -0.7181519865989685\n",
      "Conv1D_folded: -0.47337549924850464\u001b[0m\n",
      "\u001b[1;31m# 12 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 13 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21388769149780273\n",
      "Location: [0, 37, 1664]\n",
      "NewGELUActivation_original: 0.6575090885162354\n",
      "NewGELUActivation_folded: 0.8713967800140381\u001b[0m\n",
      "\u001b[1;31m# 13 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 14 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.03602820634841919\n",
      "Location: [0, 37, 324]\n",
      "Conv1D_original: -0.13701024651527405\n",
      "Conv1D_folded: -0.10098204016685486\u001b[0m\n",
      "\u001b[1;31m# 14 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 15 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.03602820634841919\n",
      "Location: [0, 37, 324]\n",
      "Dropout_original: -0.13701024651527405\n",
      "Dropout_folded: -0.10098204016685486\u001b[0m\n",
      "\u001b[1;31m# 15 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 16 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.03602820634841919\n",
      "Location: [0, 37, 324]\n",
      "GPT2MLP_original: -0.13701024651527405\n",
      "GPT2MLP_folded: -0.10098204016685486\u001b[0m\n",
      "\u001b[1;31m# 16 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 17 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.03602819889783859\n",
      "Location: [0, 37, 324]\n",
      "GPT2Block_original: -0.1414826214313507\n",
      "GPT2Block_folded: -0.10545442253351212\u001b[0m\n",
      "\u001b[1;31m# 17 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 18 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20648691058158875\n",
      "Location: [0, 1, 37, 60]\n",
      "GPT2Block_original: 0.6662099957466125\n",
      "GPT2Block_folded: 0.4597230851650238\u001b[0m\n",
      "\u001b[1;31m# 18 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 19 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2226211428642273\n",
      "Location: [0, 7, 37, 0]\n",
      "GPT2Block_original: -0.3416423201560974\n",
      "GPT2Block_folded: -0.5642634630203247\u001b[0m\n",
      "\u001b[1;31m# 19 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 20 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 20 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 21 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.4807826280593872\n",
      "Location: [0, 37, 13]\n",
      "LayerNorm_original: -0.3345334529876709\n",
      "SOLayerNorm_folded: -0.8153160810470581\u001b[0m\n",
      "\u001b[1;31m# 21 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 22 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.25433820486068726\n",
      "Location: [0, 37, 812]\n",
      "Conv1D_original: 0.8204203844070435\n",
      "Conv1D_folded: 0.5660821795463562\u001b[0m\n",
      "\u001b[1;31m# 22 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 23 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0163465216755867\n",
      "Location: [0, 0, 514]\n",
      "Conv1D_original: 0.034993212670087814\n",
      "Conv1D_folded: 0.051339734345674515\u001b[0m\n",
      "\u001b[1;31m# 23 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 24 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0163465216755867\n",
      "Location: [0, 0, 514]\n",
      "Dropout_original: 0.034993212670087814\n",
      "Dropout_folded: 0.051339734345674515\u001b[0m\n",
      "\u001b[1;31m# 24 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 25 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0163465216755867\n",
      "Location: [0, 0, 514]\n",
      "GPT2SdpaAttention_original: 0.034993212670087814\n",
      "GPT2SdpaAttention_folded: 0.051339734345674515\u001b[0m\n",
      "\u001b[1;31m# 25 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 26 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.25433820486068726\n",
      "Location: [0, 0, 37, 44]\n",
      "GPT2SdpaAttention_original: 0.8204203844070435\n",
      "GPT2SdpaAttention_folded: 0.5660821795463562\u001b[0m\n",
      "\u001b[1;31m# 26 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 27 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2511337995529175\n",
      "Location: [0, 2, 37, 58]\n",
      "GPT2SdpaAttention_original: 0.3636626601219177\n",
      "GPT2SdpaAttention_folded: 0.6147964596748352\u001b[0m\n",
      "\u001b[1;31m# 27 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 28 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 28 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 29 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.45684778690338135\n",
      "Location: [0, 37, 13]\n",
      "LayerNorm_original: 0.3024059534072876\n",
      "SOLayerNorm_folded: -0.15444184839725494\u001b[0m\n",
      "\u001b[1;31m# 29 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 30 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2771782875061035\n",
      "Location: [0, 37, 358]\n",
      "Conv1D_original: -0.14830869436264038\n",
      "Conv1D_folded: 0.12886957824230194\u001b[0m\n",
      "\u001b[1;31m# 30 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 31 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2042132019996643\n",
      "Location: [0, 37, 102]\n",
      "NewGELUActivation_original: 0.46975284814834595\n",
      "NewGELUActivation_folded: 0.6739660501480103\u001b[0m\n",
      "\u001b[1;31m# 31 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 32 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.034169092774391174\n",
      "Location: [0, 10, 279]\n",
      "Conv1D_original: -0.027229219675064087\n",
      "Conv1D_folded: 0.006939873099327087\u001b[0m\n",
      "\u001b[1;31m# 32 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 33 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.034169092774391174\n",
      "Location: [0, 10, 279]\n",
      "Dropout_original: -0.027229219675064087\n",
      "Dropout_folded: 0.006939873099327087\u001b[0m\n",
      "\u001b[1;31m# 33 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 34 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.034169092774391174\n",
      "Location: [0, 10, 279]\n",
      "GPT2MLP_original: -0.027229219675064087\n",
      "GPT2MLP_folded: 0.006939873099327087\u001b[0m\n",
      "\u001b[1;31m# 34 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 35 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0467352420091629\n",
      "Location: [0, 37, 324]\n",
      "GPT2Block_original: -0.031913235783576965\n",
      "GPT2Block_folded: 0.014822006225585938\u001b[0m\n",
      "\u001b[1;31m# 35 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 36 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.25433820486068726\n",
      "Location: [0, 0, 37, 44]\n",
      "GPT2Block_original: 0.8204203844070435\n",
      "GPT2Block_folded: 0.5660821795463562\u001b[0m\n",
      "\u001b[1;31m# 36 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 37 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2511337995529175\n",
      "Location: [0, 2, 37, 58]\n",
      "GPT2Block_original: 0.3636626601219177\n",
      "GPT2Block_folded: 0.6147964596748352\u001b[0m\n",
      "\u001b[1;31m# 37 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 38 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 38 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 39 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.4422612190246582\n",
      "Location: [0, 37, 500]\n",
      "LayerNorm_original: 2.6713199615478516\n",
      "SOLayerNorm_folded: 2.2290587425231934\u001b[0m\n",
      "\u001b[1;31m# 39 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 40 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.22702491283416748\n",
      "Location: [0, 10, 1701]\n",
      "Conv1D_original: 0.19881054759025574\n",
      "Conv1D_folded: 0.4258354604244232\u001b[0m\n",
      "\u001b[1;31m# 40 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 41 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01664125919342041\n",
      "Location: [0, 0, 550]\n",
      "Conv1D_original: 0.17513972520828247\n",
      "Conv1D_folded: 0.15849846601486206\u001b[0m\n",
      "\u001b[1;31m# 41 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 42 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01664125919342041\n",
      "Location: [0, 0, 550]\n",
      "Dropout_original: 0.17513972520828247\n",
      "Dropout_folded: 0.15849846601486206\u001b[0m\n",
      "\u001b[1;31m# 42 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 43 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01664125919342041\n",
      "Location: [0, 0, 550]\n",
      "GPT2SdpaAttention_original: 0.17513972520828247\n",
      "GPT2SdpaAttention_folded: 0.15849846601486206\u001b[0m\n",
      "\u001b[1;31m# 43 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 44 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.22241726517677307\n",
      "Location: [0, 3, 37, 59]\n",
      "GPT2SdpaAttention_original: 0.367643803358078\n",
      "GPT2SdpaAttention_folded: 0.5900610685348511\u001b[0m\n",
      "\u001b[1;31m# 44 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 45 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.22702491283416748\n",
      "Location: [0, 2, 10, 37]\n",
      "GPT2SdpaAttention_original: 0.19881054759025574\n",
      "GPT2SdpaAttention_folded: 0.4258354604244232\u001b[0m\n",
      "\u001b[1;31m# 45 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 46 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 46 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 47 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.4281008243560791\n",
      "Location: [0, 37, 500]\n",
      "LayerNorm_original: 2.787440538406372\n",
      "SOLayerNorm_folded: 2.359339714050293\u001b[0m\n",
      "\u001b[1;31m# 47 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 48 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.24415534734725952\n",
      "Location: [0, 10, 1582]\n",
      "Conv1D_original: 0.6570523381233215\n",
      "Conv1D_folded: 0.901207685470581\u001b[0m\n",
      "\u001b[1;31m# 48 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 49 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2464207410812378\n",
      "Location: [0, 10, 1582]\n",
      "NewGELUActivation_original: 0.4890822172164917\n",
      "NewGELUActivation_folded: 0.7355029582977295\u001b[0m\n",
      "\u001b[1;31m# 49 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 50 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.03771182522177696\n",
      "Location: [0, 37, 550]\n",
      "Conv1D_original: -0.06088332459330559\n",
      "Conv1D_folded: -0.09859514981508255\u001b[0m\n",
      "\u001b[1;31m# 50 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 51 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.03771182522177696\n",
      "Location: [0, 37, 550]\n",
      "Dropout_original: -0.06088332459330559\n",
      "Dropout_folded: -0.09859514981508255\u001b[0m\n",
      "\u001b[1;31m# 51 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 52 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.03771182522177696\n",
      "Location: [0, 37, 550]\n",
      "GPT2MLP_original: -0.06088332459330559\n",
      "GPT2MLP_folded: -0.09859514981508255\u001b[0m\n",
      "\u001b[1;31m# 52 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 53 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.056643277406692505\n",
      "Location: [0, 37, 393]\n",
      "GPT2Block_original: 0.2274172008037567\n",
      "GPT2Block_folded: 0.2840604782104492\u001b[0m\n",
      "\u001b[1;31m# 53 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 54 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.22241726517677307\n",
      "Location: [0, 3, 37, 59]\n",
      "GPT2Block_original: 0.367643803358078\n",
      "GPT2Block_folded: 0.5900610685348511\u001b[0m\n",
      "\u001b[1;31m# 54 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 55 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.22702491283416748\n",
      "Location: [0, 2, 10, 37]\n",
      "GPT2Block_original: 0.19881054759025574\n",
      "GPT2Block_folded: 0.4258354604244232\u001b[0m\n",
      "\u001b[1;31m# 55 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 56 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 56 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 57 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.39691436290740967\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.6492470502853394\n",
      "SOLayerNorm_folded: 2.046161413192749\u001b[0m\n",
      "\u001b[1;31m# 57 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 58 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.25852322578430176\n",
      "Location: [0, 37, 655]\n",
      "Conv1D_original: 1.2634811401367188\n",
      "Conv1D_folded: 1.004957914352417\u001b[0m\n",
      "\u001b[1;31m# 58 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 59 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.016615217551589012\n",
      "Location: [0, 0, 764]\n",
      "Conv1D_original: 0.04526965320110321\n",
      "Conv1D_folded: 0.0286544356495142\u001b[0m\n",
      "\u001b[1;31m# 59 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 60 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.016615217551589012\n",
      "Location: [0, 0, 764]\n",
      "Dropout_original: 0.04526965320110321\n",
      "Dropout_folded: 0.0286544356495142\u001b[0m\n",
      "\u001b[1;31m# 60 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 61 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.016615217551589012\n",
      "Location: [0, 0, 764]\n",
      "GPT2SdpaAttention_original: 0.04526965320110321\n",
      "GPT2SdpaAttention_folded: 0.0286544356495142\u001b[0m\n",
      "\u001b[1;31m# 61 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 62 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.22529011964797974\n",
      "Location: [0, 4, 37, 9]\n",
      "GPT2SdpaAttention_original: 0.7387946844100952\n",
      "GPT2SdpaAttention_folded: 0.5135045647621155\u001b[0m\n",
      "\u001b[1;31m# 62 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 63 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.250949501991272\n",
      "Location: [0, 6, 37, 51]\n",
      "GPT2SdpaAttention_original: -0.37682855129241943\n",
      "GPT2SdpaAttention_folded: -0.12587904930114746\u001b[0m\n",
      "\u001b[1;31m# 63 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 64 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 64 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 65 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.3868776559829712\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.1354718208312988\n",
      "SOLayerNorm_folded: 1.52234947681427\u001b[0m\n",
      "\u001b[1;31m# 65 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 66 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.23332124948501587\n",
      "Location: [0, 37, 220]\n",
      "Conv1D_original: 0.9105349183082581\n",
      "Conv1D_folded: 0.6772136688232422\u001b[0m\n",
      "\u001b[1;31m# 66 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 67 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.23691469430923462\n",
      "Location: [0, 37, 220]\n",
      "NewGELUActivation_original: 0.7453609704971313\n",
      "NewGELUActivation_folded: 0.5084462761878967\u001b[0m\n",
      "\u001b[1;31m# 67 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 68 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02931980788707733\n",
      "Location: [0, 37, 678]\n",
      "Conv1D_original: -0.0863674134016037\n",
      "Conv1D_folded: -0.11568722128868103\u001b[0m\n",
      "\u001b[1;31m# 68 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 69 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02931980788707733\n",
      "Location: [0, 37, 678]\n",
      "Dropout_original: -0.0863674134016037\n",
      "Dropout_folded: -0.11568722128868103\u001b[0m\n",
      "\u001b[1;31m# 69 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 70 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02931980788707733\n",
      "Location: [0, 37, 678]\n",
      "GPT2MLP_original: -0.0863674134016037\n",
      "GPT2MLP_folded: -0.11568722128868103\u001b[0m\n",
      "\u001b[1;31m# 70 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 71 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.058471404016017914\n",
      "Location: [0, 37, 199]\n",
      "GPT2Block_original: -0.042778030037879944\n",
      "GPT2Block_folded: 0.01569337397813797\u001b[0m\n",
      "\u001b[1;31m# 71 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 72 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.22529011964797974\n",
      "Location: [0, 4, 37, 9]\n",
      "GPT2Block_original: 0.7387946844100952\n",
      "GPT2Block_folded: 0.5135045647621155\u001b[0m\n",
      "\u001b[1;31m# 72 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 73 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.250949501991272\n",
      "Location: [0, 6, 37, 51]\n",
      "GPT2Block_original: -0.37682855129241943\n",
      "GPT2Block_folded: -0.12587904930114746\u001b[0m\n",
      "\u001b[1;31m# 73 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 74 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 74 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 75 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.3480868339538574\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.6443493366241455\n",
      "SOLayerNorm_folded: 1.992436170578003\u001b[0m\n",
      "\u001b[1;31m# 75 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 76 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.24020057916641235\n",
      "Location: [0, 37, 2013]\n",
      "Conv1D_original: 0.6604360342025757\n",
      "Conv1D_folded: 0.42023545503616333\u001b[0m\n",
      "\u001b[1;31m# 76 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 77 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01550370454788208\n",
      "Location: [0, 0, 285]\n",
      "Conv1D_original: 0.08805541694164276\n",
      "Conv1D_folded: 0.07255171239376068\u001b[0m\n",
      "\u001b[1;31m# 77 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 78 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01550370454788208\n",
      "Location: [0, 0, 285]\n",
      "Dropout_original: 0.08805541694164276\n",
      "Dropout_folded: 0.07255171239376068\u001b[0m\n",
      "\u001b[1;31m# 78 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 79 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01550370454788208\n",
      "Location: [0, 0, 285]\n",
      "GPT2SdpaAttention_original: 0.08805541694164276\n",
      "GPT2SdpaAttention_folded: 0.07255171239376068\u001b[0m\n",
      "\u001b[1;31m# 79 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 80 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.18540242314338684\n",
      "Location: [0, 7, 37, 36]\n",
      "GPT2SdpaAttention_original: -0.16468536853790283\n",
      "GPT2SdpaAttention_folded: 0.02071705460548401\u001b[0m\n",
      "\u001b[1;31m# 80 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 81 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.24020057916641235\n",
      "Location: [0, 7, 37, 29]\n",
      "GPT2SdpaAttention_original: 0.6604360342025757\n",
      "GPT2SdpaAttention_folded: 0.42023545503616333\u001b[0m\n",
      "\u001b[1;31m# 81 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 82 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 82 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 83 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.36273670196533203\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 2.0895328521728516\n",
      "SOLayerNorm_folded: 2.4522695541381836\u001b[0m\n",
      "\u001b[1;31m# 83 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 84 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21865567564964294\n",
      "Location: [0, 37, 2280]\n",
      "Conv1D_original: -0.2982686161994934\n",
      "Conv1D_folded: -0.07961294054985046\u001b[0m\n",
      "\u001b[1;31m# 84 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 85 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21601170301437378\n",
      "Location: [0, 37, 1365]\n",
      "NewGELUActivation_original: 0.7831763029098511\n",
      "NewGELUActivation_folded: 0.5671645998954773\u001b[0m\n",
      "\u001b[1;31m# 85 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 86 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02699793502688408\n",
      "Location: [0, 37, 478]\n",
      "Conv1D_original: 0.013116016052663326\n",
      "Conv1D_folded: 0.04011395201086998\u001b[0m\n",
      "\u001b[1;31m# 86 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 87 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02699793502688408\n",
      "Location: [0, 37, 478]\n",
      "Dropout_original: 0.013116016052663326\n",
      "Dropout_folded: 0.04011395201086998\u001b[0m\n",
      "\u001b[1;31m# 87 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 88 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02699793502688408\n",
      "Location: [0, 37, 478]\n",
      "GPT2MLP_original: 0.013116016052663326\n",
      "GPT2MLP_folded: 0.04011395201086998\u001b[0m\n",
      "\u001b[1;31m# 88 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 89 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.07412208616733551\n",
      "Location: [0, 37, 183]\n",
      "GPT2Block_original: -0.003182530403137207\n",
      "GPT2Block_folded: -0.07730461657047272\u001b[0m\n",
      "\u001b[1;31m# 89 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 90 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.18540242314338684\n",
      "Location: [0, 7, 37, 36]\n",
      "GPT2Block_original: -0.16468536853790283\n",
      "GPT2Block_folded: 0.02071705460548401\u001b[0m\n",
      "\u001b[1;31m# 90 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 91 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.24020057916641235\n",
      "Location: [0, 7, 37, 29]\n",
      "GPT2Block_original: 0.6604360342025757\n",
      "GPT2Block_folded: 0.42023545503616333\u001b[0m\n",
      "\u001b[1;31m# 91 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 92 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 92 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 93 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.4200724959373474\n",
      "Location: [0, 37, 183]\n",
      "LayerNorm_original: -0.004082033410668373\n",
      "SOLayerNorm_folded: -0.42415452003479004\u001b[0m\n",
      "\u001b[1;31m# 93 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 94 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.22449195384979248\n",
      "Location: [0, 37, 1990]\n",
      "Conv1D_original: 0.5013483762741089\n",
      "Conv1D_folded: 0.2768564224243164\u001b[0m\n",
      "\u001b[1;31m# 94 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 95 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01939927041530609\n",
      "Location: [0, 0, 188]\n",
      "Conv1D_original: -0.051359161734580994\n",
      "Conv1D_folded: -0.07075843214988708\u001b[0m\n",
      "\u001b[1;31m# 95 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 96 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01939927041530609\n",
      "Location: [0, 0, 188]\n",
      "Dropout_original: -0.051359161734580994\n",
      "Dropout_folded: -0.07075843214988708\u001b[0m\n",
      "\u001b[1;31m# 96 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 97 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01939927041530609\n",
      "Location: [0, 0, 188]\n",
      "GPT2SdpaAttention_original: -0.051359161734580994\n",
      "GPT2SdpaAttention_folded: -0.07075843214988708\u001b[0m\n",
      "\u001b[1;31m# 97 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 98 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20347344875335693\n",
      "Location: [0, 0, 37, 61]\n",
      "GPT2SdpaAttention_original: 0.4238986074924469\n",
      "GPT2SdpaAttention_folded: 0.22042515873908997\u001b[0m\n",
      "\u001b[1;31m# 98 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 99 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.22449195384979248\n",
      "Location: [0, 7, 37, 6]\n",
      "GPT2SdpaAttention_original: 0.5013483762741089\n",
      "GPT2SdpaAttention_folded: 0.2768564224243164\u001b[0m\n",
      "\u001b[1;31m# 99 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 100 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 100 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 101 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.4082144498825073\n",
      "Location: [0, 37, 183]\n",
      "LayerNorm_original: -0.1566174030303955\n",
      "SOLayerNorm_folded: -0.5648318529129028\u001b[0m\n",
      "\u001b[1;31m# 101 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 102 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.23570609092712402\n",
      "Location: [0, 37, 779]\n",
      "Conv1D_original: 0.28691428899765015\n",
      "Conv1D_folded: 0.05120819807052612\u001b[0m\n",
      "\u001b[1;31m# 102 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 103 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.1801537275314331\n",
      "Location: [0, 37, 1421]\n",
      "NewGELUActivation_original: 1.3730206489562988\n",
      "NewGELUActivation_folded: 1.1928669214248657\u001b[0m\n",
      "\u001b[1;31m# 103 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 104 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.026490025222301483\n",
      "Location: [0, 37, 688]\n",
      "Conv1D_original: 0.0050781480967998505\n",
      "Conv1D_folded: -0.021411877125501633\u001b[0m\n",
      "\u001b[1;31m# 104 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 105 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.026490025222301483\n",
      "Location: [0, 37, 688]\n",
      "Dropout_original: 0.0050781480967998505\n",
      "Dropout_folded: -0.021411877125501633\u001b[0m\n",
      "\u001b[1;31m# 105 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 106 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.026490025222301483\n",
      "Location: [0, 37, 688]\n",
      "GPT2MLP_original: 0.0050781480967998505\n",
      "GPT2MLP_folded: -0.021411877125501633\u001b[0m\n",
      "\u001b[1;31m# 106 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 107 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.07282274961471558\n",
      "Location: [0, 37, 393]\n",
      "GPT2Block_original: 0.285450279712677\n",
      "GPT2Block_folded: 0.3582730293273926\u001b[0m\n",
      "\u001b[1;31m# 107 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 108 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20347344875335693\n",
      "Location: [0, 0, 37, 61]\n",
      "GPT2Block_original: 0.4238986074924469\n",
      "GPT2Block_folded: 0.22042515873908997\u001b[0m\n",
      "\u001b[1;31m# 108 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 109 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.22449195384979248\n",
      "Location: [0, 7, 37, 6]\n",
      "GPT2Block_original: 0.5013483762741089\n",
      "GPT2Block_folded: 0.2768564224243164\u001b[0m\n",
      "\u001b[1;31m# 109 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 110 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 110 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 111 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.3696599006652832\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.4056882858276367\n",
      "SOLayerNorm_folded: 1.77534818649292\u001b[0m\n",
      "\u001b[1;31m# 111 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 112 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21311333775520325\n",
      "Location: [0, 37, 1006]\n",
      "Conv1D_original: -0.18448448181152344\n",
      "Conv1D_folded: 0.02862885594367981\u001b[0m\n",
      "\u001b[1;31m# 112 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 113 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01788296177983284\n",
      "Location: [0, 0, 627]\n",
      "Conv1D_original: 0.041314348578453064\n",
      "Conv1D_folded: 0.023431386798620224\u001b[0m\n",
      "\u001b[1;31m# 113 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 114 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01788296177983284\n",
      "Location: [0, 0, 627]\n",
      "Dropout_original: 0.041314348578453064\n",
      "Dropout_folded: 0.023431386798620224\u001b[0m\n",
      "\u001b[1;31m# 114 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 115 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.01788296177983284\n",
      "Location: [0, 0, 627]\n",
      "GPT2SdpaAttention_original: 0.041314348578453064\n",
      "GPT2SdpaAttention_folded: 0.023431386798620224\u001b[0m\n",
      "\u001b[1;31m# 115 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 116 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21311333775520325\n",
      "Location: [0, 3, 37, 46]\n",
      "GPT2SdpaAttention_original: -0.18448448181152344\n",
      "GPT2SdpaAttention_folded: 0.02862885594367981\u001b[0m\n",
      "\u001b[1;31m# 116 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 117 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20460131764411926\n",
      "Location: [0, 11, 37, 0]\n",
      "GPT2SdpaAttention_original: 0.6249588131904602\n",
      "GPT2SdpaAttention_folded: 0.42035749554634094\u001b[0m\n",
      "\u001b[1;31m# 117 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 118 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 118 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 119 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.3633795976638794\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.2701414823532104\n",
      "SOLayerNorm_folded: 1.6335210800170898\u001b[0m\n",
      "\u001b[1;31m# 119 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 120 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2179701328277588\n",
      "Location: [0, 37, 1152]\n",
      "Conv1D_original: 0.8819946050643921\n",
      "Conv1D_folded: 0.6640244722366333\u001b[0m\n",
      "\u001b[1;31m# 120 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 121 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2208162546157837\n",
      "Location: [0, 37, 82]\n",
      "NewGELUActivation_original: 0.6083754301071167\n",
      "NewGELUActivation_folded: 0.8291916847229004\u001b[0m\n",
      "\u001b[1;31m# 121 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 122 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.03718473017215729\n",
      "Location: [0, 37, 393]\n",
      "Conv1D_original: 0.04538213461637497\n",
      "Conv1D_folded: 0.08256686478853226\u001b[0m\n",
      "\u001b[1;31m# 122 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 123 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.03718473017215729\n",
      "Location: [0, 37, 393]\n",
      "Dropout_original: 0.04538213461637497\n",
      "Dropout_folded: 0.08256686478853226\u001b[0m\n",
      "\u001b[1;31m# 123 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 124 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.03718473017215729\n",
      "Location: [0, 37, 393]\n",
      "GPT2MLP_original: 0.04538213461637497\n",
      "GPT2MLP_folded: 0.08256686478853226\u001b[0m\n",
      "\u001b[1;31m# 124 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 125 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.1085003912448883\n",
      "Location: [0, 37, 393]\n",
      "GPT2Block_original: 0.31052806973457336\n",
      "GPT2Block_folded: 0.41902846097946167\u001b[0m\n",
      "\u001b[1;31m# 125 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 126 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21311333775520325\n",
      "Location: [0, 3, 37, 46]\n",
      "GPT2Block_original: -0.18448448181152344\n",
      "GPT2Block_folded: 0.02862885594367981\u001b[0m\n",
      "\u001b[1;31m# 126 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 127 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20460131764411926\n",
      "Location: [0, 11, 37, 0]\n",
      "GPT2Block_original: 0.6249588131904602\n",
      "GPT2Block_folded: 0.42035749554634094\u001b[0m\n",
      "\u001b[1;31m# 127 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 128 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 128 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 129 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.49475419521331787\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.426460862159729\n",
      "SOLayerNorm_folded: 1.9212150573730469\u001b[0m\n",
      "\u001b[1;31m# 129 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 130 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21904802322387695\n",
      "Location: [0, 10, 1824]\n",
      "Conv1D_original: -0.2564496397972107\n",
      "Conv1D_folded: -0.47549766302108765\u001b[0m\n",
      "\u001b[1;31m# 130 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 131 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.018687669187784195\n",
      "Location: [0, 0, 118]\n",
      "Conv1D_original: -0.06483262777328491\n",
      "Conv1D_folded: -0.04614495858550072\u001b[0m\n",
      "\u001b[1;31m# 131 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 132 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.018687669187784195\n",
      "Location: [0, 0, 118]\n",
      "Dropout_original: -0.06483262777328491\n",
      "Dropout_folded: -0.04614495858550072\u001b[0m\n",
      "\u001b[1;31m# 132 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 133 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.018687669187784195\n",
      "Location: [0, 0, 118]\n",
      "GPT2SdpaAttention_original: -0.06483262777328491\n",
      "GPT2SdpaAttention_folded: -0.04614495858550072\u001b[0m\n",
      "\u001b[1;31m# 133 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 134 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.1866702437400818\n",
      "Location: [0, 1, 37, 40]\n",
      "GPT2SdpaAttention_original: -0.8005263209342957\n",
      "GPT2SdpaAttention_folded: -0.6138560771942139\u001b[0m\n",
      "\u001b[1;31m# 134 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 135 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21904802322387695\n",
      "Location: [0, 4, 10, 32]\n",
      "GPT2SdpaAttention_original: -0.2564496397972107\n",
      "GPT2SdpaAttention_folded: -0.47549766302108765\u001b[0m\n",
      "\u001b[1;31m# 135 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 136 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 136 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 137 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.4810407757759094\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 0.9194021821022034\n",
      "SOLayerNorm_folded: 1.4004429578781128\u001b[0m\n",
      "\u001b[1;31m# 137 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 138 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20796597003936768\n",
      "Location: [0, 37, 250]\n",
      "Conv1D_original: -0.71324622631073\n",
      "Conv1D_folded: -0.9212121963500977\u001b[0m\n",
      "\u001b[1;31m# 138 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 139 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.1977936029434204\n",
      "Location: [0, 37, 272]\n",
      "NewGELUActivation_original: 0.450064480304718\n",
      "NewGELUActivation_folded: 0.6478580832481384\u001b[0m\n",
      "\u001b[1;31m# 139 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 140 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.030773591250181198\n",
      "Location: [0, 37, 251]\n",
      "Conv1D_original: -0.0029062889516353607\n",
      "Conv1D_folded: -0.03367988020181656\u001b[0m\n",
      "\u001b[1;31m# 140 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 141 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.030773591250181198\n",
      "Location: [0, 37, 251]\n",
      "Dropout_original: -0.0029062889516353607\n",
      "Dropout_folded: -0.03367988020181656\u001b[0m\n",
      "\u001b[1;31m# 141 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 142 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.030773591250181198\n",
      "Location: [0, 37, 251]\n",
      "GPT2MLP_original: -0.0029062889516353607\n",
      "GPT2MLP_folded: -0.03367988020181656\u001b[0m\n",
      "\u001b[1;31m# 142 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 143 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.11435776948928833\n",
      "Location: [0, 37, 393]\n",
      "GPT2Block_original: 0.18443593382835388\n",
      "GPT2Block_folded: 0.2987937033176422\u001b[0m\n",
      "\u001b[1;31m# 143 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 144 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.1866702437400818\n",
      "Location: [0, 1, 37, 40]\n",
      "GPT2Block_original: -0.8005263209342957\n",
      "GPT2Block_folded: -0.6138560771942139\u001b[0m\n",
      "\u001b[1;31m# 144 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 145 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21904802322387695\n",
      "Location: [0, 4, 10, 32]\n",
      "GPT2Block_original: -0.2564496397972107\n",
      "GPT2Block_folded: -0.47549766302108765\u001b[0m\n",
      "\u001b[1;31m# 145 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 146 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 146 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 147 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.4976051449775696\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 0.7644535899162292\n",
      "SOLayerNorm_folded: 1.2620587348937988\u001b[0m\n",
      "\u001b[1;31m# 147 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 148 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2255232036113739\n",
      "Location: [0, 10, 1810]\n",
      "Conv1D_original: 0.29019805788993835\n",
      "Conv1D_folded: 0.5157212615013123\u001b[0m\n",
      "\u001b[1;31m# 148 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 149 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.017346028238534927\n",
      "Location: [0, 0, 184]\n",
      "Conv1D_original: 0.006826799362897873\n",
      "Conv1D_folded: -0.010519228875637054\u001b[0m\n",
      "\u001b[1;31m# 149 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 150 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.017346028238534927\n",
      "Location: [0, 0, 184]\n",
      "Dropout_original: 0.006826799362897873\n",
      "Dropout_folded: -0.010519228875637054\u001b[0m\n",
      "\u001b[1;31m# 150 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 151 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.017346028238534927\n",
      "Location: [0, 0, 184]\n",
      "GPT2SdpaAttention_original: 0.006826799362897873\n",
      "GPT2SdpaAttention_folded: -0.010519228875637054\u001b[0m\n",
      "\u001b[1;31m# 151 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 152 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.1889784336090088\n",
      "Location: [0, 7, 37, 37]\n",
      "GPT2SdpaAttention_original: -0.7749770879745483\n",
      "GPT2SdpaAttention_folded: -0.5859986543655396\u001b[0m\n",
      "\u001b[1;31m# 152 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 153 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2255232036113739\n",
      "Location: [0, 4, 10, 18]\n",
      "GPT2SdpaAttention_original: 0.29019805788993835\n",
      "GPT2SdpaAttention_folded: 0.5157212615013123\u001b[0m\n",
      "\u001b[1;31m# 153 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 154 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 154 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 155 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.500969648361206\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 0.76439368724823\n",
      "SOLayerNorm_folded: 1.265363335609436\u001b[0m\n",
      "\u001b[1;31m# 155 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 156 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21009472012519836\n",
      "Location: [0, 37, 1855]\n",
      "Conv1D_original: -0.3057917058467865\n",
      "Conv1D_folded: -0.5158864259719849\u001b[0m\n",
      "\u001b[1;31m# 156 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 157 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21050065755844116\n",
      "Location: [0, 37, 970]\n",
      "NewGELUActivation_original: 0.7265779376029968\n",
      "NewGELUActivation_folded: 0.937078595161438\u001b[0m\n",
      "\u001b[1;31m# 157 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 158 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02775147557258606\n",
      "Location: [0, 37, 163]\n",
      "Conv1D_original: 0.11462225764989853\n",
      "Conv1D_folded: 0.08687078207731247\u001b[0m\n",
      "\u001b[1;31m# 158 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 159 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02775147557258606\n",
      "Location: [0, 37, 163]\n",
      "Dropout_original: 0.11462225764989853\n",
      "Dropout_folded: 0.08687078207731247\u001b[0m\n",
      "\u001b[1;31m# 159 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 160 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02775147557258606\n",
      "Location: [0, 37, 163]\n",
      "GPT2MLP_original: 0.11462225764989853\n",
      "GPT2MLP_folded: 0.08687078207731247\u001b[0m\n",
      "\u001b[1;31m# 160 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 161 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.12315994501113892\n",
      "Location: [0, 37, 393]\n",
      "GPT2Block_original: 0.2701645791530609\n",
      "GPT2Block_folded: 0.39332452416419983\u001b[0m\n",
      "\u001b[1;31m# 161 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 162 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.1889784336090088\n",
      "Location: [0, 7, 37, 37]\n",
      "GPT2Block_original: -0.7749770879745483\n",
      "GPT2Block_folded: -0.5859986543655396\u001b[0m\n",
      "\u001b[1;31m# 162 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 163 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2255232036113739\n",
      "Location: [0, 4, 10, 18]\n",
      "GPT2Block_original: 0.29019805788993835\n",
      "GPT2Block_folded: 0.5157212615013123\u001b[0m\n",
      "\u001b[1;31m# 163 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 164 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 164 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 165 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.5105724334716797\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.037034034729004\n",
      "SOLayerNorm_folded: 1.5476064682006836\u001b[0m\n",
      "\u001b[1;31m# 165 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 166 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20877322554588318\n",
      "Location: [0, 37, 1420]\n",
      "Conv1D_original: 0.1805264949798584\n",
      "Conv1D_folded: -0.028246726840734482\u001b[0m\n",
      "\u001b[1;31m# 166 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 167 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.020734231919050217\n",
      "Location: [0, 0, 716]\n",
      "Conv1D_original: -0.06368374079465866\n",
      "Conv1D_folded: -0.042949508875608444\u001b[0m\n",
      "\u001b[1;31m# 167 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 168 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.020734231919050217\n",
      "Location: [0, 0, 716]\n",
      "Dropout_original: -0.06368374079465866\n",
      "Dropout_folded: -0.042949508875608444\u001b[0m\n",
      "\u001b[1;31m# 168 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 169 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.020734231919050217\n",
      "Location: [0, 0, 716]\n",
      "GPT2SdpaAttention_original: -0.06368374079465866\n",
      "GPT2SdpaAttention_folded: -0.042949508875608444\u001b[0m\n",
      "\u001b[1;31m# 169 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 170 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20877322554588318\n",
      "Location: [0, 10, 37, 12]\n",
      "GPT2SdpaAttention_original: 0.1805264949798584\n",
      "GPT2SdpaAttention_folded: -0.028246726840734482\u001b[0m\n",
      "\u001b[1;31m# 170 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 171 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.1811424195766449\n",
      "Location: [0, 9, 37, 59]\n",
      "GPT2SdpaAttention_original: -0.2487969994544983\n",
      "GPT2SdpaAttention_folded: -0.4299394190311432\u001b[0m\n",
      "\u001b[1;31m# 171 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 172 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 172 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 173 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.5148812532424927\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.1200242042541504\n",
      "SOLayerNorm_folded: 1.634905457496643\u001b[0m\n",
      "\u001b[1;31m# 173 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 174 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.21645629405975342\n",
      "Location: [0, 37, 1227]\n",
      "Conv1D_original: 0.5008580684661865\n",
      "Conv1D_folded: 0.2844017744064331\u001b[0m\n",
      "\u001b[1;31m# 174 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 175 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20626157522201538\n",
      "Location: [0, 10, 1302]\n",
      "NewGELUActivation_original: 0.7799798250198364\n",
      "NewGELUActivation_folded: 0.573718249797821\u001b[0m\n",
      "\u001b[1;31m# 175 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 176 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0250706747174263\n",
      "Location: [0, 37, 661]\n",
      "Conv1D_original: 0.03260529786348343\n",
      "Conv1D_folded: 0.05767597258090973\u001b[0m\n",
      "\u001b[1;31m# 176 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 177 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0250706747174263\n",
      "Location: [0, 37, 661]\n",
      "Dropout_original: 0.03260529786348343\n",
      "Dropout_folded: 0.05767597258090973\u001b[0m\n",
      "\u001b[1;31m# 177 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 178 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.0250706747174263\n",
      "Location: [0, 37, 661]\n",
      "GPT2MLP_original: 0.03260529786348343\n",
      "GPT2MLP_folded: 0.05767597258090973\u001b[0m\n",
      "\u001b[1;31m# 178 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 179 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.11580860614776611\n",
      "Location: [0, 37, 393]\n",
      "GPT2Block_original: 0.30139726400375366\n",
      "GPT2Block_folded: 0.4172058701515198\u001b[0m\n",
      "\u001b[1;31m# 179 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 180 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20877322554588318\n",
      "Location: [0, 10, 37, 12]\n",
      "GPT2Block_original: 0.1805264949798584\n",
      "GPT2Block_folded: -0.028246726840734482\u001b[0m\n",
      "\u001b[1;31m# 180 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 181 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.1811424195766449\n",
      "Location: [0, 9, 37, 59]\n",
      "GPT2Block_original: -0.2487969994544983\n",
      "GPT2Block_folded: -0.4299394190311432\u001b[0m\n",
      "\u001b[1;31m# 181 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 182 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 182 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 183 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.46986162662506104\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.1244865655899048\n",
      "SOLayerNorm_folded: 1.5943481922149658\u001b[0m\n",
      "\u001b[1;31m# 183 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 184 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20501768589019775\n",
      "Location: [0, 37, 461]\n",
      "Conv1D_original: 0.5359957218170166\n",
      "Conv1D_folded: 0.7410134077072144\u001b[0m\n",
      "\u001b[1;31m# 184 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 185 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.020579185336828232\n",
      "Location: [0, 0, 647]\n",
      "Conv1D_original: -0.03720221295952797\n",
      "Conv1D_folded: -0.0577813982963562\u001b[0m\n",
      "\u001b[1;31m# 185 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 186 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.020579185336828232\n",
      "Location: [0, 0, 647]\n",
      "Dropout_original: -0.03720221295952797\n",
      "Dropout_folded: -0.0577813982963562\u001b[0m\n",
      "\u001b[1;31m# 186 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 187 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.020579185336828232\n",
      "Location: [0, 0, 647]\n",
      "GPT2SdpaAttention_original: -0.03720221295952797\n",
      "GPT2SdpaAttention_folded: -0.0577813982963562\u001b[0m\n",
      "\u001b[1;31m# 187 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 188 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.1973041296005249\n",
      "Location: [0, 10, 37, 35]\n",
      "GPT2SdpaAttention_original: -0.39368391036987305\n",
      "GPT2SdpaAttention_folded: -0.590988039970398\u001b[0m\n",
      "\u001b[1;31m# 188 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 189 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.19110095500946045\n",
      "Location: [0, 11, 37, 40]\n",
      "GPT2SdpaAttention_original: -0.2961219251155853\n",
      "GPT2SdpaAttention_folded: -0.10502097010612488\u001b[0m\n",
      "\u001b[1;31m# 189 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 190 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 190 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 191 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.4610152244567871\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.3305046558380127\n",
      "SOLayerNorm_folded: 1.7915198802947998\u001b[0m\n",
      "\u001b[1;31m# 191 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 192 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.2243088036775589\n",
      "Location: [0, 10, 520]\n",
      "Conv1D_original: -0.2177196592092514\n",
      "Conv1D_folded: 0.006589144468307495\u001b[0m\n",
      "\u001b[1;31m# 192 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 193 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.19753575325012207\n",
      "Location: [0, 37, 592]\n",
      "NewGELUActivation_original: 1.1355748176574707\n",
      "NewGELUActivation_folded: 1.3331105709075928\u001b[0m\n",
      "\u001b[1;31m# 193 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 194 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02630702592432499\n",
      "Location: [0, 37, 218]\n",
      "Conv1D_original: -0.037358082830905914\n",
      "Conv1D_folded: -0.011051056906580925\u001b[0m\n",
      "\u001b[1;31m# 194 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 195 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02630702592432499\n",
      "Location: [0, 37, 218]\n",
      "Dropout_original: -0.037358082830905914\n",
      "Dropout_folded: -0.011051056906580925\u001b[0m\n",
      "\u001b[1;31m# 195 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 196 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.02630702592432499\n",
      "Location: [0, 37, 218]\n",
      "GPT2MLP_original: -0.037358082830905914\n",
      "GPT2MLP_folded: -0.011051056906580925\u001b[0m\n",
      "\u001b[1;31m# 196 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 197 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.11055067181587219\n",
      "Location: [0, 37, 393]\n",
      "GPT2Block_original: 0.4568997323513031\n",
      "GPT2Block_folded: 0.5674504041671753\u001b[0m\n",
      "\u001b[1;31m# 197 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 198 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.1973041296005249\n",
      "Location: [0, 10, 37, 35]\n",
      "GPT2Block_original: -0.39368391036987305\n",
      "GPT2Block_folded: -0.590988039970398\u001b[0m\n",
      "\u001b[1;31m# 198 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 199 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.19110095500946045\n",
      "Location: [0, 11, 37, 40]\n",
      "GPT2Block_original: -0.2961219251155853\n",
      "GPT2Block_folded: -0.10502097010612488\u001b[0m\n",
      "\u001b[1;31m# 199 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 200 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 200 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 201 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.41154634952545166\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.6225162744522095\n",
      "SOLayerNorm_folded: 2.034062623977661\u001b[0m\n",
      "\u001b[1;31m# 201 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 202 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20568612217903137\n",
      "Location: [0, 10, 609]\n",
      "Conv1D_original: -0.08007419854402542\n",
      "Conv1D_folded: 0.12561193108558655\u001b[0m\n",
      "\u001b[1;31m# 202 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 203 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.018782002851366997\n",
      "Location: [0, 0, 540]\n",
      "Conv1D_original: 0.0005605518817901611\n",
      "Conv1D_folded: -0.018221450969576836\u001b[0m\n",
      "\u001b[1;31m# 203 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 204 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.018782002851366997\n",
      "Location: [0, 0, 540]\n",
      "Dropout_original: 0.0005605518817901611\n",
      "Dropout_folded: -0.018221450969576836\u001b[0m\n",
      "\u001b[1;31m# 204 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 205 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.018782002851366997\n",
      "Location: [0, 0, 540]\n",
      "GPT2SdpaAttention_original: 0.0005605518817901611\n",
      "GPT2SdpaAttention_folded: -0.018221450969576836\u001b[0m\n",
      "\u001b[1;31m# 205 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 206 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.16968339681625366\n",
      "Location: [0, 4, 37, 52]\n",
      "GPT2SdpaAttention_original: -0.49384385347366333\n",
      "GPT2SdpaAttention_folded: -0.663527250289917\u001b[0m\n",
      "\u001b[1;31m# 206 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 207 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.18333649635314941\n",
      "Location: [0, 3, 37, 31]\n",
      "GPT2SdpaAttention_original: 1.2940958738327026\n",
      "GPT2SdpaAttention_folded: 1.1107593774795532\u001b[0m\n",
      "\u001b[1;31m# 207 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 208 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 208 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 209 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.41740524768829346\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.9249247312545776\n",
      "SOLayerNorm_folded: 2.342329978942871\u001b[0m\n",
      "\u001b[1;31m# 209 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 210 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.20479032397270203\n",
      "Location: [0, 37, 1500]\n",
      "Conv1D_original: -0.19401288032531738\n",
      "Conv1D_folded: -0.3988032042980194\u001b[0m\n",
      "\u001b[1;31m# 210 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 211 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.18753314018249512\n",
      "Location: [0, 37, 1156]\n",
      "NewGELUActivation_original: 1.294943928718567\n",
      "NewGELUActivation_folded: 1.1074107885360718\u001b[0m\n",
      "\u001b[1;31m# 211 [ Fail ] NewGELUActivation_original != NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 212 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.030111856758594513\n",
      "Location: [0, 37, 654]\n",
      "Conv1D_original: -0.10908465832471848\n",
      "Conv1D_folded: -0.139196515083313\u001b[0m\n",
      "\u001b[1;31m# 212 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 213 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.030111856758594513\n",
      "Location: [0, 37, 654]\n",
      "Dropout_original: -0.10908465832471848\n",
      "Dropout_folded: -0.139196515083313\u001b[0m\n",
      "\u001b[1;31m# 213 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 214 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.030111856758594513\n",
      "Location: [0, 37, 654]\n",
      "GPT2MLP_original: -0.10908465832471848\n",
      "GPT2MLP_folded: -0.139196515083313\u001b[0m\n",
      "\u001b[1;31m# 214 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 215 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.11710812151432037\n",
      "Location: [0, 10, 118]\n",
      "GPT2Block_original: -0.3198471963405609\n",
      "GPT2Block_folded: -0.20273907482624054\u001b[0m\n",
      "\u001b[1;31m# 215 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 216 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.16968339681625366\n",
      "Location: [0, 4, 37, 52]\n",
      "GPT2Block_original: -0.49384385347366333\n",
      "GPT2Block_folded: -0.663527250289917\u001b[0m\n",
      "\u001b[1;31m# 216 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 217 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.18333649635314941\n",
      "Location: [0, 3, 37, 31]\n",
      "GPT2Block_original: 1.2940958738327026\n",
      "GPT2Block_folded: 1.1107593774795532\u001b[0m\n",
      "\u001b[1;31m# 217 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 218 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 218 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 219 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;33mMax diff: 0.4321399927139282\n",
      "Location: [0, 37, 393]\n",
      "LayerNorm_original: 1.8031209707260132\n",
      "SOLayerNorm_folded: 2.2352609634399414\u001b[0m\n",
      "\u001b[1;31m# 219 [ Fail ] LayerNorm_original != SOLayerNorm_folded\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with utils.HookManager(my_ln_model, hook_original, None, list(my_ln_model.modules())[1:]):\n",
    "    original_out = my_ln_model(input_ids)\n",
    "\n",
    "with utils.HookManager(my_so_ln_model, hook_folded, None, list(my_so_ln_model.modules())[1:]):\n",
    "    folded_out = my_so_ln_model(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34m# 220 [ Test ] folded_out[0] ?= original_out[0]\u001b[0m\n",
      "\u001b[2m=== folded_out[0] ===\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.3577e-01, -1.2654e-01, -3.7514e-01,  ..., 1.1333e-01, 6.4051e-01, -4.3048e-01],\n",
      "         [5.6186e-01, 4.8840e-01, 7.8146e-01,  ..., -8.9503e-01, 8.5370e-01, -1.2255e+00],\n",
      "         [7.8186e-01, 7.7026e-01, -2.3901e-01,  ..., 9.3298e-02, 1.0692e+00, -5.3862e-01],\n",
      "         ...,\n",
      "         [2.4021e-01, 6.9612e-01, -7.3467e-01,  ..., -2.8962e-01, -3.7554e-01, -2.4757e-01],\n",
      "         [5.1597e-02, 6.8300e-01, -1.0035e+00,  ..., -1.8950e-01, 1.0808e+00, -5.9560e-01],\n",
      "         [3.5050e-02, 2.6225e+00, -8.1549e-01,  ..., -1.8690e-01, 1.4773e+00, 7.5001e-01]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "\u001b[2m=== original_out[0] ===\u001b[0m\n",
      "tensor([[[2.1712e-02, -1.4150e-01, -3.4320e-01,  ..., 3.9669e-03, 6.5941e-01, -4.2921e-01],\n",
      "         [5.1380e-01, 4.2810e-01, 7.8693e-01,  ..., -9.6909e-01, 8.7120e-01, -1.2768e+00],\n",
      "         [7.8011e-01, 7.4773e-01, -2.6297e-01,  ..., 6.7078e-03, 1.0873e+00, -5.5771e-01],\n",
      "         ...,\n",
      "         [1.8941e-01, 7.3671e-01, -6.8908e-01,  ..., -2.5222e-01, -3.3449e-01, -2.5983e-01],\n",
      "         [3.0730e-02, 6.8014e-01, -1.0200e+00,  ..., -1.9081e-01, 1.1007e+00, -6.1009e-01],\n",
      "         [3.8301e-02, 2.6348e+00, -8.0384e-01,  ..., -1.8258e-01, 1.4888e+00, 7.4053e-01]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "\u001b[1;33mMax diff: 0.4321399927139282\n",
      "Location: [0, 37, 393]\n",
      "folded_out[0]: 2.2352609634399414\n",
      "original_out[0]: 1.8031209707260132\u001b[0m\n",
      "\u001b[1;31m# 220 [ Fail ] folded_out[0] != original_out[0]\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check.check_eq('folded_out[0]', 'original_out[0]', local_vars=locals(), abs_tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34m==== < Summary > ====\u001b[0m\n",
      "\u001b[1;32m# 0 [ Pass ]\u001b[0m Embedding_original == Embedding_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 1 [ Pass ]\u001b[0m Embedding_original == Embedding_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 2 [ Pass ]\u001b[0m Dropout_original == Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 3 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 4 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 5 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 6 [ Pass ]\u001b[0m Dropout_original == Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 7 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 8 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 9 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 10 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 11 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 12 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 13 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 14 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 15 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 16 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 17 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 18 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 19 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 20 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 21 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 22 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 23 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 24 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 25 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 26 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 27 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 28 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 29 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 30 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 31 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 32 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 33 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 34 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 35 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 36 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 37 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 38 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 39 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 40 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 41 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 42 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 43 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 44 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 45 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 46 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 47 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 48 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 49 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 50 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 51 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 52 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 53 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 54 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 55 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 56 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 57 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 58 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 59 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 60 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 61 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 62 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 63 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 64 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 65 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 66 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 67 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 68 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 69 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 70 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 71 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 72 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 73 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 74 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 75 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 76 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 77 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 78 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 79 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 80 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 81 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 82 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 83 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 84 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 85 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 86 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 87 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 88 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 89 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 90 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 91 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 92 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 93 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 94 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 95 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 96 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 97 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 98 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 99 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 100 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 101 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 102 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 103 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 104 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 105 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 106 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 107 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 108 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 109 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 110 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 111 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 112 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 113 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 114 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 115 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 116 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 117 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 118 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 119 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 120 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 121 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 122 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 123 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 124 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 125 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 126 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 127 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 128 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 129 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 130 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 131 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 132 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 133 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 134 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 135 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 136 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 137 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 138 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 139 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 140 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 141 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 142 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 143 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 144 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 145 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 146 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 147 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 148 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 149 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 150 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 151 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 152 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 153 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 154 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 155 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 156 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 157 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 158 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 159 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 160 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 161 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 162 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 163 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 164 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 165 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 166 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 167 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 168 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 169 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 170 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 171 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 172 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 173 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 174 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 175 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 176 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 177 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 178 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 179 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 180 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 181 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 182 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 183 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 184 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 185 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 186 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 187 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 188 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 189 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 190 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 191 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 192 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 193 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 194 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 195 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 196 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 197 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 198 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 199 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 200 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 201 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 202 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 203 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 204 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 205 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 206 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 207 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 208 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 209 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 210 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 211 [ Fail ]\u001b[0m NewGELUActivation_original != NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 212 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 213 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 214 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 215 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 216 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 217 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;32m# 218 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 219 [ Fail ]\u001b[0m LayerNorm_original != SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=0.01)\u001b[0m\n",
      "\u001b[1;31m# 220 [ Fail ]\u001b[0m folded_out[0] != original_out[0] \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "-------------------\n",
      "\u001b[1;34m(30/221) [\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;34m]\u001b[0m\n",
      "\u001b[1;34m==== </Summary > ====\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_analyse_hook_fns.<locals>.hook_fn() missing 1 required positional argument: 'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     my_ln_model(my_input_ids)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mHookManager(native_so_ln_model, native_so_ln_hook_pre_fn, native_so_ln_hook_fn):\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mnative_so_ln_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mHookManager(my_so_ln_model, my_so_ln_hook_fn, my_so_ln_hook_pre_fn):\n\u001b[0;32m     31\u001b[0m     my_so_ln_model(my_input_ids)\n",
      "File \u001b[1;32mc:\\Users\\BobYu\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BobYu\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1592\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1587\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward pre-hook must return None or a tuple \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1589\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_kwargs_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1590\u001b[0m             )\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1592\u001b[0m     args_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1593\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1594\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: create_analyse_hook_fns.<locals>.hook_fn() missing 1 required positional argument: 'outputs'"
     ]
    }
   ],
   "source": [
    "native_ln_model = GPT2Model(config).cuda()\n",
    "my_ln_model = GPT2Model(config).cuda()\n",
    "native_so_ln_model = GPT2Model(config).cuda()\n",
    "my_so_ln_model = GPT2Model(config).cuda()\n",
    "\n",
    "my_ln_model.load_state_dict(native_ln_model.state_dict())\n",
    "native_so_ln_model.load_state_dict(native_ln_model.state_dict())\n",
    "my_so_ln_model.load_state_dict(native_ln_model.state_dict())\n",
    "\n",
    "native_ln_model.eval()\n",
    "my_ln_model.eval()\n",
    "native_so_ln_model.eval()\n",
    "my_so_ln_model.eval()\n",
    "\n",
    "my_ln_counter = utils.Counter()\n",
    "native_so_ln_counter = utils.Counter()\n",
    "my_so_ln_counter = utils.Counter()\n",
    "\n",
    "# native_ln_hook_pre_fn, native_ln_hook_fn = utils.create_analyse_hook_fns(my_ln_counter, _print=False)\n",
    "my_ln_hook_pre_fn, my_ln_hook_fn = utils.create_analyse_hook_fns(my_ln_counter, _print=False)\n",
    "native_so_ln_hook_pre_fn, native_so_ln_hook_fn = utils.create_analyse_hook_fns(my_so_ln_counter, _print=False)\n",
    "my_so_ln_hook_pre_fn, my_so_ln_hook_fn = utils.create_analyse_hook_fns(my_so_ln_counter, _print=False)\n",
    "\n",
    "with utils.HookManager(my_ln_model, my_ln_hook_fn, my_ln_hook_pre_fn):\n",
    "    my_ln_model(my_input_ids)\n",
    "\n",
    "with utils.HookManager(native_so_ln_model, native_so_ln_hook_pre_fn, native_so_ln_hook_fn):\n",
    "    native_so_ln_model(my_input_ids)\n",
    "\n",
    "with utils.HookManager(my_so_ln_model, my_so_ln_hook_fn, my_so_ln_hook_pre_fn):\n",
    "    my_so_ln_model(my_input_ids)\n",
    "\n",
    "for layer in my_ln_counter.center_modules:\n",
    "    modules.center_modules(layer)\n",
    "\n",
    "for layer in my_so_ln_counter.center_modules:\n",
    "    modules.center_modules(layer)\n",
    "\n",
    "for layer in my_so_ln_counter.center_modules:\n",
    "    modules.center_modules(layer)\n",
    "\n",
    "for layer in my_ln_counter.layernorms:\n",
    "    modules.replace_layer_norm_forward(\n",
    "        layer,\n",
    "        forward_fn=modules.myln_forward,\n",
    "        class_name='MyLayerNorm'\n",
    "    )\n",
    "\n",
    "for layer in native_so_ln_counter.layernorms:\n",
    "    modules.replace_layer_norm_forward(\n",
    "        layer,\n",
    "        forward_fn=modules.native_soln_forward,\n",
    "        class_name='NativeSOLayerNorm'\n",
    "    )\n",
    "\n",
    "for layer in my_so_ln_counter.layernorms:\n",
    "    modules.replace_layer_norm_forward(\n",
    "        layer,\n",
    "        forward_fn=modules.soln_forward,\n",
    "        class_name='MySOLayerNorm'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-20 16:33:25,224] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1120 16:33:27.226000 3840 torch\\distributed\\elastic\\multiprocessing\\redirects.py:28] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.profiling.flops_profiler import get_model_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-20 16:33:27,340] [INFO] [profiler.py:1220:get_model_profile] Flops profiler warming-up...\n",
      "[2024-11-20 16:33:27,455] [INFO] [profiler.py:81:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         124.44 M\n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       11.17 GMACs\n",
      "fwd flops per GPU:                                                      22.36 G \n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       22.36 G \n",
      "fwd latency:                                                            197.94 ms\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    112.96 GFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'GPT2Model': '124.44 M'}\n",
      "    MACs        - {'GPT2Model': '11.17 GMACs'}\n",
      "    fwd latency - {'GPT2Model': '197.94 ms'}\n",
      "depth 1:\n",
      "    params      - {'ModuleList': '85.05 M'}\n",
      "    MACs        - {'ModuleList': '11.17 GMACs'}\n",
      "    fwd latency - {'ModuleList': '196.88 ms'}\n",
      "depth 2:\n",
      "    params      - {'GPT2Block': '85.05 M'}\n",
      "    MACs        - {'GPT2Block': '11.17 GMACs'}\n",
      "    fwd latency - {'GPT2Block': '196.88 ms'}\n",
      "depth 3:\n",
      "    params      - {'GPT2MLP': '56.67 M'}\n",
      "    MACs        - {'GPT2MLP': '7.25 GMACs'}\n",
      "    fwd latency - {'GPT2MLP': '112.57 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "GPT2Model(\n",
      "  124.44 M = 100% Params, 11.17 GMACs = 100% MACs, 197.94 ms = 100% latency, 112.96 GFLOPS\n",
      "  (wte): Embedding(38.6 M = 31.02% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 50257, 768)\n",
      "  (wpe): Embedding(786.43 K = 0.63% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1024, 768)\n",
      "  (drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 28.94 ms = 14.62% latency, 64.38 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 8.95 ms = 4.52% latency, 73.15 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 19.02 ms = 9.61% latency, 63.51 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 6 ms = 3.03% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 17.38 ms = 8.78% latency, 107.19 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 7.38 ms = 3.73% latency, 88.62 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 9 ms = 4.55% latency, 134.22 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 3.99 ms = 2.02% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 17 ms = 8.59% latency, 109.6 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.51% latency, 489.8 MFLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 5 ms = 2.52% latency, 130.94 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 10 ms = 5.05% latency, 120.8 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 4 ms = 2.02% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 20 ms = 10.1% latency, 93.16 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 993.25 us = 0.5% latency, 494.86 MFLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 6.01 ms = 3.03% latency, 108.93 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 12 ms = 6.06% latency, 100.67 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 6.01 ms = 3.03% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 18 ms = 9.09% latency, 103.51 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 6 ms = 3.03% latency, 109.06 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 999.93 us = 0.51% latency, 491.56 MFLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 10 ms = 5.05% latency, 120.79 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 2.01 ms = 1.01% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 15 ms = 7.58% latency, 124.22 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 6 ms = 3.03% latency, 109.06 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.51% latency, 491.09 MFLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 8 ms = 4.04% latency, 151.01 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.51% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 17 ms = 8.59% latency, 109.6 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 6 ms = 3.03% latency, 109.05 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 999.93 us = 0.51% latency, 491.56 MFLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 10 ms = 5.05% latency, 120.8 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 2 ms = 1.01% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 18 ms = 9.09% latency, 103.51 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 5 ms = 2.53% latency, 130.87 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 999.93 us = 0.51% latency, 491.56 MFLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 11 ms = 5.56% latency, 109.81 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 999.93 us = 0.51% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 15 ms = 7.58% latency, 124.22 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 5.99 ms = 3.03% latency, 109.16 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 8 ms = 4.04% latency, 151.07 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 9.99 ms = 5.05% latency, 186.43 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 4 ms = 2.02% latency, 163.57 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 999.69 us = 0.51% latency, 491.67 MFLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 4 ms = 2.02% latency, 301.96 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 993.49 us = 0.5% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 9.01 ms = 4.55% latency, 206.88 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 4.01 ms = 2.02% latency, 163.35 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 5 ms = 2.53% latency, 241.55 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 995.16 us = 0.5% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 931.14 MMACs = 8.33% MACs, 11.56 ms = 5.84% latency, 161.23 GFLOPS\n",
      "      (ln_1): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 327.16 MMACs = 2.93% MACs, 4 ms = 2.02% latency, 163.57 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.41% MACs, 6.56 ms = 3.31% latency, 184.23 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "------------------------------------------------------------------------------\n",
      "[2024-11-20 16:33:27,688] [INFO] [profiler.py:227:end_profile] Flops profiler finished\n"
     ]
    }
   ],
   "source": [
    "flops, macs, params = get_model_profile(\n",
    "    native_ln_model,\n",
    "    kwargs={'input_ids': input_ids},\n",
    "    print_profile=True,\n",
    "    detailed=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-20 16:33:27,697] [INFO] [profiler.py:1220:get_model_profile] Flops profiler warming-up...\n",
      "[2024-11-20 16:33:27,755] [INFO] [profiler.py:81:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         124.44 M\n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       11.48 GMACs\n",
      "fwd flops per GPU:                                                      22.95 G \n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       22.95 G \n",
      "fwd latency:                                                            155.74 ms\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    147.37 GFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'GPT2Model': '124.44 M'}\n",
      "    MACs        - {'GPT2Model': '11.48 GMACs'}\n",
      "    fwd latency - {'GPT2Model': '155.74 ms'}\n",
      "depth 1:\n",
      "    params      - {'ModuleList': '85.05 M'}\n",
      "    MACs        - {'ModuleList': '11.48 GMACs'}\n",
      "    fwd latency - {'ModuleList': '152.74 ms'}\n",
      "depth 2:\n",
      "    params      - {'GPT2Block': '85.05 M'}\n",
      "    MACs        - {'GPT2Block': '11.48 GMACs'}\n",
      "    fwd latency - {'GPT2Block': '152.74 ms'}\n",
      "depth 3:\n",
      "    params      - {'GPT2MLP': '56.67 M'}\n",
      "    MACs        - {'GPT2MLP': '7.25 GMACs'}\n",
      "    fwd latency - {'GPT2SdpaAttention': '74.88 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "GPT2Model(\n",
      "  124.44 M = 100% Params, 11.48 GMACs = 100% MACs, 155.74 ms = 100% latency, 147.37 GFLOPS\n",
      "  (wte): Embedding(38.6 M = 31.02% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 50257, 768)\n",
      "  (wpe): Embedding(786.43 K = 0.63% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS, 1024, 768)\n",
      "  (drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 18.19 ms = 11.68% latency, 105.15 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 7.19 ms = 4.62% latency, 98.01 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 10 ms = 6.42% latency, 120.8 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 5 ms = 3.21% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 13 ms = 8.35% latency, 147.07 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 8.08 ms = 5.19% latency, 87.25 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 941.04 us = 0.6% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 2.99 ms = 1.92% latency, 404.39 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 13 ms = 8.35% latency, 147.11 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 995.64 us = 0.64% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 8.09 ms = 5.19% latency, 87.15 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 969.89 us = 0.62% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 2.95 ms = 1.89% latency, 409.45 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 7.99 ms = 5.13% latency, 239.31 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 4.99 ms = 3.21% latency, 141.13 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 3 ms = 1.93% latency, 402.75 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 11 ms = 7.07% latency, 173.83 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 8 ms = 5.14% latency, 88.05 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 2 ms = 1.28% latency, 604.46 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 13.55 ms = 8.7% latency, 141.17 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1.02 ms = 0.65% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 4.49 ms = 2.88% latency, 157.09 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 5.04 ms = 3.24% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 3.01 ms = 1.93% latency, 401.92 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 11 ms = 7.06% latency, 173.85 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 5 ms = 3.21% latency, 140.93 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 999.93 us = 0.64% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 4 ms = 2.57% latency, 301.98 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 8 ms = 5.14% latency, 239 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 2.06 ms = 1.32% latency, 342.75 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 946.52 us = 0.61% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 5 ms = 3.21% latency, 241.59 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 14 ms = 8.99% latency, 136.66 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 996.11 us = 0.64% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 9 ms = 5.78% latency, 78.28 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 3 ms = 1.93% latency, 402.84 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 16 ms = 10.27% latency, 119.53 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1.01 ms = 0.65% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 6.99 ms = 4.49% latency, 100.76 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1.01 ms = 0.65% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 6.99 ms = 4.49% latency, 172.86 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 999.45 us = 0.64% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 14 ms = 8.99% latency, 136.62 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1.01 ms = 0.65% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 4 ms = 2.57% latency, 176.32 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 2.99 ms = 1.92% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 5 ms = 3.21% latency, 241.53 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 956.3 MMACs = 8.33% MACs, 13 ms = 8.35% latency, 147.13 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 352.32 MMACs = 3.07% MACs, 7 ms = 4.49% latency, 100.67 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 999.45 us = 0.64% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.26% MACs, 4 ms = 2.57% latency, 302.03 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 2 ms = 1.29% latency, 0 FLOPS)\n",
      ")\n",
      "------------------------------------------------------------------------------\n",
      "[2024-11-20 16:33:27,936] [INFO] [profiler.py:227:end_profile] Flops profiler finished\n"
     ]
    }
   ],
   "source": [
    "flops, macs, params = get_model_profile(\n",
    "    native_so_ln_model,\n",
    "    kwargs={'input_ids': input_ids},\n",
    "    print_profile=True,\n",
    "    detailed=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-20 16:33:27,946] [INFO] [profiler.py:1220:get_model_profile] Flops profiler warming-up...\n",
      "[2024-11-20 16:33:28,037] [INFO] [profiler.py:81:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         124.44 M\n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       11.78 GMACs\n",
      "fwd flops per GPU:                                                      23.56 G \n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       23.56 G \n",
      "fwd latency:                                                            155.66 ms\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    151.33 GFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'GPT2Model': '124.44 M'}\n",
      "    MACs        - {'GPT2Model': '11.78 GMACs'}\n",
      "    fwd latency - {'GPT2Model': '155.66 ms'}\n",
      "depth 1:\n",
      "    params      - {'ModuleList': '85.05 M'}\n",
      "    MACs        - {'ModuleList': '11.78 GMACs'}\n",
      "    fwd latency - {'ModuleList': '152.66 ms'}\n",
      "depth 2:\n",
      "    params      - {'GPT2Block': '85.05 M'}\n",
      "    MACs        - {'GPT2Block': '11.78 GMACs'}\n",
      "    fwd latency - {'GPT2Block': '152.66 ms'}\n",
      "depth 3:\n",
      "    params      - {'GPT2MLP': '56.67 M'}\n",
      "    MACs        - {'GPT2MLP': '7.25 GMACs'}\n",
      "    fwd latency - {'GPT2SdpaAttention': '77.2 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "GPT2Model(\n",
      "  124.44 M = 100% Params, 11.78 GMACs = 100% MACs, 155.66 ms = 100% latency, 151.33 GFLOPS\n",
      "  (wte): Embedding(38.6 M = 31.02% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 50257, 768)\n",
      "  (wpe): Embedding(786.43 K = 0.63% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1024, 768)\n",
      "  (drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 999.69 us = 0.64% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 17.05 ms = 10.95% latency, 115.13 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 8.05 ms = 5.17% latency, 93.78 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 9 ms = 5.78% latency, 134.22 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 4 ms = 2.57% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 10.99 ms = 7.06% latency, 178.55 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 996.83 us = 0.64% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 7 ms = 4.49% latency, 107.91 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 999.69 us = 0.64% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 2 ms = 1.29% latency, 603.74 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 9.51 ms = 6.11% latency, 206.44 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1.5 ms = 0.97% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 4.01 ms = 2.58% latency, 188.14 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 3.99 ms = 2.56% latency, 302.68 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 9 ms = 5.78% latency, 218.12 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 5 ms = 3.21% latency, 151.06 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 3 ms = 1.93% latency, 402.75 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 11.05 ms = 7.1% latency, 177.6 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 6.08 ms = 3.9% latency, 124.27 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1.01 ms = 0.65% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 2.96 ms = 1.9% latency, 407.41 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 975.13 us = 0.63% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 9.95 ms = 6.39% latency, 197.26 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 949.14 us = 0.61% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 4 ms = 2.57% latency, 188.84 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 3 ms = 1.93% latency, 402.24 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 10 ms = 6.43% latency, 196.21 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 997.54 us = 0.64% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 5.01 ms = 3.22% latency, 150.78 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 2 ms = 1.28% latency, 605.25 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 15 ms = 9.64% latency, 130.88 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 8 ms = 5.14% latency, 94.36 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 2.99 ms = 1.92% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 3.01 ms = 1.93% latency, 401.66 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 496.39 us = 0.32% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 18.13 ms = 11.64% latency, 108.3 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1.04 ms = 0.67% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 9.61 ms = 6.18% latency, 78.53 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1.04 ms = 0.67% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 5.38 ms = 3.46% latency, 224.57 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 14.97 ms = 9.62% latency, 131.09 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 535.01 us = 0.34% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 8.45 ms = 5.43% latency, 89.37 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 992.77 us = 0.64% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 3.99 ms = 2.57% latency, 302.39 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 14 ms = 8.99% latency, 140.24 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 7 ms = 4.5% latency, 107.89 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 6 ms = 3.85% latency, 201.35 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 981.47 MMACs = 8.33% MACs, 13 ms = 8.35% latency, 150.99 GFLOPS\n",
      "      (ln_1): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.64% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 377.49 MMACs = 3.21% MACs, 5 ms = 3.21% latency, 151.06 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 3 ms = 1.93% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5.13% MACs, 4 ms = 2.57% latency, 301.96 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): MyLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 999.69 us = 0.64% latency, 0 FLOPS)\n",
      ")\n",
      "------------------------------------------------------------------------------\n",
      "[2024-11-20 16:33:28,220] [INFO] [profiler.py:227:end_profile] Flops profiler finished\n"
     ]
    }
   ],
   "source": [
    "flops, macs, params = get_model_profile(\n",
    "    my_ln_model,\n",
    "    kwargs={'input_ids': input_ids},\n",
    "    print_profile=True,\n",
    "    detailed=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-20 16:33:28,230] [INFO] [profiler.py:1220:get_model_profile] Flops profiler warming-up...\n",
      "[2024-11-20 16:33:28,304] [INFO] [profiler.py:81:start_profile] Flops profiler started\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         124.44 M\n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       12.08 GMACs\n",
      "fwd flops per GPU:                                                      24.16 G \n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       24.16 G \n",
      "fwd latency:                                                            122.03 ms\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    197.98 GFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'GPT2Model': '124.44 M'}\n",
      "    MACs        - {'GPT2Model': '12.08 GMACs'}\n",
      "    fwd latency - {'GPT2Model': '122.03 ms'}\n",
      "depth 1:\n",
      "    params      - {'ModuleList': '85.05 M'}\n",
      "    MACs        - {'ModuleList': '12.08 GMACs'}\n",
      "    fwd latency - {'ModuleList': '120.09 ms'}\n",
      "depth 2:\n",
      "    params      - {'GPT2Block': '85.05 M'}\n",
      "    MACs        - {'GPT2Block': '12.08 GMACs'}\n",
      "    fwd latency - {'GPT2Block': '120.09 ms'}\n",
      "depth 3:\n",
      "    params      - {'GPT2MLP': '56.67 M'}\n",
      "    MACs        - {'GPT2MLP': '7.25 GMACs'}\n",
      "    fwd latency - {'GPT2SdpaAttention': '54 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "GPT2Model(\n",
      "  124.44 M = 100% Params, 12.08 GMACs = 100% MACs, 122.03 ms = 100% latency, 197.98 GFLOPS\n",
      "  (wte): Embedding(38.6 M = 31.02% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 50257, 768)\n",
      "  (wpe): Embedding(786.43 K = 0.63% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1024, 768)\n",
      "  (drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 15.01 ms = 12.3% latency, 134.16 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 5 ms = 4.1% latency, 161 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 999.45 us = 0.82% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 9.01 ms = 7.38% latency, 134.14 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 4 ms = 3.27% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 16.51 ms = 13.53% latency, 121.95 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.82% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 6 ms = 4.92% latency, 134.26 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 9.51 ms = 7.79% latency, 127.05 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 10.02 ms = 8.21% latency, 201 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.82% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 5.01 ms = 4.11% latency, 160.61 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 986.1 us = 0.81% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 3.02 ms = 2.47% latency, 400.49 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 8.99 ms = 7.37% latency, 223.92 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 988.72 us = 0.81% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 5 ms = 4.09% latency, 161.17 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1.01 ms = 0.82% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 2 ms = 1.64% latency, 604.24 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 999.69 us = 0.82% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 7.99 ms = 6.55% latency, 251.83 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.82% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 4 ms = 3.28% latency, 201.09 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 2.99 ms = 2.45% latency, 404.1 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 10.58 ms = 8.67% latency, 190.27 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 3.57 ms = 2.93% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 4.01 ms = 3.28% latency, 200.99 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.82% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 2 ms = 1.64% latency, 604.03 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 7 ms = 5.73% latency, 287.73 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 4 ms = 3.28% latency, 201.33 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 994.92 us = 0.82% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 1 ms = 0.82% latency, 1.21 TFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.82% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 6 ms = 4.92% latency, 335.33 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 997.3 us = 0.82% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 999.21 us = 0.82% latency, 805.94 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 3.01 ms = 2.46% latency, 401.76 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 7.99 ms = 6.55% latency, 251.91 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 4.99 ms = 4.09% latency, 161.3 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1.01 ms = 0.83% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 1.99 ms = 1.63% latency, 607.57 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 992.06 us = 0.81% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 10 ms = 8.19% latency, 201.35 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1.02 ms = 0.83% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 5.99 ms = 4.9% latency, 134.55 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 3 ms = 2.46% latency, 402.88 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 12 ms = 9.84% latency, 167.74 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.82% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 4 ms = 3.28% latency, 201.34 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 5 ms = 4.1% latency, 241.67 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): GPT2Block(\n",
      "      7.09 M = 5.7% Params, 1.01 GMACs = 8.33% MACs, 8 ms = 6.56% latency, 251.68 GFLOPS\n",
      "      (ln_1): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        2.36 M = 1.9% Params, 402.65 MMACs = 3.33% MACs, 5 ms = 4.1% latency, 161.07 GFLOPS\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)\n",
      "      (mlp): GPT2MLP(\n",
      "        4.72 M = 3.79% Params, 603.98 MMACs = 5% MACs, 3 ms = 2.46% latency, 402.75 GFLOPS\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.82% latency, 0 FLOPS)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): MySOLayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 1 ms = 0.82% latency, 0 FLOPS)\n",
      ")\n",
      "------------------------------------------------------------------------------\n",
      "[2024-11-20 16:33:28,446] [INFO] [profiler.py:227:end_profile] Flops profiler finished\n"
     ]
    }
   ],
   "source": [
    "flops, macs, params = get_model_profile(\n",
    "    my_so_ln_model,\n",
    "    kwargs={'input_ids': input_ids},\n",
    "    print_profile=True,\n",
    "    detailed=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    ProfilerStep*        50.45%        5.614s       100.00%       11.128s      27.819ms        2.724s        24.65%       11.050s      27.624ms           400  \n",
      "                                       aten::view         1.46%     162.228ms         1.46%     162.228ms       2.704us     284.151ms         2.57%     284.151ms       4.736us         59999  \n",
      "                                     aten::arange         0.23%      25.989ms         0.41%      46.016ms      57.520us      22.396ms         0.20%      48.335ms      60.419us           800  \n",
      "                                      aten::empty         0.53%      58.524ms         0.53%      58.524ms       2.869us     203.871ms         1.85%     203.871ms       9.994us         20400  \n",
      "                                    aten::resize_         0.04%       4.686ms         0.04%       4.686ms       3.905us       9.988ms         0.09%       9.988ms       8.323us          1200  \n",
      "                                  aten::unsqueeze         0.07%       7.310ms         0.07%       7.983ms      19.957us       9.664ms         0.09%      12.840ms      32.100us           400  \n",
      "                                 aten::as_strided         0.47%      52.565ms         0.47%      52.565ms       0.988us     313.901ms         2.84%     313.901ms       5.901us         53198  \n",
      "                                  aten::embedding         0.33%      36.263ms         0.79%      88.299ms     110.374us      18.023ms         0.16%     103.783ms     129.729us           800  \n",
      "                                    aten::reshape         0.09%      10.443ms         0.10%      11.569ms      14.461us       7.262ms         0.07%      11.966ms      14.957us           800  \n",
      "                               aten::index_select         0.32%      35.326ms         0.36%      39.512ms      49.390us      47.081ms         0.43%      71.352ms      89.190us           800  \n",
      "                                        aten::add         5.85%     650.549ms         5.85%     650.549ms      16.428us     742.919ms         6.72%     742.919ms      18.761us         39600  \n",
      "                                    aten::dropout         0.07%       7.934ms         0.07%       7.934ms       0.793us      29.475ms         0.27%      29.475ms       2.947us         10000  \n",
      "                         aten::linalg_vector_norm         1.98%     220.881ms         1.98%     220.881ms      22.088us     179.945ms         1.63%     179.945ms      17.994us         10000  \n",
      "                                        aten::mul         5.54%     616.513ms         5.54%     616.513ms      15.727us     807.089ms         7.30%     807.089ms      20.589us         39200  \n",
      "                                        aten::div         1.44%     160.367ms         1.44%     160.367ms      16.037us     171.811ms         1.55%     171.811ms      17.181us         10000  \n",
      "                                      aten::addmm         8.92%     992.228ms         8.92%     992.228ms      51.679us        3.457s        31.29%        3.457s     180.068us         19200  \n",
      "                                      aten::split         1.84%     204.888ms         5.57%     619.252ms     129.011us      84.692ms         0.77%     512.009ms     106.669us          4800  \n",
      "                                     aten::narrow         1.80%     200.561ms         3.72%     414.364ms      28.775us     290.685ms         2.63%     427.317ms      29.675us         14400  \n",
      "                                      aten::slice         1.77%     197.281ms         1.92%     213.803ms      14.847us      93.509ms         0.85%     136.632ms       9.488us         14400  \n",
      "                                    aten::permute         2.77%     307.808ms         2.89%     321.512ms      22.327us     216.675ms         1.96%     308.208ms      21.403us         14400  \n",
      "               aten::scaled_dot_product_attention         0.94%     104.835ms        11.37%        1.265s     263.514us      75.469ms         0.68%        1.289s     268.478us          4800  \n",
      "    aten::_scaled_dot_product_efficient_attention         3.26%     363.191ms        10.42%        1.160s     241.673us     207.937ms         1.88%        1.213s     252.755us          4800  \n",
      "                                  aten::transpose         3.51%     390.549ms         3.70%     412.217ms      17.176us     297.302ms         2.69%     473.371ms      19.724us         24000  \n",
      "               aten::_efficient_attention_forward         3.75%     416.903ms         4.24%     471.777ms      98.287us     431.382ms         3.90%     611.684ms     127.434us          4800  \n",
      "                                        aten::pow         1.84%     205.096ms         1.89%     210.821ms      43.921us     183.725ms         1.66%     203.468ms      42.389us          4800  \n",
      "                                aten::result_type         0.02%       2.304ms         0.02%       2.304ms       0.480us       9.457ms         0.09%       9.457ms       1.970us          4800  \n",
      "                                         aten::to         0.03%       3.421ms         0.03%       3.421ms       0.713us      10.286ms         0.09%      10.286ms       2.143us          4800  \n",
      "                                       aten::tanh         0.68%      75.283ms         0.68%      75.283ms      15.684us     119.699ms         1.08%     119.699ms      24.937us          4800  \n",
      "                                       aten::view         0.00%       1.900us         0.00%       1.900us       1.900us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                 aten::as_strided         0.00%       1.600us         0.00%       1.600us       0.800us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 11.128s\n",
      "Self CUDA time total: 11.050s\n",
      "\n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    ProfilerStep*        49.00%        5.605s       100.00%       11.438s      28.596ms        3.106s        27.24%       11.402s      28.504ms           400  \n",
      "                                       aten::view         1.35%     154.048ms         1.35%     154.048ms       2.567us     280.714ms         2.46%     280.714ms       4.679us         60000  \n",
      "                                     aten::arange         0.21%      23.999ms         0.36%      41.278ms      51.597us      21.934ms         0.19%      45.827ms      57.284us           800  \n",
      "                                      aten::empty         0.66%      75.112ms         0.66%      75.112ms       2.471us     244.587ms         2.15%     244.587ms       8.046us         30400  \n",
      "                                    aten::resize_         0.26%      29.440ms         0.26%      29.440ms       2.629us      69.917ms         0.61%      69.917ms       6.243us         11200  \n",
      "                                  aten::unsqueeze         0.07%       7.627ms         0.07%       8.258ms      20.645us      10.520ms         0.09%      13.440ms      33.600us           400  \n",
      "                                 aten::as_strided         0.43%      49.449ms         0.43%      49.449ms       0.929us     302.566ms         2.65%     302.566ms       5.687us         53200  \n",
      "                                  aten::embedding         0.31%      35.456ms         0.74%      84.513ms     105.641us      15.076ms         0.13%     100.051ms     125.064us           800  \n",
      "                                    aten::reshape         0.09%       9.820ms         0.10%      10.948ms      13.684us       6.584ms         0.06%      10.898ms      13.623us           800  \n",
      "                               aten::index_select         0.29%      33.139ms         0.32%      37.154ms      46.442us      47.443ms         0.42%      71.518ms      89.397us           800  \n",
      "                                        aten::add         5.50%     628.741ms         5.50%     628.741ms      15.877us     742.949ms         6.52%     742.949ms      18.761us         39600  \n",
      "                                    aten::dropout         0.06%       7.280ms         0.06%       7.280ms       0.728us      29.301ms         0.26%      29.301ms       2.930us         10000  \n",
      "                                       aten::mean         1.57%     180.072ms         1.57%     180.072ms      18.007us     158.619ms         1.39%     158.619ms      15.862us         10000  \n",
      "                                        aten::var         4.54%     519.068ms         4.96%     567.822ms      56.782us     298.777ms         2.62%     410.620ms      41.062us         10000  \n",
      "                                        aten::sub         1.40%     159.942ms         1.40%     159.942ms      15.994us     183.945ms         1.61%     183.945ms      18.395us         10000  \n",
      "                                       aten::sqrt         1.21%     138.710ms         1.21%     138.710ms      13.871us     140.129ms         1.23%     140.129ms      14.013us         10000  \n",
      "                                        aten::div         1.45%     165.436ms         1.45%     165.436ms      16.544us     163.268ms         1.43%     163.268ms      16.327us         10000  \n",
      "                                        aten::mul         3.65%     417.597ms         3.65%     417.597ms      14.301us     665.891ms         5.84%     665.891ms      22.804us         29200  \n",
      "                                      aten::addmm         7.92%     905.679ms         7.92%     905.679ms      47.171us        3.001s        26.32%        3.001s     156.304us         19200  \n",
      "                                      aten::split         1.63%     186.098ms         4.82%     551.445ms     114.884us      77.489ms         0.68%     472.975ms      98.536us          4800  \n",
      "                                     aten::narrow         1.57%     180.111ms         3.19%     365.347ms      25.371us     282.965ms         2.48%     395.486ms      27.464us         14400  \n",
      "                                      aten::slice         1.49%     170.320ms         1.62%     185.236ms      12.864us      77.090ms         0.68%     112.521ms       7.814us         14400  \n",
      "                                    aten::permute         2.72%     311.058ms         2.84%     324.889ms      22.562us     203.089ms         1.78%     290.031ms      20.141us         14400  \n",
      "               aten::scaled_dot_product_attention         0.83%      95.236ms        10.28%        1.176s     245.037us      70.591ms         0.62%        1.245s     259.291us          4800  \n",
      "    aten::_scaled_dot_product_efficient_attention         2.95%     337.653ms         9.45%        1.081s     225.196us     187.355ms         1.64%        1.174s     244.585us          4800  \n",
      "                                  aten::transpose         3.10%     354.337ms         3.27%     374.408ms      15.600us     277.706ms         2.44%     454.979ms      18.957us         24000  \n",
      "               aten::_efficient_attention_forward         3.49%     399.244ms         3.92%     447.927ms      93.318us     432.970ms         3.80%     603.508ms     125.731us          4800  \n",
      "                                        aten::pow         1.62%     185.170ms         1.66%     190.242ms      39.634us     169.954ms         1.49%     189.374ms      39.453us          4800  \n",
      "                                aten::result_type         0.02%       2.112ms         0.02%       2.112ms       0.440us       9.305ms         0.08%       9.305ms       1.939us          4800  \n",
      "                                         aten::to         0.03%       2.960ms         0.03%       2.960ms       0.617us      10.115ms         0.09%      10.115ms       2.108us          4799  \n",
      "                                       aten::tanh         0.60%      68.394ms         0.60%      68.394ms      14.249us     114.078ms         1.00%     114.078ms      23.766us          4800  \n",
      "                                         aten::to         0.00%       0.400us         0.00%       0.400us       0.400us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 11.438s\n",
      "Self CUDA time total: 11.402s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity, schedule\n",
    "\n",
    "my_schedule = schedule(\n",
    "    wait=100,\n",
    "    warmup=50,\n",
    "    active=100,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "my_so_ln_model.eval()\n",
    "my_ln_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    with profile(\n",
    "        activities=[\n",
    "            ProfilerActivity.CPU, ProfilerActivity.CUDA\n",
    "        ],\n",
    "        schedule=my_schedule\n",
    "    ) as prof:\n",
    "        with record_function(\"my_so_ln_model_inference\"):\n",
    "            for _ in range(1000):\n",
    "                my_so_ln_model(input_ids)\n",
    "                prof.step()\n",
    "    print(prof.key_averages().table())\n",
    "    # prof.export_chrome_trace(\"tmp/folded_trace.json\")\n",
    "\n",
    "    with profile(\n",
    "        activities=[\n",
    "            ProfilerActivity.CPU, ProfilerActivity.CUDA\n",
    "        ],\n",
    "        schedule=my_schedule\n",
    "    ) as prof:\n",
    "        with record_function(\"my_ln_model_inference\"):\n",
    "            for _ in range(1000):\n",
    "                my_ln_model(input_ids)\n",
    "                prof.step()\n",
    "    print(prof.key_averages().table())\n",
    "    # prof.export_chrome_trace(\"tmp/original_trace.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
