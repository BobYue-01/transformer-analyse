{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "config = GPT2Config()\n",
    "original_model = GPT2Model(config).cuda()\n",
    "folded_model = GPT2Model(config).cuda()\n",
    "\n",
    "folded_model.load_state_dict(original_model.state_dict())\n",
    "original_model.eval()\n",
    "folded_model.eval()\n",
    "\n",
    "import utils\n",
    "counter = utils.Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <  GPT2Model >\n",
      "   wte : Embedding\n",
      "   wpe : Embedding\n",
      "   drop : Dropout\n",
      "   h : ModuleList\n",
      "   ln_f : LayerNorm\n",
      "   <- MetadataTensor False (1, 128) 0 set()\n",
      "   <  Embedding >\n",
      "     <- MetadataTensor False (1, 128) 0 set()\n",
      "     -> MetadataTensor True (1, 128, 768) 1 {Embedding(50257, 768)}\n",
      "   </ Embedding >\n",
      "   <  Embedding >\n",
      "     <- Tensor None (1, 128) 0 set()\n",
      "     -> MetadataTensor True (1, 128, 768) 1 {Embedding(1024, 768)}\n",
      "   </ Embedding >\n",
      "   <  Dropout >\n",
      "     <- MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "     -> MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "   </ Dropout >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 2 {Embedding(50257, 768), Embedding(1024, 768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 3 {Embedding(50257, 768), Embedding(1024, 768), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 4 {Embedding(50257, 768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 4 {Embedding(50257, 768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 4 {Embedding(50257, 768), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 5 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 6 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 6 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 6 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 7 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 8 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 8 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 8 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 9 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 10 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 10 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 10 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 11 {Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 12 {Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 12 {Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 12 {Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 13 {Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 14 {Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 14 {Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 14 {Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 15 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 16 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 16 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 16 {Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 17 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 18 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 18 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 18 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 19 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 20 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 20 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 20 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 21 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 22 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 22 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 22 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 23 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 24 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  GPT2Block >\n",
      "     ln_1 : LayerNorm\n",
      "     attn : GPT2SdpaAttention\n",
      "     ln_2 : LayerNorm\n",
      "     mlp : GPT2MLP\n",
      "     <- MetadataTensor True (1, 128, 768) 24 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 24 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2SdpaAttention >\n",
      "       c_attn : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       attn_dropout : Dropout\n",
      "       resid_dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 2304) 1 {Conv1D(nf=2304, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 768) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=768)}\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "       -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     </ GPT2SdpaAttention >\n",
      "     <  LayerNorm >\n",
      "       <- MetadataTensor True (1, 128, 768) 25 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "     </ LayerNorm >\n",
      "     <  GPT2MLP >\n",
      "       c_fc : Conv1D\n",
      "       c_proj : Conv1D\n",
      "       act : NewGELUActivation\n",
      "       dropout : Dropout\n",
      "       <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "         -> MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "       </ Conv1D >\n",
      "       <  NewGELUActivation >\n",
      "         <- MetadataTensor True (1, 128, 3072) 1 {Conv1D(nf=3072, nx=768)}\n",
      "         -> MetadataTensor False (1, 128, 3072) 0 set()\n",
      "       </ NewGELUActivation >\n",
      "       <  Conv1D >\n",
      "         <- MetadataTensor False (1, 128, 3072) 0 set()\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Conv1D >\n",
      "       <  Dropout >\n",
      "         <- MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "         -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "       </ Dropout >\n",
      "       -> MetadataTensor True (1, 128, 768) 1 {Conv1D(nf=768, nx=3072)}\n",
      "     </ GPT2MLP >\n",
      "     -> MetadataTensor True (1, 128, 768) 26 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "     -> MetadataTensor False (1, 12, 128, 64) 0 set()\n",
      "   </ GPT2Block >\n",
      "   <  LayerNorm >\n",
      "     <- MetadataTensor True (1, 128, 768) 26 {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Embedding(50257, 768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072)}\n",
      "     -> MetadataTensor True (1, 128, 768) 1 {LayerNorm((768,), eps=1e-05, elementwise_affine=True)}\n",
      "   </ LayerNorm >\n",
      "LayerNorm: 25\n",
      "Foldable: 25\n",
      "Center modules: {Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Embedding(50257, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=3072), Embedding(1024, 768), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=3072), Conv1D(nf=768, nx=768), Conv1D(nf=768, nx=768)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BobYu\\miniconda3\\envs\\torch\\Lib\\site-packages\\torch\\_tensor.py:1437: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "hook_pre_fn, hook_fn = utils.create_analyse_hook_fns(counter)\n",
    "\n",
    "input_ids = torch.randint(0, 1000, (1, 128)).cuda()\n",
    "my_input_ids = utils.MetadataTensor(input_ids, centered=False).cuda()\n",
    "\n",
    "with utils.HookManager(folded_model, hook_fn, hook_pre_fn):\n",
    "    folded_model(my_input_ids)\n",
    "\n",
    "print('LayerNorm:', counter.ln_cnt)\n",
    "print('Foldable:', counter.foldable_cnt)\n",
    "print('Center modules:', counter.center_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules\n",
    "\n",
    "for layer in counter.layernorms:\n",
    "    modules.replace_layer_norm_forward(layer)\n",
    "\n",
    "for layer in counter.center_modules:\n",
    "    modules.center_modules(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_queue = []\n",
    "check = utils.Check()\n",
    "replace = False\n",
    "\n",
    "def hook_original(module, input, output):\n",
    "    name = module.__class__.__name__\n",
    "    output_queue.append((output, name))\n",
    "\n",
    "    # if isinstance(output, tuple):\n",
    "    #     output = output[0]\n",
    "\n",
    "    # with torch._tensor_str.printoptions(precision=10, sci_mode=True):\n",
    "    #     len_shape = len(output.shape)\n",
    "    #     index = tuple([0] * (len_shape - 2) + [slice(None, 4), slice(None, 4)])\n",
    "    #     print(module.__class__.__name__, output[index])\n",
    "\n",
    "def check_close_and_replace(tensor_a, tensor_b, check: utils.Check, tensor_a_str, tensor_b_str):\n",
    "    check.hide_val()\n",
    "    locals()[tensor_a_str] = tensor_a\n",
    "    locals()[tensor_b_str] = tensor_b\n",
    "    if check.check_eq(tensor_a_str, tensor_b_str, abs_tol=1e-5, local_vars=locals()):\n",
    "        if replace and isinstance(tensor_a, torch.Tensor) and isinstance(tensor_b, torch.Tensor):\n",
    "            tensor_b.data = tensor_a.data\n",
    "    check.show_val()\n",
    "\n",
    "def apply_func_to_nested_tuple_pair(t1, t2, func, *args, **kwargs):\n",
    "    if isinstance(t1, tuple) and isinstance(t2, tuple):\n",
    "        return tuple(apply_func_to_nested_tuple_pair(x1, x2, func, *args, **kwargs) for x1, x2 in zip(t1, t2))\n",
    "    else:\n",
    "        return func(t1, t2, *args, **kwargs)\n",
    "\n",
    "def hook_folded(module, input, output):\n",
    "    folded_name = module.__class__.__name__ + '_folded'\n",
    "    original_output, original_name = output_queue.pop(0)\n",
    "    original_name += '_original'\n",
    "    apply_func_to_nested_tuple_pair(original_output, output, check_close_and_replace, check, original_name, folded_name)\n",
    "\n",
    "    # if isinstance(output, tuple):\n",
    "    #     output0 = output[0]\n",
    "\n",
    "    # with torch._tensor_str.printoptions(precision=10, sci_mode=True):\n",
    "    #     len_shape = len(output0.shape)\n",
    "    #     index = tuple([0] * (len_shape - 2) + [slice(None, 4), slice(None, 4)])\n",
    "    #     print(module.__class__.__name__, output0[index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34m# 0 [ Test ] Embedding_original ?= Embedding_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0005862714606337249\u001b[0m\n",
      "\u001b[1;31m# 0 [ Fail ] Embedding_original != Embedding_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 1 [ Test ] Embedding_original ?= Embedding_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0006422363221645355\u001b[0m\n",
      "\u001b[1;31m# 1 [ Fail ] Embedding_original != Embedding_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 2 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0008070007897913456\u001b[0m\n",
      "\u001b[1;31m# 2 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 3 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 3 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 4 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 4 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 5 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0002815908519551158\u001b[0m\n",
      "\u001b[1;31m# 5 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 6 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0002815908519551158\u001b[0m\n",
      "\u001b[1;31m# 6 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 7 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0002815908519551158\u001b[0m\n",
      "\u001b[1;31m# 7 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 8 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 8 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 9 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 9 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 10 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 10 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 11 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 11 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 12 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 12 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 13 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 13 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 14 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0020238985307514668\u001b[0m\n",
      "\u001b[1;31m# 14 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 15 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0020238985307514668\u001b[0m\n",
      "\u001b[1;31m# 15 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 16 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0020238985307514668\u001b[0m\n",
      "\u001b[1;31m# 16 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 17 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0020264380145817995\u001b[0m\n",
      "\u001b[1;31m# 17 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 18 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 18 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 19 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 19 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 20 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 20 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 21 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 21 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 22 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 22 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 23 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0005615214467979968\u001b[0m\n",
      "\u001b[1;31m# 23 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 24 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0005615214467979968\u001b[0m\n",
      "\u001b[1;31m# 24 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 25 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0005615214467979968\u001b[0m\n",
      "\u001b[1;31m# 25 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 26 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 26 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 27 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 27 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 28 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 28 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 29 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 29 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 30 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 30 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 31 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 31 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 32 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0032453748863190413\u001b[0m\n",
      "\u001b[1;31m# 32 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 33 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0032453748863190413\u001b[0m\n",
      "\u001b[1;31m# 33 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 34 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0032453748863190413\u001b[0m\n",
      "\u001b[1;31m# 34 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 35 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.004427415784448385\u001b[0m\n",
      "\u001b[1;31m# 35 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 36 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 36 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 37 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 37 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 38 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 38 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 39 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 39 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 40 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 40 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 41 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.00045808861614204943\u001b[0m\n",
      "\u001b[1;31m# 41 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 42 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.00045808861614204943\u001b[0m\n",
      "\u001b[1;31m# 42 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 43 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.00045808861614204943\u001b[0m\n",
      "\u001b[1;31m# 43 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 44 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 44 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 45 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 45 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 46 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 46 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 47 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 47 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 48 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 48 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 49 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 49 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 50 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0017248853109776974\u001b[0m\n",
      "\u001b[1;31m# 50 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 51 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0017248853109776974\u001b[0m\n",
      "\u001b[1;31m# 51 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 52 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0017248853109776974\u001b[0m\n",
      "\u001b[1;31m# 52 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 53 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.004650811664760113\u001b[0m\n",
      "\u001b[1;31m# 53 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 54 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 54 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 55 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 55 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 56 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 56 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 57 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 57 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 58 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 58 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 59 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0010710005881264806\u001b[0m\n",
      "\u001b[1;31m# 59 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 60 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0010710005881264806\u001b[0m\n",
      "\u001b[1;31m# 60 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 61 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0010710005881264806\u001b[0m\n",
      "\u001b[1;31m# 61 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 62 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 62 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 63 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 63 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 64 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 64 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 65 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 65 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 66 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 66 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 67 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 67 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 68 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.001607805141247809\u001b[0m\n",
      "\u001b[1;31m# 68 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 69 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.001607805141247809\u001b[0m\n",
      "\u001b[1;31m# 69 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 70 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.001607805141247809\u001b[0m\n",
      "\u001b[1;31m# 70 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 71 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.004080196842551231\u001b[0m\n",
      "\u001b[1;31m# 71 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 72 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 72 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 73 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 73 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 74 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 74 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 75 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 75 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 76 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 76 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 77 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0004386283690109849\u001b[0m\n",
      "\u001b[1;31m# 77 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 78 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0004386283690109849\u001b[0m\n",
      "\u001b[1;31m# 78 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 79 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0004386283690109849\u001b[0m\n",
      "\u001b[1;31m# 79 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 80 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 80 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 81 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 81 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 82 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 82 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 83 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 83 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 84 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 84 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 85 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 85 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 86 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0020019225776195526\u001b[0m\n",
      "\u001b[1;31m# 86 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 87 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0020019225776195526\u001b[0m\n",
      "\u001b[1;31m# 87 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 88 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0020019225776195526\u001b[0m\n",
      "\u001b[1;31m# 88 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 89 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.00464108120650053\u001b[0m\n",
      "\u001b[1;31m# 89 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 90 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 90 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 91 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 91 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 92 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 92 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 93 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 93 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 94 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 94 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 95 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0026832385919988155\u001b[0m\n",
      "\u001b[1;31m# 95 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 96 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0026832385919988155\u001b[0m\n",
      "\u001b[1;31m# 96 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 97 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0026832385919988155\u001b[0m\n",
      "\u001b[1;31m# 97 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 98 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 98 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 99 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 99 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 100 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 100 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 101 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 101 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 102 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 102 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 103 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 103 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 104 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0014515225775539875\u001b[0m\n",
      "\u001b[1;31m# 104 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 105 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0014515225775539875\u001b[0m\n",
      "\u001b[1;31m# 105 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 106 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0014515225775539875\u001b[0m\n",
      "\u001b[1;31m# 106 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 107 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0042579746805131435\u001b[0m\n",
      "\u001b[1;31m# 107 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 108 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 108 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 109 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 109 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 110 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 110 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 111 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 111 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 112 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 112 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 113 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0019257236272096634\u001b[0m\n",
      "\u001b[1;31m# 113 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 114 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0019257236272096634\u001b[0m\n",
      "\u001b[1;31m# 114 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 115 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0019257236272096634\u001b[0m\n",
      "\u001b[1;31m# 115 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 116 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 116 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 117 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 117 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 118 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 118 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 119 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 119 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 120 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 120 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 121 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 121 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 122 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.002370656467974186\u001b[0m\n",
      "\u001b[1;31m# 122 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 123 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.002370656467974186\u001b[0m\n",
      "\u001b[1;31m# 123 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 124 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.002370656467974186\u001b[0m\n",
      "\u001b[1;31m# 124 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 125 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.004524326883256435\u001b[0m\n",
      "\u001b[1;31m# 125 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 126 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 126 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 127 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 127 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 128 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 128 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 129 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 129 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 130 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 130 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 131 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.00039303634548559785\u001b[0m\n",
      "\u001b[1;31m# 131 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 132 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.00039303634548559785\u001b[0m\n",
      "\u001b[1;31m# 132 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 133 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.00039303634548559785\u001b[0m\n",
      "\u001b[1;31m# 133 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 134 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 134 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 135 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 135 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 136 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 136 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 137 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 137 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 138 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 138 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 139 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 139 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 140 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0025664740242064\u001b[0m\n",
      "\u001b[1;31m# 140 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 141 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0025664740242064\u001b[0m\n",
      "\u001b[1;31m# 141 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 142 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0025664740242064\u001b[0m\n",
      "\u001b[1;31m# 142 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 143 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.005096031352877617\u001b[0m\n",
      "\u001b[1;31m# 143 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 144 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 144 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 145 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 145 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 146 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 146 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 147 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 147 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 148 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 148 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 149 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.001677055610343814\u001b[0m\n",
      "\u001b[1;31m# 149 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 150 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.001677055610343814\u001b[0m\n",
      "\u001b[1;31m# 150 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 151 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.001677055610343814\u001b[0m\n",
      "\u001b[1;31m# 151 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 152 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 152 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 153 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 153 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 154 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 154 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 155 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 155 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 156 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 156 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 157 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 157 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 158 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0014240312157198787\u001b[0m\n",
      "\u001b[1;31m# 158 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 159 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0014240312157198787\u001b[0m\n",
      "\u001b[1;31m# 159 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 160 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0014240312157198787\u001b[0m\n",
      "\u001b[1;31m# 160 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 161 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.005401648581027985\u001b[0m\n",
      "\u001b[1;31m# 161 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 162 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 162 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 163 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 163 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 164 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 164 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 165 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 165 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 166 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 166 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 167 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0014029918238520622\u001b[0m\n",
      "\u001b[1;31m# 167 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 168 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0014029918238520622\u001b[0m\n",
      "\u001b[1;31m# 168 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 169 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0014029918238520622\u001b[0m\n",
      "\u001b[1;31m# 169 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 170 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 170 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 171 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 171 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 172 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 172 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 173 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 173 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 174 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 174 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 175 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 175 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 176 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.00371006247587502\u001b[0m\n",
      "\u001b[1;31m# 176 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 177 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.00371006247587502\u001b[0m\n",
      "\u001b[1;31m# 177 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 178 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.00371006247587502\u001b[0m\n",
      "\u001b[1;31m# 178 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 179 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.005983787588775158\u001b[0m\n",
      "\u001b[1;31m# 179 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 180 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 180 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 181 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 181 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 182 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 182 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 183 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 183 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 184 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 184 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 185 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0019292532233521342\u001b[0m\n",
      "\u001b[1;31m# 185 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 186 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0019292532233521342\u001b[0m\n",
      "\u001b[1;31m# 186 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 187 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0019292532233521342\u001b[0m\n",
      "\u001b[1;31m# 187 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 188 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 188 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 189 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 189 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 190 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 190 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 191 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 191 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 192 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 192 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 193 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 193 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 194 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0018535529961809516\u001b[0m\n",
      "\u001b[1;31m# 194 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 195 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0018535529961809516\u001b[0m\n",
      "\u001b[1;31m# 195 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 196 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0018535529961809516\u001b[0m\n",
      "\u001b[1;31m# 196 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 197 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.006284954957664013\u001b[0m\n",
      "\u001b[1;31m# 197 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 198 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 198 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 199 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 199 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 200 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 200 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 201 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 201 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 202 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 202 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 203 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0010547102428972721\u001b[0m\n",
      "\u001b[1;31m# 203 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 204 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0010547102428972721\u001b[0m\n",
      "\u001b[1;31m# 204 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 205 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0010547102428972721\u001b[0m\n",
      "\u001b[1;31m# 205 [ Fail ] GPT2SdpaAttention_original != GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 206 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 206 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 207 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 207 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 208 [ Test ] GPT2SdpaAttention_original ?= GPT2SdpaAttention_folded\u001b[0m\n",
      "\u001b[1;32m# 208 [ Pass ] GPT2SdpaAttention_original == GPT2SdpaAttention_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 209 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 209 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 210 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;32m# 210 [ Pass ] Conv1D_original == Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 211 [ Test ] NewGELUActivation_original ?= NewGELUActivation_folded\u001b[0m\n",
      "\u001b[1;32m# 211 [ Pass ] NewGELUActivation_original == NewGELUActivation_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 212 [ Test ] Conv1D_original ?= Conv1D_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0016080813948065042\u001b[0m\n",
      "\u001b[1;31m# 212 [ Fail ] Conv1D_original != Conv1D_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 213 [ Test ] Dropout_original ?= Dropout_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0016080813948065042\u001b[0m\n",
      "\u001b[1;31m# 213 [ Fail ] Dropout_original != Dropout_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 214 [ Test ] GPT2MLP_original ?= GPT2MLP_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0016080813948065042\u001b[0m\n",
      "\u001b[1;31m# 214 [ Fail ] GPT2MLP_original != GPT2MLP_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 215 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;33mMean abs diff: 0.0066222501918673515\u001b[0m\n",
      "\u001b[1;31m# 215 [ Fail ] GPT2Block_original != GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 216 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 216 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 217 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 217 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 218 [ Test ] GPT2Block_original ?= GPT2Block_folded\u001b[0m\n",
      "\u001b[1;32m# 218 [ Pass ] GPT2Block_original == GPT2Block_folded\u001b[0m\n",
      "\n",
      "\u001b[1;34m# 219 [ Test ] LayerNorm_original ?= SOLayerNorm_folded\u001b[0m\n",
      "\u001b[1;32m# 219 [ Pass ] LayerNorm_original == SOLayerNorm_folded\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with utils.HookManager(original_model, hook_original, None, list(original_model.modules())[1:]):\n",
    "    original_out = original_model(input_ids)\n",
    "\n",
    "with utils.HookManager(folded_model, hook_folded, None, list(folded_model.modules())[1:]):\n",
    "    folded_out = folded_model(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34m# 220 [ Test ] folded_out[0] ?= original_out[0]\u001b[0m\n",
      "\u001b[2m=== folded_out[0] ===\u001b[0m\n",
      "tensor([[[2.1711e-02, -1.4150e-01, -3.4320e-01,  ..., 3.9684e-03, 6.5941e-01, -4.2921e-01],\n",
      "         [5.1380e-01, 4.2810e-01, 7.8693e-01,  ..., -9.6909e-01, 8.7120e-01, -1.2768e+00],\n",
      "         [7.8011e-01, 7.4773e-01, -2.6297e-01,  ..., 6.7087e-03, 1.0873e+00, -5.5771e-01],\n",
      "         ...,\n",
      "         [1.8941e-01, 7.3671e-01, -6.8908e-01,  ..., -2.5222e-01, -3.3449e-01, -2.5983e-01],\n",
      "         [3.0729e-02, 6.8014e-01, -1.0200e+00,  ..., -1.9081e-01, 1.1007e+00, -6.1009e-01],\n",
      "         [3.8300e-02, 2.6348e+00, -8.0384e-01,  ..., -1.8258e-01, 1.4888e+00, 7.4053e-01]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "\u001b[2m=== original_out[0] ===\u001b[0m\n",
      "tensor([[[2.1712e-02, -1.4150e-01, -3.4320e-01,  ..., 3.9669e-03, 6.5941e-01, -4.2921e-01],\n",
      "         [5.1380e-01, 4.2810e-01, 7.8693e-01,  ..., -9.6909e-01, 8.7120e-01, -1.2768e+00],\n",
      "         [7.8011e-01, 7.4773e-01, -2.6297e-01,  ..., 6.7078e-03, 1.0873e+00, -5.5771e-01],\n",
      "         ...,\n",
      "         [1.8941e-01, 7.3671e-01, -6.8908e-01,  ..., -2.5222e-01, -3.3449e-01, -2.5983e-01],\n",
      "         [3.0730e-02, 6.8014e-01, -1.0200e+00,  ..., -1.9081e-01, 1.1007e+00, -6.1009e-01],\n",
      "         [3.8301e-02, 2.6348e+00, -8.0384e-01,  ..., -1.8258e-01, 1.4888e+00, 7.4053e-01]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "\u001b[1;32m# 220 [ Pass ] folded_out[0] == original_out[0]\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check.check_eq('folded_out[0]', 'original_out[0]', local_vars=locals(), abs_tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34m==== < Summary > ====\u001b[0m\n",
      "\u001b[1;31m# 0 [ Fail ]\u001b[0m Embedding_original != Embedding_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 1 [ Fail ]\u001b[0m Embedding_original != Embedding_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 2 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 3 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 4 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 5 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 6 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 7 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 8 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 9 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 10 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 11 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 12 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 13 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 14 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 15 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 16 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 17 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 18 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 19 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 20 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 21 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 22 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 23 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 24 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 25 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 26 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 27 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 28 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 29 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 30 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 31 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 32 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 33 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 34 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 35 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 36 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 37 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 38 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 39 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 40 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 41 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 42 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 43 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 44 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 45 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 46 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 47 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 48 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 49 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 50 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 51 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 52 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 53 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 54 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 55 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 56 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 57 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 58 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 59 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 60 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 61 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 62 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 63 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 64 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 65 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 66 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 67 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 68 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 69 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 70 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 71 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 72 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 73 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 74 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 75 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 76 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 77 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 78 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 79 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 80 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 81 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 82 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 83 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 84 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 85 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 86 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 87 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 88 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 89 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 90 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 91 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 92 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 93 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 94 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 95 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 96 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 97 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 98 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 99 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 100 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 101 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 102 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 103 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 104 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 105 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 106 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 107 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 108 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 109 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 110 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 111 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 112 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 113 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 114 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 115 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 116 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 117 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 118 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 119 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 120 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 121 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 122 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 123 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 124 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 125 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 126 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 127 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 128 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 129 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 130 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 131 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 132 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 133 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 134 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 135 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 136 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 137 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 138 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 139 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 140 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 141 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 142 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 143 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 144 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 145 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 146 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 147 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 148 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 149 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 150 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 151 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 152 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 153 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 154 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 155 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 156 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 157 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 158 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 159 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 160 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 161 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 162 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 163 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 164 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 165 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 166 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 167 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 168 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 169 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 170 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 171 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 172 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 173 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 174 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 175 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 176 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 177 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 178 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 179 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 180 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 181 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 182 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 183 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 184 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 185 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 186 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 187 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 188 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 189 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 190 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 191 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 192 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 193 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 194 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 195 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 196 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 197 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 198 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 199 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 200 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 201 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 202 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 203 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 204 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 205 [ Fail ]\u001b[0m GPT2SdpaAttention_original != GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 206 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 207 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 208 [ Pass ]\u001b[0m GPT2SdpaAttention_original == GPT2SdpaAttention_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 209 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 210 [ Pass ]\u001b[0m Conv1D_original == Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 211 [ Pass ]\u001b[0m NewGELUActivation_original == NewGELUActivation_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 212 [ Fail ]\u001b[0m Conv1D_original != Conv1D_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 213 [ Fail ]\u001b[0m Dropout_original != Dropout_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 214 [ Fail ]\u001b[0m GPT2MLP_original != GPT2MLP_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;31m# 215 [ Fail ]\u001b[0m GPT2Block_original != GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 216 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 217 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 218 [ Pass ]\u001b[0m GPT2Block_original == GPT2Block_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 219 [ Pass ]\u001b[0m LayerNorm_original == SOLayerNorm_folded \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "\u001b[1;32m# 220 [ Pass ]\u001b[0m folded_out[0] == original_out[0] \u001b[2m(rel_tol=1e-05, abs_tol=1e-05)\u001b[0m\n",
      "-------------------\n",
      "\u001b[1;34m(134/221) [\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;31mX\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;32m=\u001b[0m\u001b[1;34m]\u001b[0m\n",
      "\u001b[1;34m==== </Summary > ====\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = GPT2Model(config).cuda()\n",
    "folded_model = GPT2Model(config).cuda()\n",
    "\n",
    "folded_model.load_state_dict(original_model.state_dict())\n",
    "original_model.eval()\n",
    "folded_model.eval()\n",
    "\n",
    "hook_pre_fn, hook_fn = utils.create_analyse_hook_fns(counter, _print=False)\n",
    "\n",
    "original_counter = utils.Counter()\n",
    "folded_counter = utils.Counter()\n",
    "\n",
    "with utils.HookManager(original_model, hook_fn, hook_pre_fn):\n",
    "    original_model(my_input_ids)\n",
    "\n",
    "with utils.HookManager(folded_model, hook_fn, hook_pre_fn):\n",
    "    folded_model(my_input_ids)\n",
    "\n",
    "for layer in original_counter.layernorms:\n",
    "    modules.replace_layer_norm_forward(layer, forward_fn=modules.myln_forward)\n",
    "\n",
    "for layer in folded_counter.layernorms:\n",
    "    modules.replace_layer_norm_forward(layer, forward_fn=modules.soln_forward)\n",
    "\n",
    "for layer in folded_counter.center_modules:\n",
    "    modules.center_modules(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         original_model_inference        46.18%        3.649s       100.00%        7.900s        7.900s     450.348ms         5.68%        7.933s        7.933s             1  \n",
      "                                       aten::view         1.16%      91.986ms         1.16%      91.986ms       4.599us     174.355ms         2.20%     174.355ms       8.718us         20000  \n",
      "                                     aten::arange         0.13%      10.381ms         0.22%      17.456ms      87.282us       8.352ms         0.11%      14.069ms      70.345us           200  \n",
      "                                      aten::empty         0.91%      71.872ms         0.91%      71.872ms       5.704us     240.784ms         3.04%     240.784ms      19.110us         12600  \n",
      "                                    aten::resize_         0.02%       1.934ms         0.02%       1.934ms       6.446us       2.581ms         0.03%       2.581ms       8.603us           300  \n",
      "                                  aten::unsqueeze         0.05%       3.696ms         0.05%       4.285ms      42.850us       2.336ms         0.03%       3.339ms      33.390us           100  \n",
      "                                 aten::as_strided         0.32%      25.512ms         0.32%      25.512ms       1.918us     109.981ms         1.39%     109.981ms       8.269us         13300  \n",
      "                                  aten::embedding         0.21%      16.445ms         0.50%      39.331ms     196.654us       6.168ms         0.08%      37.823ms     189.115us           200  \n",
      "                                    aten::reshape         0.06%       4.691ms         0.07%       5.137ms      25.683us       3.523ms         0.04%       4.813ms      24.065us           200  \n",
      "                               aten::index_select         0.20%      15.419ms         0.22%      17.348ms      86.740us      16.164ms         0.20%      26.084ms     130.420us           200  \n",
      "                                        aten::add         2.25%     177.977ms         2.25%     177.977ms      36.322us     692.975ms         8.74%     692.975ms     141.423us          4900  \n",
      "                                    aten::dropout         0.05%       4.162ms         0.05%       4.162ms       1.665us       7.378ms         0.09%       7.378ms       2.951us          2500  \n",
      "                                 aten::layer_norm         1.28%     101.123ms         8.78%     693.640ms     277.456us      23.190ms         0.29%     425.096ms     170.038us          2500  \n",
      "                          aten::native_layer_norm         6.83%     539.808ms         7.50%     592.518ms     237.007us     140.167ms         1.77%     401.906ms     160.762us          2500  \n",
      "                                      aten::addmm         6.10%     482.288ms         6.10%     482.288ms     100.477us        3.938s        49.64%        3.938s     820.439us          4800  \n",
      "                                      aten::split         1.35%     106.908ms         6.45%     509.830ms     424.859us      25.630ms         0.32%     202.197ms     168.498us          1200  \n",
      "                                     aten::narrow         3.72%     293.564ms         5.10%     402.923ms     111.923us     130.045ms         1.64%     176.567ms      49.046us          3600  \n",
      "                                      aten::slice         1.28%     101.233ms         1.38%     109.359ms      30.377us      30.281ms         0.38%      46.522ms      12.923us          3600  \n",
      "                                    aten::permute         3.80%     300.211ms         3.88%     306.883ms      85.245us      60.566ms         0.76%      97.469ms      27.075us          3600  \n",
      "               aten::scaled_dot_product_attention         0.68%      53.901ms        15.08%        1.191s     992.522us      25.055ms         0.32%     565.413ms     471.178us          1200  \n",
      "    aten::_scaled_dot_product_efficient_attention         8.48%     669.962ms        14.39%        1.137s     947.604us      84.003ms         1.06%     540.358ms     450.298us          1200  \n",
      "                                  aten::transpose         3.10%     244.908ms         3.23%     255.034ms      42.506us      91.129ms         1.15%     146.963ms      24.494us          6000  \n",
      "               aten::_efficient_attention_forward         2.84%     224.458ms         3.21%     253.388ms     211.157us     246.667ms         3.11%     318.367ms     265.306us          1200  \n",
      "                                        aten::mul         1.96%     155.071ms         1.96%     155.071ms      32.306us     935.735ms        11.80%     935.735ms     194.945us          4800  \n",
      "                                        aten::pow         6.38%     504.414ms         6.43%     507.652ms     423.043us     255.896ms         3.23%     262.445ms     218.704us          1200  \n",
      "                                aten::result_type         0.02%       1.493ms         0.02%       1.493ms       1.244us       3.261ms         0.04%       3.261ms       2.717us          1200  \n",
      "                                         aten::to         0.02%       1.745ms         0.02%       1.745ms       1.454us       3.288ms         0.04%       3.288ms       2.740us          1200  \n",
      "                                       aten::tanh         0.59%      46.322ms         0.59%      46.322ms      38.602us     224.722ms         2.83%     224.722ms     187.268us          1200  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 7.900s\n",
      "Self CUDA time total: 7.933s\n",
      "\n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                           folded_model_inference        43.43%        3.356s       100.00%        7.726s        7.726s     457.200ms         5.89%        7.762s        7.762s             1  \n",
      "                                       aten::view         1.20%      92.408ms         1.20%      92.408ms       4.620us     161.112ms         2.08%     161.112ms       8.056us         20000  \n",
      "                                     aten::arange         0.15%      11.534ms         0.25%      19.295ms      96.475us       8.695ms         0.11%      14.830ms      74.150us           200  \n",
      "                                      aten::empty         0.92%      71.387ms         0.92%      71.387ms       5.666us     243.359ms         3.14%     243.359ms      19.314us         12600  \n",
      "                                    aten::resize_         0.02%       1.763ms         0.02%       1.763ms       5.877us       2.091ms         0.03%       2.091ms       6.970us           300  \n",
      "                                  aten::unsqueeze         0.04%       3.209ms         0.05%       3.563ms      35.629us       1.845ms         0.02%       2.991ms      29.910us           100  \n",
      "                                 aten::as_strided         0.32%      24.425ms         0.32%      24.425ms       1.836us     111.888ms         1.44%     111.888ms       8.413us         13300  \n",
      "                                  aten::embedding         0.19%      14.443ms         0.52%      40.402ms     202.010us       5.859ms         0.08%      35.293ms     176.465us           200  \n",
      "                                    aten::reshape         0.07%       5.556ms         0.08%       6.097ms      30.487us       3.445ms         0.04%       5.053ms      25.265us           200  \n",
      "                               aten::index_select         0.23%      17.666ms         0.25%      19.432ms      97.162us      14.713ms         0.19%      23.511ms     117.555us           200  \n",
      "                                        aten::add         2.28%     176.452ms         2.28%     176.452ms      36.011us     702.814ms         9.05%     702.814ms     143.431us          4900  \n",
      "                                    aten::dropout         0.05%       4.205ms         0.05%       4.205ms       1.682us       7.375ms         0.10%       7.375ms       2.950us          2500  \n",
      "                                 aten::layer_norm         1.42%     110.074ms         8.71%     672.926ms     269.170us      21.675ms         0.28%     416.842ms     166.737us          2500  \n",
      "                          aten::native_layer_norm         6.56%     507.179ms         7.28%     562.852ms     225.141us     135.991ms         1.75%     395.167ms     158.067us          2500  \n",
      "                                      aten::addmm         6.11%     472.094ms         6.11%     472.094ms      98.353us        3.757s        48.41%        3.757s     782.766us          4800  \n",
      "                                      aten::split         1.34%     103.197ms         6.70%     517.591ms     431.326us      25.414ms         0.33%     210.229ms     175.191us          1200  \n",
      "                                     aten::narrow         4.07%     314.692ms         5.36%     414.394ms     115.109us     138.270ms         1.78%     184.815ms      51.337us          3600  \n",
      "                                      aten::slice         1.19%      91.692ms         1.29%      99.702ms      27.695us      29.289ms         0.38%      46.545ms      12.929us          3600  \n",
      "                                    aten::permute         3.32%     256.458ms         3.40%     262.831ms      73.009us      60.926ms         0.78%      97.045ms      26.957us          3600  \n",
      "               aten::scaled_dot_product_attention         0.62%      47.967ms        17.75%        1.372s       1.143ms      25.132ms         0.32%     558.696ms     465.580us          1200  \n",
      "    aten::_scaled_dot_product_efficient_attention        11.44%     884.161ms        17.13%        1.324s       1.103ms      77.453ms         1.00%     533.564ms     444.637us          1200  \n",
      "                                  aten::transpose         3.21%     248.233ms         3.34%     257.921ms      42.987us      97.364ms         1.25%     154.731ms      25.788us          6000  \n",
      "               aten::_efficient_attention_forward         2.51%     193.901ms         2.85%     220.005ms     183.337us     242.825ms         3.13%     310.535ms     258.779us          1200  \n",
      "                                        aten::mul         2.01%     155.432ms         2.01%     155.432ms      32.382us     914.107ms        11.78%     914.107ms     190.439us          4800  \n",
      "                                        aten::pow         6.78%     523.857ms         6.82%     526.937ms     439.114us     289.929ms         3.74%     296.631ms     247.192us          1200  \n",
      "                                aten::result_type         0.02%       1.269ms         0.02%       1.269ms       1.058us       2.626ms         0.03%       2.626ms       2.188us          1200  \n",
      "                                         aten::to         0.02%       1.810ms         0.02%       1.810ms       1.509us       4.076ms         0.05%       4.076ms       3.397us          1200  \n",
      "                                       aten::tanh         0.46%      35.485ms         0.46%      35.485ms      29.571us     219.280ms         2.83%     219.280ms     182.733us          1200  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 7.726s\n",
      "Self CUDA time total: 7.762s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with torch.no_grad():\n",
    "    with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA\n",
    "    ]) as prof:\n",
    "        with record_function(\"original_model_inference\"):\n",
    "            for _ in range(100):\n",
    "                original_model(input_ids)\n",
    "    print(prof.key_averages().table())\n",
    "    prof.export_chrome_trace(\"original_trace.json\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA\n",
    "    ]) as prof:\n",
    "        with record_function(\"folded_model_inference\"):\n",
    "            for _ in range(100):\n",
    "                folded_model(input_ids)\n",
    "    print(prof.key_averages().table())\n",
    "    prof.export_chrome_trace(\"folded_trace.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
