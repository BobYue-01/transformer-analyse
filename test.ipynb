{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BobYu\\miniconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\BobYu\\miniconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from transformers import GPT2Config, GPT2Model\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# config = GPT2Config()\n",
    "# model = GPT2Model(config)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt2.svg'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "input_ids = torch.randint(0, 1000, (1, 128))\n",
    "out = model(input_ids)\n",
    "\n",
    "logits = out.logits\n",
    "\n",
    "net_vis = make_dot(logits, params=dict(model.named_parameters()), show_attrs=False, show_saved=False)\n",
    "net_vis.format = 'svg'\n",
    "net_vis.render('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BobYu\\miniconda3\\envs\\torch\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "c:\\Users\\BobYu\\miniconda3\\envs\\torch\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n"
     ]
    }
   ],
   "source": [
    "# import hiddenlayer as hl\n",
    "# g = hl.build_graph(model, input_ids)\n",
    "# g.save(\"gpt2_model_hl\", format=\"png\")\n",
    "\n",
    "# save the model as onnx\n",
    "input_names = [\"input_ids\"]\n",
    "output_names = [\"logits\"]\n",
    "torch.onnx.export(model, input_ids, \"gpt2.onnx\", input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 is a model developed by OpenAI. It allows us to do more with less at a higher cost through better user experience while giving you more control over how you use your smart phone.\n",
      "\n",
      "If you are like most Android users, you are familiar with the Android operating system as it is generally used as a browser or application on the Internet, but there is a vast amount of information available about it that can only be viewed through your phone. So, it has a big impact on\n"
     ]
    }
   ],
   "source": [
    "prompt = \"GPT2 is a model developed by OpenAI.\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    "    attention_mask=attention_mask,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_info = []\n",
    "hooks = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_pre_fn(module, input):\n",
    "    try:\n",
    "        input_shape = tuple(tuple(x.size()) for x in input)\n",
    "    except AttributeError:\n",
    "        input_shape = None\n",
    "\n",
    "    layer_info.append(('pre', module.__class__.__name__, input_shape))\n",
    "\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    try:\n",
    "        output_shape = tuple(tuple(x.size()) for x in output)\n",
    "    except AttributeError:\n",
    "        output_shape = None\n",
    "\n",
    "    layer_info.append(('post', module.__class__.__name__, output_shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   <  GPT2LMHeadModel <-  ((1, 128),) >\n",
      "     <  GPT2Model <-  ((1, 128),) >\n",
      "       <  Embedding <-  ((1, 128),) >\n",
      "       </ Embedding  -> ((128, 768),) >\n",
      "       <  Embedding <-  ((1, 128),) >\n",
      "       </ Embedding  -> ((128, 768),) >\n",
      "       <  Dropout <-  ((1, 128, 768),) >\n",
      "       </ Dropout  -> ((128, 768),) >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  GPT2Block <-  ((1, 128, 768),) >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2SdpaAttention <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 2304),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2SdpaAttention  -> None >\n",
      "         <  LayerNorm <-  ((1, 128, 768),) >\n",
      "         </ LayerNorm  -> ((128, 768),) >\n",
      "         <  GPT2MLP <-  ((1, 128, 768),) >\n",
      "           <  Conv1D <-  ((1, 128, 768),) >\n",
      "           </ Conv1D  -> ((128, 3072),) >\n",
      "           <  NewGELUActivation <-  ((1, 128, 3072),) >\n",
      "           </ NewGELUActivation  -> ((128, 3072),) >\n",
      "           <  Conv1D <-  ((1, 128, 3072),) >\n",
      "           </ Conv1D  -> ((128, 768),) >\n",
      "           <  Dropout <-  ((1, 128, 768),) >\n",
      "           </ Dropout  -> ((128, 768),) >\n",
      "         </ GPT2MLP  -> ((128, 768),) >\n",
      "       </ GPT2Block  -> None >\n",
      "       <  LayerNorm <-  ((1, 128, 768),) >\n",
      "       </ LayerNorm  -> ((128, 768),) >\n",
      "     </ GPT2Model  -> None >\n",
      "     <  Linear <-  ((1, 128, 768),) >\n",
      "     </ Linear  -> ((128, 50257),) >\n",
      "   </ GPT2LMHeadModel  -> None >\n",
      "Pure layers: ['Embedding', 'Embedding', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'Conv1D', 'Dropout', 'LayerNorm', 'Conv1D', 'NewGELUActivation', 'Conv1D', 'Dropout', 'LayerNorm', 'Linear']\n",
      "LayerNorm: 25\n",
      "Linear--LayerNorm: 0\n",
      "Conv1d--LayerNorm: 0\n",
      "Linear--LayerNorm (Ignore Dropout): 0\n",
      "Conv1d--LayerNorm (Ignore Dropout): 24\n"
     ]
    }
   ],
   "source": [
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "for layer in model.named_modules():\n",
    "    hooks.append(layer[1].register_forward_pre_hook(hook_pre_fn))\n",
    "    hooks.append(layer[1].register_forward_hook(hook_fn))\n",
    "\n",
    "input_ids = torch.randint(0, 1000, (1, 128))\n",
    "out = model(input_ids)\n",
    "\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "ln_cnt = 0\n",
    "linear_ln_cnt = 0\n",
    "conv1d_ln_cnt = 0\n",
    "linear_dropout_ln_cnt = 0\n",
    "conv1d_dropout_ln_cnt = 0\n",
    "\n",
    "indent = 0\n",
    "\n",
    "for i, layer in enumerate(layer_info):\n",
    "    if layer[0] == 'pre':\n",
    "        print('  ' * indent, '< ', layer[1], '<- ', layer[2], '>')\n",
    "        indent += 1\n",
    "    elif layer[0] == 'post':\n",
    "        indent -= 1\n",
    "        print('  ' * indent, '</', layer[1], ' ->', layer[2], '>')\n",
    "\n",
    "pure_layers = []\n",
    "for i in range(len(layer_info) - 1):\n",
    "    if layer_info[i][0] == 'pre' and layer_info[i + 1][0] == 'post' and layer_info[i][1] == layer_info[i + 1][1]:\n",
    "        pure_layers.append(layer_info[i][1])\n",
    "\n",
    "print('Pure layers:', pure_layers)\n",
    "\n",
    "for i, layer in enumerate(pure_layers):\n",
    "    if layer == 'LayerNorm':\n",
    "        ln_cnt += 1\n",
    "        if i - 1 > 0:\n",
    "            if pure_layers[i - 1] == 'Linear':\n",
    "                linear_ln_cnt += 1\n",
    "            elif i - 1 > 0 and pure_layers[i - 1] == 'Conv1D':\n",
    "                conv1d_ln_cnt += 1\n",
    "\n",
    "no_dropout_layers = [pure_layers[i] for i in range(len(pure_layers)) if pure_layers[i] != 'Dropout']\n",
    "\n",
    "for i, layer in enumerate(no_dropout_layers):\n",
    "    if layer == 'LayerNorm':\n",
    "        if i - 1 > 0:\n",
    "            if no_dropout_layers[i - 1] == 'Linear':\n",
    "                linear_dropout_ln_cnt += 1\n",
    "            elif i - 1 > 0 and no_dropout_layers[i - 1] == 'Conv1D':\n",
    "                conv1d_dropout_ln_cnt += 1\n",
    "\n",
    "print('LayerNorm:', ln_cnt)\n",
    "print('Linear--LayerNorm:', linear_ln_cnt)\n",
    "print('Conv1d--LayerNorm:', conv1d_ln_cnt)\n",
    "print('Linear--LayerNorm (Ignore Dropout):', linear_dropout_ln_cnt)\n",
    "print('Conv1d--LayerNorm (Ignore Dropout):', conv1d_dropout_ln_cnt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
